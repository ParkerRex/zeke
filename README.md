# ZEKE - AI-Powered News Intelligence Platform

**Status (2025-09-05)**: Core pipeline working! 47 raw items ‚Üí 16 contents ‚Üí 17 stories ‚Üí 9 overlays ‚Üí 9 embeddings processed. Worker running every 5 min with **real OpenAI GPT-4o-mini analysis**!

ZEKE is an intelligent news aggregation and analysis platform that ingests content from multiple sources (RSS, HN, Reddit, YouTube, arXiv), extracts and normalizes content, generates AI-powered summaries and insights, and serves them through a modern web interface.

## Architecture Overview

ZEKE consists of three main components:

1. **Next.js Web Application** - User interface and API endpoints
2. **Cloud Run Worker** - Background data processing pipeline
3. **Supabase Database** - PostgreSQL with pgvector for storage and vector search

### Current Data Flow

```
RSS Feeds (2 sources) ‚Üí raw_items (47) ‚Üí contents (16) ‚Üí stories (17) ‚Üí ‚úÖ overlays (9) ‚Üí ‚úÖ embeddings (9)
                                                                          ‚Üë Real OpenAI Analysis ‚Üë
```

## Current Status

### ‚úÖ What's Working

**Data Pipeline Infrastructure:**

- **Ingestion**: 47 raw items from 2 RSS sources (Hacker News + Ars Technica)
- **Content Processing**: 16 contents extracted, 17 stories created with clean text
- **AI Analysis**: 9 overlays with GPT-4o-mini summaries, 9 embeddings with text-embedding-3-small
- **Worker Infrastructure**: pg-boss scheduling, Cloud Run deployment, 5-minute intervals
- **Database**: All tables created, pgvector enabled, proper indexes and constraints
- **Job Processing**: ingest:pull (17 completed), ingest:fetch-content (42 completed), analyze:llm (working)

**Next.js Application:**

- Modern React 19 + Next.js 15 setup with TypeScript
- Supabase integration for database and authentication
- Stripe integration for subscriptions and payments
- shadcn/ui components and Tailwind CSS styling
- Email system with React Email and Resend

### ‚úÖ Recently Fixed

**LLM Analysis Pipeline (WORKING):**

- Real OpenAI GPT-4o-mini integration generating contextual "why it matters" summaries
- Smart chili scoring (0-5) and confidence ratings (0-1) based on content analysis
- Real embeddings using text-embedding-3-small (1536-dimensional vectors)
- Graceful fallbacks to stub analysis if OpenAI API fails
- Proper error handling and JSON parsing for reliable operation

### ‚ùå Current Issues

**API Endpoints (INCOMPLETE):**

- `/api/stories` returns mock data instead of real database content
- `/api/stories/[id]` not fully implemented with real data
- No connection between processed stories and frontend display

### üöß In Development

**Frontend Features:**

- Story reader interface and annotation system
- Tab-based navigation with pinning and keyboard shortcuts
- Company and industry analysis panels
- See [frontend task list](docs/prompts/wdim-tasks.md) for detailed status

**Pipeline Features:**

- Content clustering and deduplication
- Additional sources (Reddit, arXiv, YouTube)
- PDF and video transcript processing
- See [pipeline task list](docs/plans/pipeline-tasks.md) for detailed status

## Technology Stack

### Core Technologies

- **Frontend**: Next.js 15, React 19, TypeScript, Tailwind CSS, shadcn/ui
- **Backend**: Supabase (PostgreSQL + pgvector), Stripe, Resend
- **Worker**: Node.js, TypeScript, pg-boss, Cloud Run
- **Content Processing**: Mozilla Readability, fast-xml-parser, jsdom

### Data Pipeline Components

- **Scheduling**: pg-boss with cron-based job scheduling
- **Ingestion**: RSS/Atom feed parsing, content fetching with 15s timeouts
- **Extraction**: HTML cleaning with Readability, content hashing for deduplication
- **Analysis**: LLM integration for summaries and scoring (currently broken)
- **Storage**: PostgreSQL with pgvector for embeddings, Supabase Storage for raw content

## Quick Start

### Prerequisites

- Node.js 20+
- pnpm (recommended) or npm
- Supabase account
- Stripe account (for payments)
- Resend account (for emails)

### 1. Clone and Install

```bash
git clone <your-repo>
cd zeke
pnpm install
cd worker && pnpm install && cd ..
```

### 2. Environment Setup

```bash
cp .env.local.example .env.local
# Fill in your Supabase, Stripe, and Resend credentials
```

### 3. Database Setup

```bash
# Link to your Supabase project
pnpm run supabase:link

# Run migrations to create tables
pnpm run migration:up

# Generate TypeScript types
pnpm run generate-types
```

### 4. Run Development Servers

**Frontend:**

```bash
pnpm run dev
# Visit http://localhost:3000
```

**Worker (for pipeline development):**

```bash
cd worker
cp .env.example .env.local
# Add your DATABASE_URL and other worker-specific env vars
pnpm run dev
```

### 5. Stripe Setup (for payments)

```bash
# Install Stripe CLI
brew install stripe/stripe-cli/stripe

# Configure products
stripe fixtures ./stripe-fixtures.json --api-key YOUR_STRIPE_SECRET_KEY
```

## Database Schema

The pipeline uses the following core tables:

- **`sources`** - RSS feeds and other content sources (2 configured)
- **`raw_items`** - Unprocessed items from sources (47 items)
- **`contents`** - Extracted and cleaned content with content_hash (16 items)
- **`stories`** - Normalized story records (15 items)
- **`story_overlays`** - AI-generated summaries and scores (0 items - BROKEN)
- **`story_embeddings`** - Vector embeddings for search (0 items - BROKEN)
- **`clusters`** - Groups of related stories (not yet implemented)
- **`highlights`** - User annotations and highlights

See [pipeline diagrams](docs/plans/pipeline-diagrams.md) for detailed schema and relationships.

## Data Pipeline Architecture

### How It Works (Current)

1. **Ingestion** (`ingest:pull`): Every 5 minutes, worker fetches new items from RSS feeds
2. **Content Fetch** (`ingest:fetch-content`): Downloads and extracts clean text using Readability
3. **Story Creation**: Generates content_hash for deduplication, creates story records
4. **Analysis** (`analyze:llm`): **BROKEN** - Should generate summaries, scores, and embeddings

### Worker Deployment

The worker runs on Google Cloud Run and processes jobs from pg-boss queues:

```bash
cd worker

# Deploy to production
pnpm run deploy:prod

# View logs
pnpm run logs

# View error logs only
pnpm run logs:errors
```

### Job Queues (pg-boss)

- **`system:heartbeat`** - Health check every 5 minutes ‚úÖ
- **`ingest:pull`** - Fetch new RSS items every 5 minutes ‚úÖ
- **`ingest:fetch-content`** - Extract content from URLs ‚úÖ
- **`analyze:llm`** - Generate AI summaries and embeddings ‚úÖ WORKING

## Known Issues & Next Steps

### üö® Critical Issues

1. **API Endpoints Return Mock Data** (highest priority)
   - `/api/stories` serves fixtures instead of real database content
   - Need to implement database queries in `src/features/stories/controllers/`
   - This is now the main blocker for frontend development

### üìã Next Steps

1. **Connect APIs to Database** - Replace mock data with real story queries (highest priority)
2. **Add Content Sources** - Implement Reddit, arXiv, YouTube ingestion
3. **Implement Clustering** - Group related stories by similarity
4. **Build Reader Interface** - Story viewing and annotation features
5. **Enhance AI Analysis** - Add citation extraction and improved confidence scoring

See detailed task lists:

- [Pipeline Tasks](docs/plans/pipeline-tasks.md) - Worker and data processing
- [Frontend Tasks](docs/prompts/wdim-tasks.md) - UI and user experience

## Development Guide

### File Structure

```
‚îú‚îÄ‚îÄ src/                    # Next.js application
‚îÇ   ‚îú‚îÄ‚îÄ app/               # App router pages and API routes
‚îÇ   ‚îú‚îÄ‚îÄ components/        # Reusable UI components
‚îÇ   ‚îú‚îÄ‚îÄ features/          # Feature-specific code (stories, auth, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ libs/              # Shared utilities and clients
‚îú‚îÄ‚îÄ worker/                # Background processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ src/               # Worker source code
‚îÇ   ‚îî‚îÄ‚îÄ scripts/           # Deployment and utility scripts
‚îú‚îÄ‚îÄ supabase/              # Database migrations and config
‚îî‚îÄ‚îÄ docs/                  # Project documentation
```

### Key Directories

- **`src/features/stories/`** - Story-related controllers and types (currently uses mock data)
- **`worker/src/`** - Data pipeline implementation (ingestion, extraction, analysis)
- **`supabase/migrations/`** - Database schema definitions
- **`docs/plans/`** - Architecture documentation and task tracking

### Environment Variables

**Frontend (`.env.local`):**

```bash
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
STRIPE_SECRET_KEY=your_stripe_secret
RESEND_API_KEY=your_resend_key
```

**Worker (`worker/.env.local`):**

```bash
DATABASE_URL=postgresql://worker:password@host:5432/postgres?sslmode=require
BOSS_SCHEMA=pgboss
BOSS_CRON_TZ=UTC
BOSS_MIGRATE=false
```

### Debugging the Pipeline

1. **Check job status:**

   ```sql
   SELECT name, state, COUNT(*) FROM pgboss.job GROUP BY name, state;
   ```

2. **View recent stories:**

   ```sql
   SELECT title, canonical_url, created_at FROM stories ORDER BY created_at DESC LIMIT 10;
   ```

3. **Check worker logs:**
   ```bash
   cd worker && pnpm run logs:errors
   ```

## Contributing

1. **Fix the LLM Analysis Bug** - The most critical issue blocking AI features
2. **Connect APIs to Database** - Replace mock data with real queries
3. **Add Content Sources** - Implement additional ingestion sources
4. **Improve UI/UX** - See [frontend tasks](docs/prompts/wdim-tasks.md)

## Documentation

- [Pipeline Architecture](docs/plans/pipeline-diagrams.md) - Visual diagrams of system components
- [Pipeline Specification](docs/plans/pipeline-spec.md) - Detailed technical specification
- [Pipeline Tasks](docs/plans/pipeline-tasks.md) - Current development status and tasks
- [Frontend Tasks](docs/prompts/wdim-tasks.md) - UI development checklist

---

**Current Priority**: Connect APIs to database to serve real story data with AI-powered summaries and insights.
