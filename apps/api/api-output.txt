This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-09-15T18:01:08.250Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
supabase/
  migrations/
    20250906120000_baseline_all.sql
    20250906131500_source_metrics.sql
    20250906133000_admin_realtime_tables.sql
    20250906220500_worker_policies_admin_tables.sql
    20250906221000_worker_select_policies.sql
    20250911120000_update_handle_new_user_avatar_coalesce.sql
    20250914173416_optimize_rls_auth_functions.sql
    20250914173444_consolidate_permissive_policies.sql
    20250914175429_fix_function_search_path_security.sql
    20250914175606_move_vector_extension_to_extensions_schema.sql
    20250914211053_add_missing_sources_fields.sql
    20250914211837_enable_rls_content_pipeline.sql
    20250915162756_grant_worker_write_source_metrics.sql
    20250915175811_fix_worker_role_permissions.sql
    20250915180000_add_worker_select_source_metrics.sql
    20250915180001_add_worker_contents_permissions.sql
  seeds/
    youtube_sources.sql
  .gitignore
  config.toml
  seed.sql
.env.local
.env.ts
instrumentation.ts
package.json
vitest.config.ts

================================================================
Files
================================================================

================
File: supabase/migrations/20250906120000_baseline_all.sql
================
-- Extensions
create extension if not exists pgcrypto;
create extension if not exists vector;

-- USERS (linked to auth.users)
create table if not exists public.users (
  id uuid references auth.users not null primary key,
  full_name text,
  avatar_url text,
  billing_address jsonb,
  payment_method jsonb,
  is_admin boolean not null default false
);
alter table public.users enable row level security;
create policy "Can view own user data." on users for select using (auth.uid() = id);
create policy "Can update own user data." on users for update using (auth.uid() = id);

/**
* This trigger automatically creates a user entry when a new user signs up via Supabase Auth.
*/
create function public.handle_new_user()
returns trigger as $$
begin
  insert into public.users (id, full_name, avatar_url)
  values (new.id, new.raw_user_meta_data->>'full_name', new.raw_user_meta_data->>'avatar_url');
  return new;
end;
$$ language plpgsql security definer;
create trigger on_auth_user_created
  after insert on auth.users
  for each row execute procedure public.handle_new_user();

/**
* CUSTOMERS
* Note: this is a private table that contains a mapping of user IDs to Stripe customer IDs.
*/
create table customers (
  -- UUID from auth.users
  id uuid references auth.users not null primary key,
  -- The user's customer ID in Stripe. User must not be able to update this.
  stripe_customer_id text
);
alter table customers enable row level security;
-- No policies as this is a private table that the user must not have access to.

/**
* PRODUCTS
* Note: products are created and managed in Stripe and synced to our DB via Stripe webhooks.
*/
create table products (
  -- Product ID from Stripe, e.g. prod_1234.
  id text primary key,
  -- Whether the product is currently available for purchase.
  active boolean,
  -- The product's name, meant to be displayable to the customer. Whenever this product is sold via a subscription, name will show up on associated invoice line item descriptions.
  name text,
  -- The product's description, meant to be displayable to the customer. Use this field to optionally store a long form explanation of the product being sold for your own rendering purposes.
  description text,
  -- A URL of the product image in Stripe, meant to be displayable to the customer.
  image text,
  -- Set of key-value pairs, used to store additional information about the object in a structured format.
  metadata jsonb
);
alter table products enable row level security;
create policy "Allow public read-only access." on products for select using (true);

/**
* PRICES
* Note: prices are created and managed in Stripe and synced to our DB via Stripe webhooks.
*/
create type pricing_type as enum ('one_time', 'recurring');
create type pricing_plan_interval as enum ('day', 'week', 'month', 'year');
create table prices (
  -- Price ID from Stripe, e.g. price_1234.
  id text primary key,
  -- The ID of the prduct that this price belongs to.
  product_id text references products,
  -- Whether the price can be used for new purchases.
  active boolean,
  -- A brief description of the price.
  description text,
  -- The unit amount as a positive integer in the smallest currency unit (e.g., 100 cents for US$1.00 or 100 for Â¥100, a zero-decimal currency).
  unit_amount bigint,
  -- Three-letter ISO currency code, in lowercase.
  currency text check (char_length(currency) = 3),
  -- One of `one_time` or `recurring` depending on whether the price is for a one-time purchase or a recurring (subscription) purchase.
  type pricing_type,
  -- The frequency at which a subscription is billed. One of `day`, `week`, `month` or `year`.
  interval pricing_plan_interval,
  -- The number of intervals (specified in the `interval` attribute) between subscription billings. For example, `interval=month` and `interval_count=3` bills every 3 months.
  interval_count integer,
  -- Default number of trial days when subscribing a customer to this price using [`trial_from_plan=true`](https://stripe.com/docs/api#create_subscription-trial_from_plan).
  trial_period_days integer,
  -- Set of key-value pairs, used to store additional information about the object in a structured format.
  metadata jsonb
);
alter table prices enable row level security;
create policy "Allow public read-only access." on prices for select using (true);

/**
* SUBSCRIPTIONS
* Note: subscriptions are created and managed in Stripe and synced to our DB via Stripe webhooks.
*/
create type subscription_status as enum ('trialing', 'active', 'canceled', 'incomplete', 'incomplete_expired', 'past_due', 'unpaid', 'paused');
create table subscriptions (
  -- Subscription ID from Stripe, e.g. sub_1234.
  id text primary key,
  user_id uuid references auth.users not null,
  -- The status of the subscription object, one of subscription_status type above.
  status subscription_status,
  -- Set of key-value pairs, used to store additional information about the object in a structured format.
  metadata jsonb,
  -- ID of the price that created this subscription.
  price_id text references prices,
  -- Quantity multiplied by the unit amount of the price creates the amount of the subscription. Can be used to charge multiple seats.
  quantity integer,
  -- If true the subscription has been canceled by the user and will be deleted at the end of the billing period.
  cancel_at_period_end boolean,
  -- Time at which the subscription was created.
  created timestamp with time zone default timezone('utc'::text, now()) not null,
  -- Start of the current period that the subscription has been invoiced for.
  current_period_start timestamp with time zone default timezone('utc'::text, now()) not null,
  -- End of the current period that the subscription has been invoiced for. At the end of this period, a new invoice will be created.
  current_period_end timestamp with time zone default timezone('utc'::text, now()) not null,
  -- If the subscription has ended, the timestamp of the date the subscription ended.
  ended_at timestamp with time zone default timezone('utc'::text, now()),
  -- A date in the future at which the subscription will automatically get canceled.
  cancel_at timestamp with time zone default timezone('utc'::text, now()),
  -- If the subscription has been canceled, the date of that cancellation. If the subscription was canceled with `cancel_at_period_end`, `canceled_at` will still reflect the date of the initial cancellation request, not the end of the subscription period when the subscription is automatically moved to a canceled state.
  canceled_at timestamp with time zone default timezone('utc'::text, now()),
  -- If the subscription has a trial, the beginning of that trial.
  trial_start timestamp with time zone default timezone('utc'::text, now()),
  -- If the subscription has a trial, the end of that trial.
  trial_end timestamp with time zone default timezone('utc'::text, now())
);
alter table subscriptions enable row level security;
create policy "Can only view own subs data." on subscriptions for select using (auth.uid() = user_id);

/**
 * REALTIME SUBSCRIPTIONS
 * Only allow realtime listening on public tables.
 */
drop publication if exists supabase_realtime;
create publication supabase_realtime for table products, prices;

-- CONTENT PIPELINE TABLES
create table if not exists public.sources (
  id uuid primary key default gen_random_uuid(),
  kind text not null,
  name text,
  url text,
  domain text,
  authority_score numeric,
  last_cursor jsonb,
  last_checked timestamptz,
  metadata jsonb,
  active boolean not null default true,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);
alter table public.sources enable row level security;

create table if not exists public.raw_items (
  id uuid primary key default gen_random_uuid(),
  source_id uuid not null references public.sources(id) on delete cascade,
  external_id text not null,
  url text not null,
  kind text,
  title text,
  metadata jsonb,
  discovered_at timestamptz default now(),
  status text,
  error text,
  attempts int default 0
);
create unique index if not exists raw_items_source_external_unique on public.raw_items(source_id, external_id);

create table if not exists public.contents (
  id uuid primary key default gen_random_uuid(),
  raw_item_id uuid not null references public.raw_items(id) on delete cascade,
  text text,
  html_url text,
  transcript_url text,
  pdf_url text,
  audio_url text,
  duration_seconds integer,
  view_count bigint,
  transcript_vtt text,
  lang text,
  extracted_at timestamptz default now(),
  content_hash text not null
);
create index if not exists contents_content_hash_idx on public.contents(content_hash);
create index if not exists idx_contents_audio_url on public.contents(audio_url) where audio_url is not null;
create index if not exists idx_contents_transcript_url on public.contents(transcript_url) where transcript_url is not null;
create index if not exists idx_contents_transcript_vtt on public.contents(id) where transcript_vtt is not null;
create index if not exists idx_raw_items_youtube on public.raw_items(kind, external_id) where kind = 'youtube';

create table if not exists public.stories (
  id uuid primary key default gen_random_uuid(),
  content_id uuid not null unique references public.contents(id) on delete restrict,
  canonical_url text,
  kind text,
  title text,
  primary_url text,
  published_at timestamptz,
  cluster_key text,
  created_at timestamptz default now()
);
create index if not exists stories_cluster_key_idx on public.stories(cluster_key);

create table if not exists public.clusters (
  cluster_key text primary key,
  representative_story_id uuid references public.stories(id),
  created_at timestamptz default now()
);

create table if not exists public.story_overlays (
  story_id uuid primary key references public.stories(id) on delete cascade,
  why_it_matters text,
  chili int,
  confidence numeric,
  citations jsonb,
  model_version text,
  analyzed_at timestamptz
);

create table if not exists public.story_embeddings (
  story_id uuid primary key references public.stories(id) on delete cascade,
  embedding vector(1536) not null,
  model_version text
);
do $$ begin
  begin
    execute 'create index if not exists story_embeddings_l2_idx on public.story_embeddings using ivfflat (embedding vector_l2_ops) with (lists = 100)';
  exception when others then null; end;
end $$;

create table if not exists public.highlights (
  id uuid primary key default gen_random_uuid(),
  user_id uuid not null,
  story_id uuid not null references public.stories(id) on delete cascade,
  content_id uuid not null references public.contents(id) on delete cascade,
  span jsonb not null,
  created_at timestamptz default now()
);
create index if not exists highlights_user_idx on public.highlights(user_id);
create index if not exists highlights_story_idx on public.highlights(story_id);
alter table public.highlights enable row level security;
do $$ begin
  begin
    drop policy if exists highlights_owner_all on public.highlights;
    create policy highlights_owner_all on public.highlights for all using (auth.uid() = user_id) with check (auth.uid() = user_id);
  exception when others then null; end;
end $$;

-- Helpful indexes
create index if not exists raw_items_source_discovered_idx on public.raw_items(source_id, discovered_at desc);
create index if not exists stories_created_idx on public.stories(created_at desc);
create index if not exists stories_published_idx on public.stories(published_at);
create index if not exists clusters_rep_story_idx on public.clusters(representative_story_id);
create index if not exists sources_domain_idx on public.sources(domain);
create index if not exists contents_raw_item_idx on public.contents(raw_item_id);
create index if not exists contents_extracted_idx on public.contents(extracted_at desc);
create index if not exists sources_last_checked_idx on public.sources(last_checked);

-- YouTube-specific metadata constraint and helper
do $$ begin
  begin
    alter table public.sources add constraint check_youtube_metadata
      check ((kind not in ('youtube_channel','youtube_search')) or (metadata is not null and metadata <> '{}'::jsonb));
  exception when duplicate_object then null; end;
end $$;

create or replace function public.get_youtube_sources()
returns table (
  id uuid,
  kind text,
  name text,
  url text,
  domain text,
  metadata jsonb,
  last_cursor jsonb
) language sql as $$
  select id, kind, name, url, domain, metadata, last_cursor
  from public.sources
  where kind in ('youtube_channel','youtube_search') and metadata is not null
$$;

-- pg-boss setup (schema + permissions). Avoid hardcoded passwords.
do $$
begin
  if not exists (select from pg_catalog.pg_roles where rolname = 'worker') then
    create role worker with login;
  end if;
end
$$;

create schema if not exists pgboss;

do $$ begin
  begin
    create type pgboss.job_state as enum ('created','retry','active','completed','cancelled','failed');
  exception when duplicate_object then null; end;
end $$;

create table if not exists pgboss.version (
  version int primary key,
  maintained_on timestamptz,
  cron_on timestamptz,
  monitored_on timestamptz
);

create table if not exists pgboss.queue (
  name text,
  policy text,
  retry_limit int,
  retry_delay int,
  retry_backoff bool,
  expire_seconds int,
  retention_minutes int,
  dead_letter text references pgboss.queue (name),
  partition_name text,
  created_on timestamptz not null default now(),
  updated_on timestamptz not null default now(),
  primary key (name)
);

create table if not exists pgboss.schedule (
  name text references pgboss.queue on delete cascade,
  cron text not null,
  timezone text,
  data jsonb,
  options jsonb,
  created_on timestamptz not null default now(),
  updated_on timestamptz not null default now(),
  primary key (name)
);

create table if not exists pgboss.subscription (
  event text not null,
  name text not null references pgboss.queue on delete cascade,
  created_on timestamptz not null default now(),
  updated_on timestamptz not null default now(),
  primary key(event, name)
);

create table if not exists pgboss.job (
  id uuid not null default gen_random_uuid(),
  name text not null,
  priority integer not null default(0),
  data jsonb,
  state pgboss.job_state not null default('created'),
  retry_limit integer not null default(2),
  retry_count integer not null default(0),
  retry_delay integer not null default(0),
  retry_backoff boolean not null default false,
  start_after timestamptz not null default now(),
  started_on timestamptz,
  singleton_key text,
  singleton_on timestamp without time zone,
  expire_in interval not null default interval '15 minutes',
  created_on timestamptz not null default now(),
  completed_on timestamptz,
  keep_until timestamptz not null default now() + interval '14 days',
  output jsonb,
  dead_letter text,
  policy text,
  primary key (name, id)
) partition by list (name);

create table if not exists pgboss.archive (like pgboss.job);
alter table pgboss.archive add column if not exists archived_on timestamptz not null default now();
alter table pgboss.archive drop constraint if exists archive_pkey;
alter table pgboss.archive add primary key (name, id);
create index if not exists archive_i1 on pgboss.archive(archived_on);

create or replace function pgboss.create_queue(queue_name text, options json)
returns void as $$
declare
  table_name varchar := 'j' || encode(sha224(queue_name::bytea), 'hex');
  queue_created_on timestamptz;
begin
  with q as (
    insert into pgboss.queue (name, policy, retry_limit, retry_delay, retry_backoff, expire_seconds, retention_minutes, dead_letter, partition_name)
    values (
      queue_name,
      options->>'policy',
      (options->>'retryLimit')::int,
      (options->>'retryDelay')::int,
      (options->>'retryBackoff')::bool,
      (options->>'expireInSeconds')::int,
      (options->>'retentionMinutes')::int,
      options->>'deadLetter',
      table_name
    ) on conflict do nothing returning created_on)
  select created_on into queue_created_on from q;

  if queue_created_on is null then return; end if;

  execute format('create table pgboss.%I (like pgboss.job including defaults)', table_name);
  execute format('alter table pgboss.%1$I add primary key (name, id)', table_name);
  execute format('alter table pgboss.%1$I add constraint q_fkey foreign key (name) references pgboss.queue (name) on delete restrict deferrable initially deferred', table_name);
  execute format('alter table pgboss.%1$I add constraint dlq_fkey foreign key (dead_letter) references pgboss.queue (name) on delete restrict deferrable initially deferred', table_name);
  execute format('create unique index %1$s_i1 on pgboss.%1$I (name, coalesce(singleton_key, '''')) where state = ''created'' and policy = ''short''', table_name);
  execute format('create index %1$s_i2 on pgboss.%1$I (name, start_after) include (priority, created_on, id) where state < ''active''', table_name);
  execute format('create index %1$s_i3 on pgboss.%1$I (name) include (priority, created_on, id) where state = ''created'' and policy = ''singleton''', table_name);
  execute format('alter table pgboss.%I add constraint cjc check (name=%L)', table_name, queue_name);
  execute format('alter table pgboss.job attach partition pgboss.%I for values in (%L)', table_name, queue_name);
end;
$$ language plpgsql;

create or replace function pgboss.delete_queue(queue_name text)
returns void as $$
declare
  table_name varchar;
begin
  select partition_name into table_name from pgboss.queue where name = queue_name;
  if table_name is not null then execute format('drop table pgboss.%I', table_name); end if;
  delete from pgboss.queue where name = queue_name;
end;
$$ language plpgsql;

insert into pgboss.version (version) values (24) on conflict do nothing;

-- Grant ownership and permissions to worker
do $$ begin
  begin execute 'grant worker to postgres'; exception when others then null; end;
end $$;
alter schema pgboss owner to worker;
alter type pgboss.job_state owner to worker;
alter table pgboss.version owner to worker;
alter table pgboss.queue owner to worker;
alter table pgboss.schedule owner to worker;
alter table pgboss.subscription owner to worker;
alter table pgboss.job owner to worker;
alter table pgboss.archive owner to worker;
alter function pgboss.create_queue(text, json) owner to worker;
alter function pgboss.delete_queue(text) owner to worker;
do $$ begin
  begin execute 'revoke worker from postgres'; exception when others then null; end;
end $$;

grant usage, create on schema pgboss to worker;
grant all privileges on all tables in schema pgboss to worker;
grant all privileges on all sequences in schema pgboss to worker;
grant all privileges on all functions in schema pgboss to worker;
grant usage on type pgboss.job_state to worker;

================
File: supabase/migrations/20250906131500_source_metrics.sql
================
-- Source metrics aggregation + realtime

create table if not exists public.source_metrics (
  source_id uuid primary key references public.sources(id) on delete cascade,
  raw_total int default 0,
  contents_total int default 0,
  stories_total int default 0,
  raw_24h int default 0,
  contents_24h int default 0,
  stories_24h int default 0,
  last_raw_at timestamptz,
  last_content_at timestamptz,
  last_story_at timestamptz,
  updated_at timestamptz not null default now()
);

alter table public.source_metrics enable row level security;

-- Admins only can select metrics
do $$ begin
  begin
    create policy source_metrics_admin_select on public.source_metrics
      for select using (
        exists (
          select 1 from public.users u where u.id = auth.uid() and coalesce(u.is_admin,false) = true
        )
      );
  exception when duplicate_object then null; end;
end $$;

-- Function to refresh metrics for one/all sources
create or replace function public.refresh_source_metrics(_source_id uuid default null)
returns void
language plpgsql as $$
begin
  if _source_id is null then
    -- Recompute for all sources
    insert into public.source_metrics as m (
      source_id,
      raw_total,
      contents_total,
      stories_total,
      raw_24h,
      contents_24h,
      stories_24h,
      last_raw_at,
      last_content_at,
      last_story_at,
      updated_at
    )
    select s.id,
      coalesce((select count(*) from public.raw_items r where r.source_id = s.id), 0) as raw_total,
      coalesce((select count(*) from public.contents c join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id), 0) as contents_total,
      coalesce((select count(*) from public.stories st join public.contents c on c.id = st.content_id join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id), 0) as stories_total,
      coalesce((select count(*) from public.raw_items r where r.source_id = s.id and r.discovered_at > now() - interval '24 hours'), 0) as raw_24h,
      coalesce((select count(*) from public.contents c join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id and c.extracted_at > now() - interval '24 hours'), 0) as contents_24h,
      coalesce((select count(*) from public.stories st join public.contents c on c.id = st.content_id join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id and st.created_at > now() - interval '24 hours'), 0) as stories_24h,
      (select max(discovered_at) from public.raw_items r where r.source_id = s.id) as last_raw_at,
      (select max(extracted_at) from public.contents c join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id) as last_content_at,
      (select max(st.created_at) from public.stories st join public.contents c on c.id = st.content_id join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id) as last_story_at,
      now()
    from public.sources s
    on conflict (source_id) do update set
      raw_total = excluded.raw_total,
      contents_total = excluded.contents_total,
      stories_total = excluded.stories_total,
      raw_24h = excluded.raw_24h,
      contents_24h = excluded.contents_24h,
      stories_24h = excluded.stories_24h,
      last_raw_at = excluded.last_raw_at,
      last_content_at = excluded.last_content_at,
      last_story_at = excluded.last_story_at,
      updated_at = excluded.updated_at;
  else
    -- Recompute for one source
    insert into public.source_metrics as m (
      source_id, raw_total, contents_total, stories_total, raw_24h, contents_24h, stories_24h, last_raw_at, last_content_at, last_story_at, updated_at
    )
    select s.id,
      coalesce((select count(*) from public.raw_items r where r.source_id = s.id), 0),
      coalesce((select count(*) from public.contents c join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id), 0),
      coalesce((select count(*) from public.stories st join public.contents c on c.id = st.content_id join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id), 0),
      coalesce((select count(*) from public.raw_items r where r.source_id = s.id and r.discovered_at > now() - interval '24 hours'), 0),
      coalesce((select count(*) from public.contents c join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id and c.extracted_at > now() - interval '24 hours'), 0),
      coalesce((select count(*) from public.stories st join public.contents c on c.id = st.content_id join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id and st.created_at > now() - interval '24 hours'), 0),
      (select max(discovered_at) from public.raw_items r where r.source_id = s.id),
      (select max(extracted_at) from public.contents c join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id),
      (select max(st.created_at) from public.stories st join public.contents c on c.id = st.content_id join public.raw_items r on r.id = c.raw_item_id where r.source_id = s.id),
      now()
    from public.sources s where s.id = _source_id
    on conflict (source_id) do update set
      raw_total = excluded.raw_total,
      contents_total = excluded.contents_total,
      stories_total = excluded.stories_total,
      raw_24h = excluded.raw_24h,
      contents_24h = excluded.contents_24h,
      stories_24h = excluded.stories_24h,
      last_raw_at = excluded.last_raw_at,
      last_content_at = excluded.last_content_at,
      last_story_at = excluded.last_story_at,
      updated_at = excluded.updated_at;
  end if;
end;
$$;

-- Trigger functions to refresh per source slice
create or replace function public.tg_refresh_metrics_on_raw_items()
returns trigger language plpgsql as $$
begin
  perform public.refresh_source_metrics(NEW.source_id);
  return null;
end;$$;

create or replace function public.tg_refresh_metrics_on_contents()
returns trigger language plpgsql as $$
declare _sid uuid; begin
  select r.source_id into _sid from public.raw_items r where r.id = NEW.raw_item_id;
  if _sid is not null then perform public.refresh_source_metrics(_sid); end if;
  return null; end;$$;

create or replace function public.tg_refresh_metrics_on_stories()
returns trigger language plpgsql as $$
declare _sid uuid; begin
  select r.source_id into _sid from public.raw_items r join public.contents c on c.id = NEW.content_id and c.raw_item_id = r.id;
  if _sid is not null then perform public.refresh_source_metrics(_sid); end if;
  return null; end;$$;

drop trigger if exists tr_refresh_metrics_raw_items on public.raw_items;
create trigger tr_refresh_metrics_raw_items after insert on public.raw_items for each row execute procedure public.tg_refresh_metrics_on_raw_items();

drop trigger if exists tr_refresh_metrics_contents on public.contents;
create trigger tr_refresh_metrics_contents after insert on public.contents for each row execute procedure public.tg_refresh_metrics_on_contents();

drop trigger if exists tr_refresh_metrics_stories on public.stories;
create trigger tr_refresh_metrics_stories after insert on public.stories for each row execute procedure public.tg_refresh_metrics_on_stories();

-- Add table to realtime publication
do $$ begin
  begin
    alter publication supabase_realtime add table public.source_metrics;
  exception when undefined_object then null; end;
end $$;

================
File: supabase/migrations/20250906133000_admin_realtime_tables.sql
================
-- Quota, Source Health, Job Metrics (Realtime)

-- Platform quota snapshot
create table if not exists public.platform_quota (
  provider text primary key,
  quota_limit int,
  used int,
  remaining int,
  reset_at timestamptz,
  updated_at timestamptz not null default now()
);
alter table public.platform_quota enable row level security;
do $$ begin
  begin
    create policy platform_quota_admin_select on public.platform_quota for select using (
      exists (select 1 from public.users u where u.id = auth.uid() and coalesce(u.is_admin,false) = true)
    );
  exception when duplicate_object then null; end;
end $$;

-- Source health snapshot
do $$ begin
  begin
    create type public.health_status as enum ('ok','warn','error');
  exception when duplicate_object then null; end;
end $$;
create table if not exists public.source_health (
  source_id uuid primary key references public.sources(id) on delete cascade,
  status public.health_status not null default 'ok',
  last_success_at timestamptz,
  last_error_at timestamptz,
  error_24h int default 0,
  message text,
  updated_at timestamptz not null default now()
);
alter table public.source_health enable row level security;
do $$ begin
  begin
    create policy source_health_admin_select on public.source_health for select using (
      exists (select 1 from public.users u where u.id = auth.uid() and coalesce(u.is_admin,false) = true)
    );
  exception when duplicate_object then null; end;
end $$;

-- Job metrics mirror
create table if not exists public.job_metrics (
  name text not null,
  state text not null,
  count int not null default 0,
  updated_at timestamptz not null default now(),
  primary key (name, state)
);
alter table public.job_metrics enable row level security;
do $$ begin
  begin
    create policy job_metrics_admin_select on public.job_metrics for select using (
      exists (select 1 from public.users u where u.id = auth.uid() and coalesce(u.is_admin,false) = true)
    );
  exception when duplicate_object then null; end;
end $$;

-- Add to realtime publication
do $$ begin
  begin
    alter publication supabase_realtime add table public.platform_quota, public.source_health, public.job_metrics;
  exception when undefined_object then null; end;
end $$;

================
File: supabase/migrations/20250906220500_worker_policies_admin_tables.sql
================
-- Allow 'worker' DB role to insert/update admin realtime tables under RLS
-- Safe to reapply: each CREATE POLICY is wrapped to ignore duplicates.

-- platform_quota: permit worker to insert/update
do $$ begin
  begin
    create policy platform_quota_worker_insert on public.platform_quota
      for insert to worker with check (true);
  exception when duplicate_object then null; end;
  begin
    create policy platform_quota_worker_update on public.platform_quota
      for update to worker using (true) with check (true);
  exception when duplicate_object then null; end;
end $$;

-- source_health: permit worker to insert/update
do $$ begin
  begin
    create policy source_health_worker_insert on public.source_health
      for insert to worker with check (true);
  exception when duplicate_object then null; end;
  begin
    create policy source_health_worker_update on public.source_health
      for update to worker using (true) with check (true);
  exception when duplicate_object then null; end;
end $$;

-- job_metrics: permit worker to insert/update
do $$ begin
  begin
    create policy job_metrics_worker_insert on public.job_metrics
      for insert to worker with check (true);
  exception when duplicate_object then null; end;
  begin
    create policy job_metrics_worker_update on public.job_metrics
      for update to worker using (true) with check (true);
  exception when duplicate_object then null; end;
end $$;

================
File: supabase/migrations/20250906221000_worker_select_policies.sql
================
-- Allow 'worker' to SELECT rows from admin realtime tables for upsert flows
-- (ON CONFLICT DO UPDATE requires row visibility under RLS semantics)

do $$ begin
  begin
    create policy platform_quota_worker_select on public.platform_quota
      for select to worker using (true);
  exception when duplicate_object then null; end;
end $$;

do $$ begin
  begin
    create policy source_health_worker_select on public.source_health
      for select to worker using (true);
  exception when duplicate_object then null; end;
end $$;

do $$ begin
  begin
    create policy job_metrics_worker_select on public.job_metrics
      for select to worker using (true);
  exception when duplicate_object then null; end;
end $$;

================
File: supabase/migrations/20250911120000_update_handle_new_user_avatar_coalesce.sql
================
-- Update new-user trigger to handle Google metadata keys and backfill existing users
create or replace function public.handle_new_user()
returns trigger as $$
begin
  insert into public.users (id, full_name, avatar_url)
  values (
    new.id,
    coalesce(
      new.raw_user_meta_data->>'full_name',
      new.raw_user_meta_data->>'name'
    ),
    coalesce(
      new.raw_user_meta_data->>'avatar_url',
      new.raw_user_meta_data->>'picture'
    )
  );
  return new;
end;
$$ language plpgsql security definer;

-- Backfill missing profile fields from auth.users metadata
update public.users u
set
  full_name = coalesce(u.full_name, au.raw_user_meta_data->>'full_name', au.raw_user_meta_data->>'name'),
  avatar_url = coalesce(u.avatar_url, au.raw_user_meta_data->>'avatar_url', au.raw_user_meta_data->>'picture')
from auth.users au
where u.id = au.id
  and (u.full_name is null or u.avatar_url is null);

================
File: supabase/migrations/20250914173416_optimize_rls_auth_functions.sql
================
-- Optimize RLS policies to fix auth function re-evaluation performance issues
-- This addresses Supabase Performance Advisor warnings about auth.<function>() calls
-- being re-evaluated for each row instead of once per query
--
-- Performance Issue: RLS policies using auth.uid() directly cause the function
-- to be called for every row being evaluated, which is inefficient for large result sets.
-- 
-- Solution: Replace auth.<function>() with (select auth.<function>()) to ensure
-- the function is evaluated once per query and cached for all rows.
--
-- Security Impact: NONE - This is a performance optimization that maintains
-- identical security behavior and access control.

-- =============================================================================
-- USERS TABLE OPTIMIZATION
-- =============================================================================

-- Drop existing policies
DROP POLICY IF EXISTS "Can view own user data." ON public.users;
DROP POLICY IF EXISTS "Can update own user data." ON public.users;

-- Recreate with optimized auth function calls
CREATE POLICY "users_select_own" ON public.users
  FOR SELECT TO authenticated
  USING ((SELECT auth.uid()) = id);

CREATE POLICY "users_update_own" ON public.users
  FOR UPDATE TO authenticated
  USING ((SELECT auth.uid()) = id)
  WITH CHECK ((SELECT auth.uid()) = id);

-- =============================================================================
-- SUBSCRIPTIONS TABLE OPTIMIZATION
-- =============================================================================

-- Drop existing policy
DROP POLICY IF EXISTS "Can only view own subs data." ON public.subscriptions;

-- Recreate with optimized auth function call
CREATE POLICY "subscriptions_select_own" ON public.subscriptions
  FOR SELECT TO authenticated
  USING ((SELECT auth.uid()) = user_id);

-- =============================================================================
-- HIGHLIGHTS TABLE OPTIMIZATION
-- =============================================================================

-- Drop existing policy
DROP POLICY IF EXISTS "highlights_owner_all" ON public.highlights;

-- Recreate with optimized auth function calls
CREATE POLICY "highlights_select_own" ON public.highlights
  FOR SELECT TO authenticated
  USING ((SELECT auth.uid()) = user_id);

CREATE POLICY "highlights_insert_own" ON public.highlights
  FOR INSERT TO authenticated
  WITH CHECK ((SELECT auth.uid()) = user_id);

CREATE POLICY "highlights_update_own" ON public.highlights
  FOR UPDATE TO authenticated
  USING ((SELECT auth.uid()) = user_id)
  WITH CHECK ((SELECT auth.uid()) = user_id);

CREATE POLICY "highlights_delete_own" ON public.highlights
  FOR DELETE TO authenticated
  USING ((SELECT auth.uid()) = user_id);

-- =============================================================================
-- ADMIN TABLES OPTIMIZATION (source_metrics, platform_quota, source_health, job_metrics)
-- =============================================================================

-- Update is_admin_user function to use cached auth.uid()
CREATE OR REPLACE FUNCTION public.is_admin_user()
RETURNS boolean
LANGUAGE sql
SECURITY DEFINER
STABLE
AS $$
  SELECT COALESCE(
    (SELECT is_admin FROM public.users WHERE id = (SELECT auth.uid())),
    false
  );
$$;

-- Drop existing admin policies for source_metrics
DROP POLICY IF EXISTS "source_metrics_admin_select" ON public.source_metrics;

-- Recreate with direct function call (function is now optimized internally)
CREATE POLICY "source_metrics_admin_select" ON public.source_metrics
  FOR SELECT TO authenticated
  USING (public.is_admin_user());

-- Drop existing admin policies for platform_quota
DROP POLICY IF EXISTS "platform_quota_admin_select" ON public.platform_quota;

-- Recreate with direct function call
CREATE POLICY "platform_quota_admin_select" ON public.platform_quota
  FOR SELECT TO authenticated
  USING (public.is_admin_user());

-- Drop existing admin policies for source_health
DROP POLICY IF EXISTS "source_health_admin_select" ON public.source_health;

-- Recreate with direct function call
CREATE POLICY "source_health_admin_select" ON public.source_health
  FOR SELECT TO authenticated
  USING (public.is_admin_user());

-- Drop existing admin policies for job_metrics
DROP POLICY IF EXISTS "job_metrics_admin_select" ON public.job_metrics;

-- Recreate with direct function call
CREATE POLICY "job_metrics_admin_select" ON public.job_metrics
  FOR SELECT TO authenticated
  USING (public.is_admin_user());

-- =============================================================================
-- PERFORMANCE NOTES
-- =============================================================================

-- The optimizations in this migration:
-- 1. Replace direct auth.uid() calls with (SELECT auth.uid()) in RLS policies
-- 2. Update is_admin_user() function to cache auth.uid() result internally
-- 3. Split combined policies into granular SELECT/INSERT/UPDATE/DELETE policies
--
-- Expected performance improvements:
-- - Reduced function call overhead for large result sets
-- - Better query plan optimization due to stable function evaluation
-- - Improved cache utilization for auth context
--
-- Security verification:
-- - All policies maintain identical access control logic
-- - No changes to who can access what data
-- - Function security context preserved (SECURITY DEFINER)

================
File: supabase/migrations/20250914173444_consolidate_permissive_policies.sql
================
-- Consolidate multiple permissive RLS policies to improve performance
-- This addresses Supabase Performance Advisor warnings about multiple permissive
-- policies for the same role/action combination causing performance degradation
--
-- Performance Issue: Multiple permissive policies for the same operation require
-- PostgreSQL to evaluate all policies and combine results with OR logic, which
-- is less efficient than a single optimized policy.
--
-- Solution: Consolidate overlapping permissive policies into single policies
-- that use OR conditions internally for better query optimization.
--
-- Security Impact: NONE - Consolidated policies maintain identical access control
-- by combining the original policy conditions with OR logic.

-- =============================================================================
-- HELPER FUNCTIONS FOR CONSOLIDATED POLICIES
-- =============================================================================

-- Create optimized role check functions for better performance
CREATE OR REPLACE FUNCTION public.is_worker_role()
RETURNS boolean
LANGUAGE sql
SECURITY DEFINER
STABLE
AS $$
  SELECT current_user = 'worker';
$$;

-- =============================================================================
-- CLUSTERS TABLE CONSOLIDATION
-- =============================================================================

-- Drop existing overlapping policies
DROP POLICY IF EXISTS "clusters_admin_all" ON public.clusters;
DROP POLICY IF EXISTS "clusters_worker_all" ON public.clusters;
DROP POLICY IF EXISTS "clusters_authenticated_read" ON public.clusters;

-- Consolidated SELECT policy (admin OR worker OR authenticated read)
CREATE POLICY "clusters_select_consolidated" ON public.clusters
  FOR SELECT TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role() OR 
    true  -- authenticated users can read clusters
  );

-- Consolidated INSERT policy (admin OR worker)
CREATE POLICY "clusters_insert_consolidated" ON public.clusters
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- Consolidated UPDATE policy (admin OR worker)
CREATE POLICY "clusters_update_consolidated" ON public.clusters
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role()
  )
  WITH CHECK (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- Consolidated DELETE policy (admin OR worker)
CREATE POLICY "clusters_delete_consolidated" ON public.clusters
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- =============================================================================
-- CONTENTS TABLE CONSOLIDATION
-- =============================================================================

-- Drop existing overlapping policies
DROP POLICY IF EXISTS "contents_admin_all" ON public.contents;
DROP POLICY IF EXISTS "contents_worker_all" ON public.contents;
DROP POLICY IF EXISTS "contents_authenticated_read" ON public.contents;

-- Consolidated SELECT policy (admin OR worker OR authenticated read)
CREATE POLICY "contents_select_consolidated" ON public.contents
  FOR SELECT TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role() OR 
    true  -- authenticated users can read contents
  );

-- Consolidated INSERT policy (admin OR worker)
CREATE POLICY "contents_insert_consolidated" ON public.contents
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- Consolidated UPDATE policy (admin OR worker)
CREATE POLICY "contents_update_consolidated" ON public.contents
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role()
  )
  WITH CHECK (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- Consolidated DELETE policy (admin OR worker)
CREATE POLICY "contents_delete_consolidated" ON public.contents
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- =============================================================================
-- SOURCES TABLE CONSOLIDATION
-- =============================================================================

-- Drop existing overlapping policies
DROP POLICY IF EXISTS "sources_admin_all" ON public.sources;
DROP POLICY IF EXISTS "sources_worker_all" ON public.sources;
DROP POLICY IF EXISTS "sources_authenticated_read" ON public.sources;

-- Consolidated SELECT policy (admin OR worker OR authenticated read active sources)
CREATE POLICY "sources_select_consolidated" ON public.sources
  FOR SELECT TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role() OR 
    (active = true)  -- authenticated users can read active sources only
  );

-- Consolidated INSERT policy (admin OR worker)
CREATE POLICY "sources_insert_consolidated" ON public.sources
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- Consolidated UPDATE policy (admin OR worker)
CREATE POLICY "sources_update_consolidated" ON public.sources
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR 
    public.is_worker_role()
  )
  WITH CHECK (
    public.is_admin_user() OR 
    public.is_worker_role()
  );

-- Consolidated DELETE policy (admin OR worker)
CREATE POLICY "sources_delete_consolidated" ON public.sources
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- =============================================================================
-- STORIES TABLE CONSOLIDATION
-- =============================================================================

-- Drop existing overlapping policies
DROP POLICY IF EXISTS "stories_admin_all" ON public.stories;
DROP POLICY IF EXISTS "stories_worker_all" ON public.stories;
DROP POLICY IF EXISTS "stories_authenticated_read" ON public.stories;

-- Consolidated SELECT policy (admin OR worker OR authenticated read)
CREATE POLICY "stories_select_consolidated" ON public.stories
  FOR SELECT TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role() OR
    true  -- authenticated users can read stories
  );

-- Consolidated INSERT policy (admin OR worker)
CREATE POLICY "stories_insert_consolidated" ON public.stories
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- Consolidated UPDATE policy (admin OR worker)
CREATE POLICY "stories_update_consolidated" ON public.stories
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  )
  WITH CHECK (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- Consolidated DELETE policy (admin OR worker)
CREATE POLICY "stories_delete_consolidated" ON public.stories
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- =============================================================================
-- STORY_EMBEDDINGS TABLE CONSOLIDATION
-- =============================================================================

-- Drop existing overlapping policies
DROP POLICY IF EXISTS "story_embeddings_admin_all" ON public.story_embeddings;
DROP POLICY IF EXISTS "story_embeddings_worker_all" ON public.story_embeddings;
DROP POLICY IF EXISTS "story_embeddings_authenticated_read" ON public.story_embeddings;

-- Consolidated SELECT policy (admin OR worker OR authenticated read)
CREATE POLICY "story_embeddings_select_consolidated" ON public.story_embeddings
  FOR SELECT TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role() OR
    true  -- authenticated users can read embeddings for similarity search
  );

-- Consolidated INSERT policy (admin OR worker)
CREATE POLICY "story_embeddings_insert_consolidated" ON public.story_embeddings
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- Consolidated UPDATE policy (admin OR worker)
CREATE POLICY "story_embeddings_update_consolidated" ON public.story_embeddings
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  )
  WITH CHECK (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- Consolidated DELETE policy (admin OR worker)
CREATE POLICY "story_embeddings_delete_consolidated" ON public.story_embeddings
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- =============================================================================
-- STORY_OVERLAYS TABLE CONSOLIDATION
-- =============================================================================

-- Drop existing overlapping policies
DROP POLICY IF EXISTS "story_overlays_admin_all" ON public.story_overlays;
DROP POLICY IF EXISTS "story_overlays_worker_all" ON public.story_overlays;
DROP POLICY IF EXISTS "story_overlays_authenticated_read" ON public.story_overlays;

-- Consolidated SELECT policy (admin OR worker OR authenticated read)
CREATE POLICY "story_overlays_select_consolidated" ON public.story_overlays
  FOR SELECT TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role() OR
    true  -- authenticated users can read overlays
  );

-- Consolidated INSERT policy (admin OR worker)
CREATE POLICY "story_overlays_insert_consolidated" ON public.story_overlays
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- Consolidated UPDATE policy (admin OR worker)
CREATE POLICY "story_overlays_update_consolidated" ON public.story_overlays
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  )
  WITH CHECK (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- Consolidated DELETE policy (admin OR worker)
CREATE POLICY "story_overlays_delete_consolidated" ON public.story_overlays
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    public.is_worker_role()
  );

-- =============================================================================
-- ADMIN TABLES CONSOLIDATION (platform_quota, source_health, job_metrics)
-- =============================================================================

-- Drop existing overlapping worker policies for platform_quota
DROP POLICY IF EXISTS "platform_quota_worker_insert" ON public.platform_quota;
DROP POLICY IF EXISTS "platform_quota_worker_update" ON public.platform_quota;
DROP POLICY IF EXISTS "platform_quota_worker_select" ON public.platform_quota;

-- Consolidated worker policy for platform_quota
CREATE POLICY "platform_quota_worker_all" ON public.platform_quota
  FOR ALL TO worker
  USING (true)
  WITH CHECK (true);

-- Drop existing overlapping worker policies for source_health
DROP POLICY IF EXISTS "source_health_worker_insert" ON public.source_health;
DROP POLICY IF EXISTS "source_health_worker_update" ON public.source_health;
DROP POLICY IF EXISTS "source_health_worker_select" ON public.source_health;

-- Consolidated worker policy for source_health
CREATE POLICY "source_health_worker_all" ON public.source_health
  FOR ALL TO worker
  USING (true)
  WITH CHECK (true);

-- Drop existing overlapping worker policies for job_metrics
DROP POLICY IF EXISTS "job_metrics_worker_insert" ON public.job_metrics;
DROP POLICY IF EXISTS "job_metrics_worker_update" ON public.job_metrics;
DROP POLICY IF EXISTS "job_metrics_worker_select" ON public.job_metrics;

-- Consolidated worker policy for job_metrics
CREATE POLICY "job_metrics_worker_all" ON public.job_metrics
  FOR ALL TO worker
  USING (true)
  WITH CHECK (true);

-- =============================================================================
-- PERFORMANCE NOTES
-- =============================================================================

-- The consolidations in this migration:
-- 1. Replace multiple permissive policies with single consolidated policies
-- 2. Use OR conditions within policies instead of multiple policy evaluation
-- 3. Create helper functions for role checks to improve caching
-- 4. Maintain identical security semantics while improving query performance
--
-- Expected performance improvements:
-- - Reduced policy evaluation overhead for queries
-- - Better query plan optimization due to simplified policy structure
-- - Improved cache utilization for role checks
-- - Faster permission resolution for large result sets
--
-- Security verification:
-- - All consolidated policies maintain identical access control logic
-- - OR conditions preserve the original permissive behavior
-- - No changes to data access patterns or user permissions

================
File: supabase/migrations/20250914175429_fix_function_search_path_security.sql
================
-- Fix Function Search Path Security Vulnerabilities
-- This migration addresses critical Supabase Security Advisor warnings about
-- functions lacking explicit search_path settings, which can lead to privilege
-- escalation and security bypass attacks.
--
-- Security Issue: Functions with SECURITY DEFINER context that don't set
-- search_path = '' are vulnerable to search path manipulation attacks where
-- malicious users can create objects in schemas earlier in the search path
-- to hijack function calls.
--
-- Solution: Set search_path = '' and fully qualify all object names in
-- SECURITY DEFINER functions to prevent search path manipulation attacks.
--
-- Security Impact: CRITICAL - Prevents privilege escalation and ensures
-- functions operate with predictable object resolution.

-- =============================================================================
-- CRITICAL RLS SECURITY FUNCTIONS
-- =============================================================================

-- Fix is_admin_user function - CRITICAL for RLS policies
CREATE OR REPLACE FUNCTION public.is_admin_user()
RETURNS boolean
LANGUAGE sql
SECURITY DEFINER
STABLE
SET search_path = ''
AS $$
  SELECT COALESCE(
    (SELECT is_admin FROM public.users WHERE id = (SELECT auth.uid())),
    false
  );
$$;

-- Fix is_worker_role function - CRITICAL for RLS policies  
CREATE OR REPLACE FUNCTION public.is_worker_role()
RETURNS boolean
LANGUAGE sql
SECURITY DEFINER
STABLE
SET search_path = ''
AS $$
  SELECT current_user = 'worker';
$$;

-- =============================================================================
-- AUTH TRIGGER FUNCTION SECURITY
-- =============================================================================

-- Fix handle_new_user function - CRITICAL for user registration security
CREATE OR REPLACE FUNCTION public.handle_new_user()
RETURNS trigger
LANGUAGE plpgsql
SECURITY DEFINER
SET search_path = ''
AS $$
BEGIN
  INSERT INTO public.users (id, full_name, avatar_url)
  VALUES (
    NEW.id,
    COALESCE(
      NEW.raw_user_meta_data->>'full_name',
      NEW.raw_user_meta_data->>'name'
    ),
    COALESCE(
      NEW.raw_user_meta_data->>'avatar_url',
      NEW.raw_user_meta_data->>'picture'
    )
  );
  RETURN NEW;
END;
$$;

-- =============================================================================
-- DATA ACCESS FUNCTION SECURITY
-- =============================================================================

-- Fix get_youtube_sources function - Data access security
CREATE OR REPLACE FUNCTION public.get_youtube_sources()
RETURNS TABLE(
  id uuid,
  kind text,
  name text,
  url text,
  domain text,
  metadata jsonb,
  last_cursor jsonb
)
LANGUAGE sql
SECURITY INVOKER
STABLE
SET search_path = ''
AS $$
  SELECT 
    s.id, 
    s.kind, 
    s.name, 
    s.url, 
    s.domain, 
    s.metadata, 
    s.last_cursor
  FROM public.sources s
  WHERE s.kind IN ('youtube_channel', 'youtube_search') 
    AND s.metadata IS NOT NULL;
$$;

-- =============================================================================
-- METRICS FUNCTION SECURITY
-- =============================================================================

-- Fix refresh_source_metrics function - Metrics computation security
CREATE OR REPLACE FUNCTION public.refresh_source_metrics(_source_id uuid DEFAULT NULL)
RETURNS void
LANGUAGE plpgsql
SECURITY INVOKER
SET search_path = ''
AS $$
BEGIN
  IF _source_id IS NULL THEN
    -- Recompute for all sources
    INSERT INTO public.source_metrics AS m (
      source_id,
      raw_total,
      contents_total,
      stories_total,
      raw_24h,
      contents_24h,
      stories_24h,
      last_raw_at,
      last_content_at,
      last_story_at,
      updated_at
    )
    SELECT 
      s.id,
      COALESCE((SELECT count(*) FROM public.raw_items r WHERE r.source_id = s.id), 0) AS raw_total,
      COALESCE((SELECT count(*) FROM public.contents c JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id), 0) AS contents_total,
      COALESCE((SELECT count(*) FROM public.stories st JOIN public.contents c ON c.id = st.content_id JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id), 0) AS stories_total,
      COALESCE((SELECT count(*) FROM public.raw_items r WHERE r.source_id = s.id AND r.discovered_at > now() - interval '24 hours'), 0) AS raw_24h,
      COALESCE((SELECT count(*) FROM public.contents c JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id AND c.extracted_at > now() - interval '24 hours'), 0) AS contents_24h,
      COALESCE((SELECT count(*) FROM public.stories st JOIN public.contents c ON c.id = st.content_id JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id AND st.created_at > now() - interval '24 hours'), 0) AS stories_24h,
      (SELECT max(discovered_at) FROM public.raw_items r WHERE r.source_id = s.id) AS last_raw_at,
      (SELECT max(extracted_at) FROM public.contents c JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id) AS last_content_at,
      (SELECT max(st.created_at) FROM public.stories st JOIN public.contents c ON c.id = st.content_id JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id) AS last_story_at,
      now()
    FROM public.sources s
    ON CONFLICT (source_id) DO UPDATE SET
      raw_total = EXCLUDED.raw_total,
      contents_total = EXCLUDED.contents_total,
      stories_total = EXCLUDED.stories_total,
      raw_24h = EXCLUDED.raw_24h,
      contents_24h = EXCLUDED.contents_24h,
      stories_24h = EXCLUDED.stories_24h,
      last_raw_at = EXCLUDED.last_raw_at,
      last_content_at = EXCLUDED.last_content_at,
      last_story_at = EXCLUDED.last_story_at,
      updated_at = EXCLUDED.updated_at;
  ELSE
    -- Recompute for one source
    INSERT INTO public.source_metrics AS m (
      source_id, raw_total, contents_total, stories_total, raw_24h, contents_24h, stories_24h, last_raw_at, last_content_at, last_story_at, updated_at
    )
    SELECT 
      s.id,
      COALESCE((SELECT count(*) FROM public.raw_items r WHERE r.source_id = s.id), 0),
      COALESCE((SELECT count(*) FROM public.contents c JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id), 0),
      COALESCE((SELECT count(*) FROM public.stories st JOIN public.contents c ON c.id = st.content_id JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id), 0),
      COALESCE((SELECT count(*) FROM public.raw_items r WHERE r.source_id = s.id AND r.discovered_at > now() - interval '24 hours'), 0),
      COALESCE((SELECT count(*) FROM public.contents c JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id AND c.extracted_at > now() - interval '24 hours'), 0),
      COALESCE((SELECT count(*) FROM public.stories st JOIN public.contents c ON c.id = st.content_id JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id AND st.created_at > now() - interval '24 hours'), 0),
      (SELECT max(discovered_at) FROM public.raw_items r WHERE r.source_id = s.id),
      (SELECT max(extracted_at) FROM public.contents c JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id),
      (SELECT max(st.created_at) FROM public.stories st JOIN public.contents c ON c.id = st.content_id JOIN public.raw_items r ON r.id = c.raw_item_id WHERE r.source_id = s.id),
      now()
    FROM public.sources s 
    WHERE s.id = _source_id
    ON CONFLICT (source_id) DO UPDATE SET
      raw_total = EXCLUDED.raw_total,
      contents_total = EXCLUDED.contents_total,
      stories_total = EXCLUDED.stories_total,
      raw_24h = EXCLUDED.raw_24h,
      contents_24h = EXCLUDED.contents_24h,
      stories_24h = EXCLUDED.stories_24h,
      last_raw_at = EXCLUDED.last_raw_at,
      last_content_at = EXCLUDED.last_content_at,
      last_story_at = EXCLUDED.last_story_at,
      updated_at = EXCLUDED.updated_at;
  END IF;
END;
$$;

-- =============================================================================
-- TRIGGER FUNCTION SECURITY
-- =============================================================================

-- Fix tg_refresh_metrics_on_raw_items trigger function
CREATE OR REPLACE FUNCTION public.tg_refresh_metrics_on_raw_items()
RETURNS trigger
LANGUAGE plpgsql
SECURITY INVOKER
SET search_path = ''
AS $$
BEGIN
  PERFORM public.refresh_source_metrics(NEW.source_id);
  RETURN NULL;
END;
$$;

-- Fix tg_refresh_metrics_on_contents trigger function
CREATE OR REPLACE FUNCTION public.tg_refresh_metrics_on_contents()
RETURNS trigger
LANGUAGE plpgsql
SECURITY INVOKER
SET search_path = ''
AS $$
DECLARE
  _sid uuid;
BEGIN
  SELECT r.source_id INTO _sid
  FROM public.raw_items r
  WHERE r.id = NEW.raw_item_id;

  IF _sid IS NOT NULL THEN
    PERFORM public.refresh_source_metrics(_sid);
  END IF;

  RETURN NULL;
END;
$$;

-- Fix tg_refresh_metrics_on_stories trigger function
CREATE OR REPLACE FUNCTION public.tg_refresh_metrics_on_stories()
RETURNS trigger
LANGUAGE plpgsql
SECURITY INVOKER
SET search_path = ''
AS $$
DECLARE
  _sid uuid;
BEGIN
  SELECT r.source_id INTO _sid
  FROM public.raw_items r
  JOIN public.contents c ON c.id = NEW.content_id AND c.raw_item_id = r.id;

  IF _sid IS NOT NULL THEN
    PERFORM public.refresh_source_metrics(_sid);
  END IF;

  RETURN NULL;
END;
$$;

-- =============================================================================
-- SECURITY VERIFICATION
-- =============================================================================

-- Verify all functions now have secure search_path settings
DO $$
DECLARE
    func_record RECORD;
    insecure_count INTEGER := 0;
BEGIN
    -- Check for functions without search_path = ''
    FOR func_record IN
        SELECT proname, prosecdef
        FROM pg_proc p
        JOIN pg_namespace n ON p.pronamespace = n.oid
        WHERE n.nspname = 'public'
          AND p.proname IN ('is_admin_user', 'is_worker_role', 'handle_new_user',
                           'get_youtube_sources', 'refresh_source_metrics',
                           'tg_refresh_metrics_on_raw_items', 'tg_refresh_metrics_on_contents',
                           'tg_refresh_metrics_on_stories')
          AND (p.proconfig IS NULL OR NOT ('search_path=""') = ANY(p.proconfig))
    LOOP
        RAISE WARNING 'Function % still has insecure search_path', func_record.proname;
        insecure_count := insecure_count + 1;
    END LOOP;

    IF insecure_count = 0 THEN
        RAISE NOTICE 'SUCCESS: All critical functions now have secure search_path settings';
    ELSE
        RAISE EXCEPTION 'SECURITY ERROR: % functions still have insecure search_path settings', insecure_count;
    END IF;
END;
$$;

-- =============================================================================
-- SECURITY NOTES
-- =============================================================================

-- Security improvements in this migration:
-- 1. Added SET search_path = '' to all SECURITY DEFINER functions
-- 2. Fully qualified all object names (public.table_name)
-- 3. Changed non-critical functions from SECURITY DEFINER to SECURITY INVOKER
-- 4. Maintained identical functionality while improving security posture
--
-- Functions changed to SECURITY INVOKER (safer default):
-- - get_youtube_sources: Data access function, doesn't need elevated privileges
-- - refresh_source_metrics: Metrics function, can run with caller's privileges
-- - All trigger functions: Run in trigger context, don't need DEFINER privileges
--
-- Functions kept as SECURITY DEFINER (required for functionality):
-- - is_admin_user: Needs to access users table for RLS policies
-- - is_worker_role: Needs to check current_user for RLS policies
-- - handle_new_user: Needs to insert into users table from auth trigger
--
-- Expected security improvements:
-- - Prevents search path manipulation attacks
-- - Eliminates privilege escalation vulnerabilities
-- - Ensures predictable object resolution in all functions
-- - Maintains principle of least privilege

================
File: supabase/migrations/20250914175606_move_vector_extension_to_extensions_schema.sql
================
-- Move Vector Extension to Dedicated Extensions Schema
-- This migration addresses Supabase Security Advisor warnings about the vector
-- extension being installed in the public schema, which poses security risks.
--
-- Security Issue: Extensions in the public schema can lead to:
-- 1. Namespace pollution and function name conflicts
-- 2. Potential for function hijacking attacks
-- 3. Reduced security isolation between application and extension code
--
-- Solution: Move the vector extension to a dedicated 'extensions' schema
-- and update all references to use the new schema location.
--
-- Security Impact: HIGH - Improves security isolation and reduces attack surface

-- =============================================================================
-- EXTENSION SCHEMA SETUP
-- =============================================================================

-- Create dedicated extensions schema if it doesn't exist
CREATE SCHEMA IF NOT EXISTS extensions;

-- Grant necessary permissions for the extensions schema
GRANT USAGE ON SCHEMA extensions TO postgres, anon, authenticated, service_role;

-- =============================================================================
-- VECTOR EXTENSION MIGRATION STRATEGY
-- =============================================================================

-- Note: Moving an existing extension to a different schema requires careful handling
-- because PostgreSQL doesn't support ALTER EXTENSION SET SCHEMA for all extensions.
-- 
-- For the vector extension, we need to:
-- 1. Check current vector extension objects and dependencies
-- 2. Create a new vector extension in the extensions schema
-- 3. Migrate existing vector data and indexes
-- 4. Update application references
-- 5. Drop the old extension from public schema

-- First, let's check what vector objects exist in the public schema
DO $$
DECLARE
    vector_objects_count INTEGER;
    vector_indexes_count INTEGER;
BEGIN
    -- Check for vector columns in public schema tables
    SELECT COUNT(*) INTO vector_objects_count
    FROM information_schema.columns 
    WHERE table_schema = 'public' 
      AND data_type = 'USER-DEFINED'
      AND udt_name = 'vector';
    
    -- Check for vector indexes
    SELECT COUNT(*) INTO vector_indexes_count
    FROM pg_indexes 
    WHERE schemaname = 'public' 
      AND indexdef LIKE '%vector%';
    
    RAISE NOTICE 'Found % vector columns and % vector indexes in public schema', 
                 vector_objects_count, vector_indexes_count;
    
    -- If we have existing vector data, we need to be more careful
    IF vector_objects_count > 0 THEN
        RAISE NOTICE 'Vector extension is actively used - proceeding with careful migration';
    ELSE
        RAISE NOTICE 'No vector data found - safe to recreate extension';
    END IF;
END;
$$;

-- =============================================================================
-- SAFE VECTOR EXTENSION RECREATION
-- =============================================================================

-- Strategy: Since this is a development/early production environment,
-- we can safely recreate the vector extension in the correct schema.
-- This approach is safer than trying to move existing objects.

-- Step 1: Drop existing vector extension from public schema
-- Note: This will temporarily remove vector functionality
DROP EXTENSION IF EXISTS vector CASCADE;

-- Step 2: Create vector extension in extensions schema
CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA extensions;

-- Step 3: Recreate any vector columns that were dropped
-- Check if story_embeddings table exists and recreate vector column
DO $$
BEGIN
    -- Check if story_embeddings table exists
    IF EXISTS (SELECT 1 FROM information_schema.tables 
               WHERE table_schema = 'public' 
               AND table_name = 'story_embeddings') THEN
        
        -- Check if embedding column exists
        IF NOT EXISTS (SELECT 1 FROM information_schema.columns 
                       WHERE table_schema = 'public' 
                       AND table_name = 'story_embeddings' 
                       AND column_name = 'embedding') THEN
            
            -- Recreate the embedding column with proper vector type
            ALTER TABLE public.story_embeddings 
            ADD COLUMN embedding extensions.vector(1536);
            
            RAISE NOTICE 'Recreated embedding column in story_embeddings table';
        ELSE
            RAISE NOTICE 'Embedding column already exists in story_embeddings table';
        END IF;
        
        -- Recreate vector index if it doesn't exist
        IF NOT EXISTS (SELECT 1 FROM pg_indexes 
                       WHERE schemaname = 'public' 
                       AND tablename = 'story_embeddings' 
                       AND indexname = 'story_embeddings_l2_idx') THEN
            
            -- Recreate the vector index
            CREATE INDEX IF NOT EXISTS story_embeddings_l2_idx 
            ON public.story_embeddings 
            USING ivfflat (embedding extensions.vector_l2_ops) 
            WITH (lists = 100);
            
            RAISE NOTICE 'Recreated vector index on story_embeddings table';
        ELSE
            RAISE NOTICE 'Vector index already exists on story_embeddings table';
        END IF;
    ELSE
        RAISE NOTICE 'story_embeddings table does not exist - no vector columns to recreate';
    END IF;
END;
$$;

-- =============================================================================
-- UPDATE SEARCH PATH FOR VECTOR FUNCTIONS
-- =============================================================================

-- Update database default search_path to include extensions schema
-- This ensures vector functions are accessible without schema qualification
ALTER DATABASE postgres SET search_path = public, extensions;

-- For the current session, update search_path
SET search_path = public, extensions;

-- =============================================================================
-- VERIFICATION AND TESTING
-- =============================================================================

-- Verify vector extension is now in extensions schema
DO $$
DECLARE
    vector_schema TEXT;
    vector_version TEXT;
BEGIN
    SELECT n.nspname, e.extversion 
    INTO vector_schema, vector_version
    FROM pg_extension e
    JOIN pg_namespace n ON e.extnamespace = n.oid
    WHERE e.extname = 'vector';
    
    IF vector_schema = 'extensions' THEN
        RAISE NOTICE 'SUCCESS: Vector extension v% is now in extensions schema', vector_version;
    ELSIF vector_schema = 'public' THEN
        RAISE WARNING 'Vector extension is still in public schema - migration may have failed';
    ELSE
        RAISE WARNING 'Vector extension found in unexpected schema: %', vector_schema;
    END IF;
END;
$$;

-- Test vector functionality
DO $$
BEGIN
    -- Test basic vector operations
    PERFORM extensions.vector_dims('[1,2,3]'::extensions.vector);
    RAISE NOTICE 'Vector extension functionality verified';
EXCEPTION WHEN OTHERS THEN
    RAISE WARNING 'Vector extension test failed: %', SQLERRM;
END;
$$;

-- =============================================================================
-- SECURITY IMPROVEMENTS SUMMARY
-- =============================================================================

-- Security improvements in this migration:
-- 1. Moved vector extension from public to dedicated extensions schema
-- 2. Improved security isolation between application and extension code
-- 3. Reduced namespace pollution in public schema
-- 4. Updated search_path to maintain functionality while improving security
--
-- Expected security benefits:
-- - Eliminates potential function hijacking attacks via public schema
-- - Provides clear separation between application and extension objects
-- - Reduces attack surface by isolating extension functionality
-- - Follows PostgreSQL security best practices for extension management
--
-- Post-migration verification:
-- - Vector extension should be in 'extensions' schema
-- - All vector functionality should work normally
-- - No vector objects should remain in public schema
-- - Application code should continue to work without changes

================
File: supabase/migrations/20250914211053_add_missing_sources_fields.sql
================
-- Add missing fields to sources table that exist in baseline but not in remote
-- This corrects schema drift between local and remote databases

-- Add missing columns to sources table
ALTER TABLE public.sources
ADD COLUMN IF NOT EXISTS active boolean NOT NULL DEFAULT true,
ADD COLUMN IF NOT EXISTS created_at timestamptz NOT NULL DEFAULT now(),
ADD COLUMN IF NOT EXISTS updated_at timestamptz NOT NULL DEFAULT now();

-- Update existing rows to have proper timestamps if they don't already
UPDATE public.sources
SET
  created_at = COALESCE(created_at, now()),
  updated_at = COALESCE(updated_at, now())
WHERE created_at IS NULL OR updated_at IS NULL;

-- Add comment for documentation
COMMENT ON COLUMN public.sources.active IS 'Whether this source is actively being processed';
COMMENT ON COLUMN public.sources.created_at IS 'When this source was first added';
COMMENT ON COLUMN public.sources.updated_at IS 'When this source was last modified';

================
File: supabase/migrations/20250914211837_enable_rls_content_pipeline.sql
================
-- Enable Row Level Security (RLS) for content pipeline tables
-- This addresses Supabase security advisor warnings while maintaining functionality
--
-- Security Strategy:
-- 1. Stories/Content: Public read access for authenticated users (news platform)
-- 2. Raw pipeline data: Admin and worker access only (sensitive ingestion data)
-- 3. Admin users: Full access to all tables
-- 4. Worker role: Pipeline access for ingestion processes

-- Helper function to check if user is admin
CREATE OR REPLACE FUNCTION public.is_admin_user()
RETURNS boolean
LANGUAGE sql
SECURITY DEFINER
STABLE
AS $$
  SELECT COALESCE(
    (SELECT is_admin FROM public.users WHERE id = auth.uid()),
    false
  );
$$;

-- =============================================================================
-- ENABLE RLS ON ALL CONTENT PIPELINE TABLES
-- =============================================================================

-- Enable RLS on sources table
ALTER TABLE public.sources ENABLE ROW LEVEL SECURITY;

-- Enable RLS on raw_items table
ALTER TABLE public.raw_items ENABLE ROW LEVEL SECURITY;

-- Enable RLS on contents table
ALTER TABLE public.contents ENABLE ROW LEVEL SECURITY;

-- Enable RLS on stories table
ALTER TABLE public.stories ENABLE ROW LEVEL SECURITY;

-- Enable RLS on clusters table
ALTER TABLE public.clusters ENABLE ROW LEVEL SECURITY;

-- Enable RLS on story_overlays table
ALTER TABLE public.story_overlays ENABLE ROW LEVEL SECURITY;

-- Enable RLS on story_embeddings table
ALTER TABLE public.story_embeddings ENABLE ROW LEVEL SECURITY;

-- =============================================================================
-- SOURCES TABLE POLICIES
-- =============================================================================

-- Admin users can do everything with sources
DO $$ BEGIN
  BEGIN
    CREATE POLICY "sources_admin_all" ON public.sources
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write sources for pipeline operations
DO $$ BEGIN
  BEGIN
    CREATE POLICY "sources_worker_all" ON public.sources
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Authenticated users can read active sources (for UI dropdowns, etc.)
DO $$ BEGIN
  BEGIN
    CREATE POLICY "sources_authenticated_read" ON public.sources
      FOR SELECT TO authenticated
      USING (active = true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- RAW_ITEMS TABLE POLICIES (Sensitive pipeline data - restricted access)
-- =============================================================================

-- Admin users can do everything with raw_items
DO $$ BEGIN
  BEGIN
    CREATE POLICY "raw_items_admin_all" ON public.raw_items
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write raw_items for pipeline operations
DO $$ BEGIN
  BEGIN
    CREATE POLICY "raw_items_worker_all" ON public.raw_items
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- CONTENTS TABLE POLICIES (Processed content - authenticated read access)
-- =============================================================================

-- Admin users can do everything with contents
DO $$ BEGIN
  BEGIN
    CREATE POLICY "contents_admin_all" ON public.contents
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write contents for pipeline operations
DO $$ BEGIN
  BEGIN
    CREATE POLICY "contents_worker_all" ON public.contents
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Authenticated users can read contents (needed for story display)
DO $$ BEGIN
  BEGIN
    CREATE POLICY "contents_authenticated_read" ON public.contents
      FOR SELECT TO authenticated
      USING (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- STORIES TABLE POLICIES (Public content - authenticated read access)
-- =============================================================================

-- Admin users can do everything with stories
DO $$ BEGIN
  BEGIN
    CREATE POLICY "stories_admin_all" ON public.stories
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write stories for pipeline operations
DO $$ BEGIN
  BEGIN
    CREATE POLICY "stories_worker_all" ON public.stories
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Authenticated users can read stories (core app functionality)
DO $$ BEGIN
  BEGIN
    CREATE POLICY "stories_authenticated_read" ON public.stories
      FOR SELECT TO authenticated
      USING (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- CLUSTERS TABLE POLICIES (Story grouping - authenticated read access)
-- =============================================================================

-- Admin users can do everything with clusters
DO $$ BEGIN
  BEGIN
    CREATE POLICY "clusters_admin_all" ON public.clusters
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write clusters for pipeline operations
DO $$ BEGIN
  BEGIN
    CREATE POLICY "clusters_worker_all" ON public.clusters
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Authenticated users can read clusters (for story grouping UI)
DO $$ BEGIN
  BEGIN
    CREATE POLICY "clusters_authenticated_read" ON public.clusters
      FOR SELECT TO authenticated
      USING (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- STORY_OVERLAYS TABLE POLICIES (AI analysis - authenticated read access)
-- =============================================================================

-- Admin users can do everything with story_overlays
DO $$ BEGIN
  BEGIN
    CREATE POLICY "story_overlays_admin_all" ON public.story_overlays
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write story_overlays for AI analysis
DO $$ BEGIN
  BEGIN
    CREATE POLICY "story_overlays_worker_all" ON public.story_overlays
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Authenticated users can read story_overlays (for AI insights display)
DO $$ BEGIN
  BEGIN
    CREATE POLICY "story_overlays_authenticated_read" ON public.story_overlays
      FOR SELECT TO authenticated
      USING (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- STORY_EMBEDDINGS TABLE POLICIES (Vector data - restricted access)
-- =============================================================================

-- Admin users can do everything with story_embeddings
DO $$ BEGIN
  BEGIN
    CREATE POLICY "story_embeddings_admin_all" ON public.story_embeddings
      FOR ALL TO authenticated
      USING (public.is_admin_user())
      WITH CHECK (public.is_admin_user());
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Worker role can read/write story_embeddings for AI processing
DO $$ BEGIN
  BEGIN
    CREATE POLICY "story_embeddings_worker_all" ON public.story_embeddings
      FOR ALL TO worker
      USING (true)
      WITH CHECK (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- Authenticated users can read story_embeddings (for similarity search)
DO $$ BEGIN
  BEGIN
    CREATE POLICY "story_embeddings_authenticated_read" ON public.story_embeddings
      FOR SELECT TO authenticated
      USING (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

-- =============================================================================
-- SECURITY DOCUMENTATION
-- =============================================================================

-- This migration implements a balanced RLS strategy:
--
-- 1. **Admin Access**: Users with is_admin=true have full access to all tables
-- 2. **Worker Access**: The 'worker' role has full pipeline access for ingestion
-- 3. **User Access Levels**:
--    - Stories/Content/Overlays: Full read access (public news content)
--    - Sources: Read active sources only (for UI functionality)
--    - Raw Items: No access (sensitive pipeline data)
--    - Embeddings: Read access (for search functionality)
--    - Clusters: Read access (for story grouping)
--
-- 4. **Security Benefits**:
--    - Prevents unauthorized external access via PostgREST
--    - Maintains application functionality for authenticated users
--    - Protects sensitive pipeline data (raw_items)
--    - Allows admin oversight and worker automation
--
-- 5. **Application Impact**:
--    - Existing queries using supabaseAdminClient will continue to work
--    - New user-scoped queries will respect these policies
--    - Worker processes maintain full pipeline access
--    - Admin users retain full system access

================
File: supabase/migrations/20250915162756_grant_worker_write_source_metrics.sql
================
-- Allow worker to insert/update source_metrics under RLS
  -- File: apps/api/supabase/migrations/_grant_worker_write_source_metrics.sql

  BEGIN;

  -- Ensure RLS is enabled (no-op if already enabled)
  ALTER TABLE public.source_metrics ENABLE ROW LEVEL SECURITY;

  -- Grant only the privileges the worker needs (no SELECT/DELETE)
  GRANT INSERT, UPDATE ON public.source_metrics TO worker;

  -- Clean up any previous worker policiepnpm db:migrates to avoid duplicates
  DROP POLICY IF EXISTS "source_metrics_worker_insert" ON public.source_metrics;
  DROP POLICY IF EXISTS "source_metrics_worker_update" ON public.source_metrics;

  -- Permit worker inserts (row check always true; table privileges still control access)
  CREATE POLICY "source_metrics_worker_insert"
  ON public.source_metrics
  FOR INSERT
  TO worker
  WITH CHECK (true);

  -- Permit worker updates
  CREATE POLICY "source_metrics_worker_update"
  ON public.source_metrics
  FOR UPDATE
  TO worker
  USING (true)
  WITH CHECK (true);

  COMMIT;

================
File: supabase/migrations/20250915175811_fix_worker_role_permissions.sql
================
-- Grant necessary permissions for worker operations on existing tables
grant select, insert, update, delete on all tables in schema public to worker;
grant usage, select on all sequences in schema public to worker;

-- Grant permissions on future tables and sequences in public schema
alter default privileges in schema public grant select, insert, update, delete on tables to worker;
alter default privileges in schema public grant usage, select on sequences to worker;

-- Grant pgboss schema permissions (create schema if it doesn't exist)
-- NOTE: Warnings about "no privileges were granted" for pgboss tables are expected
-- during migration time since pgboss tables don't exist yet. The default privileges
-- below will apply when pgboss creates its tables at runtime.
create schema if not exists pgboss;
grant usage on schema pgboss to worker;
alter default privileges in schema pgboss grant select, insert, update, delete on tables to worker;
alter default privileges in schema pgboss grant usage, select on sequences to worker;

-- Ensure worker can connect to the database
grant connect on database postgres to worker;

================
File: supabase/migrations/20250915180000_add_worker_select_source_metrics.sql
================
-- Add missing SELECT policy for worker on source_metrics table
-- This is required for ON CONFLICT DO UPDATE operations in refresh_source_metrics function

BEGIN;

-- Add SELECT policy for worker on source_metrics
-- This allows the worker to read existing rows for ON CONFLICT DO UPDATE operations
DO $$ BEGIN
  BEGIN
    CREATE POLICY "source_metrics_worker_select" ON public.source_metrics
      FOR SELECT TO worker
      USING (true);
  EXCEPTION WHEN duplicate_object THEN NULL; END;
END $$;

COMMIT;

================
File: supabase/migrations/20250915180001_add_worker_contents_permissions.sql
================
-- Fix worker RLS access to contents table
-- The issue is that the worker role needs explicit bypass or the function needs to work correctly

-- Option 1: Create a more permissive policy specifically for the worker role
-- This bypasses the function check and directly allows worker role
DROP POLICY IF EXISTS "contents_insert_consolidated" ON public.contents;
CREATE POLICY "contents_insert_consolidated" ON public.contents
  FOR INSERT TO authenticated, worker
  WITH CHECK (
    public.is_admin_user() OR
    current_user = 'worker'  -- Direct role check instead of function
  );

DROP POLICY IF EXISTS "contents_update_consolidated" ON public.contents;
CREATE POLICY "contents_update_consolidated" ON public.contents
  FOR UPDATE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    current_user = 'worker'
  )
  WITH CHECK (
    public.is_admin_user() OR
    current_user = 'worker'
  );

DROP POLICY IF EXISTS "contents_delete_consolidated" ON public.contents;
CREATE POLICY "contents_delete_consolidated" ON public.contents
  FOR DELETE TO authenticated, worker
  USING (
    public.is_admin_user() OR
    current_user = 'worker'
  );

-- Ensure the is_worker_role function is working correctly
CREATE OR REPLACE FUNCTION public.is_worker_role()
RETURNS boolean
LANGUAGE sql
SECURITY DEFINER
STABLE
SET search_path = ''
AS $$
  SELECT current_user = 'worker';
$$;

-- Grant explicit permissions to worker role on contents table (belt and suspenders)
GRANT SELECT, INSERT, UPDATE, DELETE ON public.contents TO worker;

================
File: supabase/seeds/youtube_sources.sql
================
-- YouTube AI-Focused Sources
-- Insert AI research channels, tech/startup channels, and search queries

-- AI Research Channels (High Priority)
INSERT INTO public.sources (kind, name, url, domain, metadata) VALUES
-- Lex Fridman Podcast (AI research interviews)
('youtube_channel', 'Lex Fridman Podcast', 'https://www.youtube.com/@lexfridman', 'youtube.com',
 '{"channel_id": "UCSHZKyawb77ixDdsGog4iWA", "upload_playlist_id": "UUSHZKyawb77ixDdsGog4iWA", "category": "ai_research", "priority": "high", "max_videos_per_run": 10}'),

-- Two Minute Papers (AI research explanations)
('youtube_channel', 'Two Minute Papers', 'https://www.youtube.com/@TwoMinutePapers', 'youtube.com',
 '{"channel_id": "UCbfYPyITQ-7l4upoX8nvctg", "upload_playlist_id": "UUbfYPyITQ-7l4upoX8nvctg", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

-- Anthropic (AI safety and research)
('youtube_channel', 'Anthropic', 'https://www.youtube.com/@AnthropicAI', 'youtube.com',
 '{"channel_id": "UCpvYfVOIbW2Tz9Qs8rdvp7w", "upload_playlist_id": "UUpvYfVOIbW2Tz9Qs8rdvp7w", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

-- OpenAI (GPT, ChatGPT, DALL-E research)
('youtube_channel', 'OpenAI', 'https://www.youtube.com/@OpenAI', 'youtube.com',
 '{"channel_id": "UCXZCJLdBC09xxGZ6gcdrc6A", "upload_playlist_id": "UUXZCJLdBC09xxGZ6gcdrc6A", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

-- DeepMind (Google DeepMind research)
('youtube_channel', 'DeepMind', 'https://www.youtube.com/@DeepMind', 'youtube.com',
 '{"channel_id": "UCP7jMXSY2xbc3KCAE0MHQ-A", "upload_playlist_id": "UUP7jMXSY2xbc3KCAE0MHQ-A", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

-- MIT CSAIL (Computer Science and Artificial Intelligence Laboratory)
('youtube_channel', 'MIT CSAIL', 'https://www.youtube.com/@MITCSAIL', 'youtube.com',
 '{"channel_id": "UCBpxspUNl1Th33XbugiHJzw", "upload_playlist_id": "UUBpxspUNl1Th33XbugiHJzw", "category": "ai_research", "priority": "high", "max_videos_per_run": 8}'),

-- Stanford HAI (Human-Centered AI Institute)
('youtube_channel', 'Stanford HAI', 'https://www.youtube.com/@StanfordHAI', 'youtube.com',
 '{"channel_id": "UC4R8DWoMoI7CAwX8_LjQHig", "upload_playlist_id": "UU4R8DWoMoI7CAwX8_LjQHig", "category": "ai_research", "priority": "high", "max_videos_per_run": 6}')

ON CONFLICT (kind, name) DO NOTHING;

-- Tech/Startup AI Channels (Medium Priority)
INSERT INTO public.sources (kind, name, url, domain, metadata) VALUES
-- Y Combinator (startup talks, many AI companies)
('youtube_channel', 'Y Combinator', 'https://www.youtube.com/@ycombinator', 'youtube.com',
 '{"channel_id": "UCcefcZRL2oaA_uBNeo5UOWg", "upload_playlist_id": "UUcefcZRL2oaA_uBNeo5UOWg", "category": "startup_tech", "priority": "medium", "max_videos_per_run": 8}')

ON CONFLICT (kind, name) DO NOTHING;

-- AI Conference Channels (Medium Priority)
INSERT INTO public.sources (kind, name, url, domain, metadata) VALUES
-- NeurIPS (Neural Information Processing Systems)
('youtube_channel', 'NeurIPS', 'https://www.youtube.com/@NeurIPSConf', 'youtube.com',
 '{"channel_id": "UC_4dWbdOgJwZHWP-FGlx2yw", "upload_playlist_id": "UU_4dWbdOgJwZHWP-FGlx2yw", "category": "ai_research", "priority": "medium", "max_videos_per_run": 10}'),

-- ICML (International Conference on Machine Learning)
('youtube_channel', 'ICML', 'https://www.youtube.com/@ICMLConf', 'youtube.com',
 '{"channel_id": "UC74C4hHWkAWDp5LrHmZbfvA", "upload_playlist_id": "UU74C4hHWkAWDp5LrHmZbfvA", "category": "ai_research", "priority": "medium", "max_videos_per_run": 8}')

ON CONFLICT (kind, name) DO NOTHING;

-- Search-Based Discovery (Lower Priority, Higher Quota Cost)
INSERT INTO public.sources (kind, name, url, domain, metadata) VALUES
-- AI research paper explanations
('youtube_search', 'AI Research Papers 2024', NULL, 'youtube.com',
 '{"query": "AI research paper explained 2024", "order": "date", "max_results": 10, "published_after": "2024-01-01T00:00:00Z", "duration": "medium", "category": "ai_research"}'),

-- AI startup funding announcements
('youtube_search', 'AI Startup Funding 2024', NULL, 'youtube.com',
 '{"query": "AI startup funding announcement 2024", "order": "relevance", "max_results": 5, "published_after": "2024-01-01T00:00:00Z", "category": "startup_news"}'),

-- AI breakthrough announcements
('youtube_search', 'AI Breakthrough 2024', NULL, 'youtube.com',
 '{"query": "AI breakthrough announcement 2024", "order": "date", "max_results": 8, "published_after": "2024-01-01T00:00:00Z", "duration": "medium", "category": "ai_research"}')

ON CONFLICT (kind, name) DO NOTHING;

================
File: supabase/.gitignore
================
# Supabase
.branches
.temp

# dotenvx
.env.keys
.env.local
.env.*.local

================
File: supabase/config.toml
================
project_id = "zeke"

[api]
enabled = true
port = 54321
schemas = ["public", "graphql_public"]
extra_search_path = ["public", "extensions"]
max_rows = 10000

[api.tls]
enabled = false

[db]
port = 54322
shadow_port = 54320
major_version = 17

[db.pooler]
enabled = false
port = 54329
pool_mode = "transaction"
default_pool_size = 20
max_client_conn = 100

[db.migrations]
enabled = true
schema_paths = []

[db.seed]
enabled = true
sql_paths = ["./seed.sql"]

[realtime]
enabled = true

[studio]
enabled = true
port = 54323
api_url = "http://127.0.0.1"
openai_api_key = "env(OPENAI_API_KEY)"

[inbucket]
enabled = true
port = 54324

[storage]
enabled = true
file_size_limit = "50MiB"

[auth]
enabled = true
site_url = "http://localhost:3000"
additional_redirect_urls = [
  "http://localhost:3000",
  "http://127.0.0.1:3000",
]
jwt_expiry = 3600
enable_refresh_token_rotation = false
refresh_token_reuse_interval = 10
enable_signup = true
enable_anonymous_sign_ins = false
enable_manual_linking = false

[auth.external.google]
enabled = true
client_id = "env(GOOGLE_CLIENT_ID)"
secret = "env(GOOGLE_SECRET)"
redirect_uri = "http://127.0.0.1:54321/auth/v1/callback"
url = ""
skip_nonce_check = false

[analytics]
enabled = true
port = 54327
backend = "postgres"

================
File: supabase/seed.sql
================
-- Ensure we only touch the public schema during seeds
set search_path = public;

-- Example RSS sources (optional seed)
insert into public.sources (kind, name, url, domain)
values
  ('rss', 'Hacker News â Front Page', 'https://hnrss.org/frontpage', 'news.ycombinator.com'),
  ('rss', 'Ars Technica', 'https://feeds.arstechnica.com/arstechnica/index', 'arstechnica.com')
on conflict do nothing;

-- NOTE: Intentionally skipping direct inserts into pgboss.schedule here to avoid
-- permission issues during `supabase db reset` when seeds run under roles that
-- donât have write access to the pgboss schema. The worker sets up recurring
-- schedules on startup.

-- pg-boss schema version is initialized in the baseline migration; no-op here

-- YouTube AI-Focused Sources
-- Insert AI research channels, tech/startup channels, and search queries
insert into public.sources (kind, name, url, domain, metadata)
values
  -- AI Research Channels (High Priority)
  ('youtube_channel', 'Lex Fridman Podcast', 'https://www.youtube.com/@lexfridman', 'youtube.com',
   '{"channel_id": "UCSHZKyawb77ixDdsGog4iWA", "upload_playlist_id": "UUSHZKyawb77ixDdsGog4iWA", "category": "ai_research", "priority": "high", "max_videos_per_run": 10}'),

  ('youtube_channel', 'Two Minute Papers', 'https://www.youtube.com/@TwoMinutePapers', 'youtube.com',
   '{"channel_id": "UCbfYPyITQ-7l4upoX8nvctg", "upload_playlist_id": "UUbfYPyITQ-7l4upoX8nvctg", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

  ('youtube_channel', 'Anthropic', 'https://www.youtube.com/@AnthropicAI', 'youtube.com',
   '{"channel_id": "UCpvYfVOIbW2Tz9Qs8rdvp7w", "upload_playlist_id": "UUpvYfVOIbW2Tz9Qs8rdvp7w", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

  ('youtube_channel', 'OpenAI', 'https://www.youtube.com/@OpenAI', 'youtube.com',
   '{"channel_id": "UCXZCJLdBC09xxGZ6gcdrc6A", "upload_playlist_id": "UUXZCJLdBC09xxGZ6gcdrc6A", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

  ('youtube_channel', 'DeepMind', 'https://www.youtube.com/@DeepMind', 'youtube.com',
   '{"channel_id": "UCP7jMXSY2xbc3KCAE0MHQ-A", "upload_playlist_id": "UUP7jMXSY2xbc3KCAE0MHQ-A", "category": "ai_research", "priority": "high", "max_videos_per_run": 5}'),

  ('youtube_channel', 'MIT CSAIL', 'https://www.youtube.com/@MITCSAIL', 'youtube.com',
   '{"channel_id": "UCBpxspUNl1Th33XbugiHJzw", "upload_playlist_id": "UUBpxspUNl1Th33XbugiHJzw", "category": "ai_research", "priority": "high", "max_videos_per_run": 8}'),

  ('youtube_channel', 'Stanford HAI', 'https://www.youtube.com/@StanfordHAI', 'youtube.com',
   '{"channel_id": "UC4R8DWoMoI7CAwX8_LjQHig", "upload_playlist_id": "UU4R8DWoMoI7CAwX8_LjQHig", "category": "ai_research", "priority": "high", "max_videos_per_run": 6}'),

  -- Tech/Startup AI Channels (Medium Priority)
  ('youtube_channel', 'Y Combinator', 'https://www.youtube.com/@ycombinator', 'youtube.com',
   '{"channel_id": "UCcefcZRL2oaA_uBNeo5UOWg", "upload_playlist_id": "UUcefcZRL2oaA_uBNeo5UOWg", "category": "startup_tech", "priority": "medium", "max_videos_per_run": 8}'),

  -- AI Conference Channels (Medium Priority)
  ('youtube_channel', 'NeurIPS', 'https://www.youtube.com/@NeurIPSConf', 'youtube.com',
   '{"channel_id": "UC_4dWbdOgJwZHWP-FGlx2yw", "upload_playlist_id": "UU_4dWbdOgJwZHWP-FGlx2yw", "category": "ai_research", "priority": "medium", "max_videos_per_run": 10}'),

  ('youtube_channel', 'ICML', 'https://www.youtube.com/@ICMLConf', 'youtube.com',
   '{"channel_id": "UC74C4hHWkAWDp5LrHmZbfvA", "upload_playlist_id": "UU74C4hHWkAWDp5LrHmZbfvA", "category": "ai_research", "priority": "medium", "max_videos_per_run": 8}'),

  -- Search-Based Discovery (Lower Priority, Higher Quota Cost)
  ('youtube_search', 'AI Research Papers 2024', NULL, 'youtube.com',
   '{"query": "AI research paper explained 2024", "order": "date", "max_results": 10, "published_after": "2024-01-01T00:00:00Z", "duration": "medium", "category": "ai_research"}'),

  ('youtube_search', 'AI Startup Funding 2024', NULL, 'youtube.com',
   '{"query": "AI startup funding announcement 2024", "order": "relevance", "max_results": 5, "published_after": "2024-01-01T00:00:00Z", "category": "startup_news"}'),

  ('youtube_search', 'AI Breakthrough 2024', NULL, 'youtube.com',
   '{"query": "AI breakthrough announcement 2024", "order": "date", "max_results": 8, "published_after": "2024-01-01T00:00:00Z", "duration": "medium", "category": "ai_research"}')
on conflict do nothing;

================
File: .env.local
================
PROJECT_ID="hblelrtwdpukaymtpchv"
GOOGLE_CLIENT_ID="712422159608-7vhfp0ucurccpti3lfujvgc30oiutnma.apps.googleusercontent.com"
GOOGLE_SECRET="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"

# Worker DB password for local development
WORKER_DB_PASSWORD="worker_password"

================
File: .env.ts
================
import { createEnv } from '@t3-oss/env-nextjs';
import { keys as analytics } from '@zeke/analytics/keys';
import { keys as auth } from '@zeke/auth/keys';
import { keys as email } from '@zeke/email/keys';
import { keys as core } from '@zeke/next-config/keys';
import { keys as observability } from '@zeke/observability/keys';
import { keys as database } from '@zeke/supabase/keys';

export const env = createEnv({
  extends: [auth(), analytics(), core(), database(), email(), observability()],
  server: {},
  client: {},
  runtimeEnv: {},
});

================
File: instrumentation.ts
================
import { initializeSentry } from '@zeke/observability/instrumentation';

export const register = initializeSentry();

================
File: package.json
================
{
  "name": "api",
  "private": true,
  "scripts": {
    "dev": "supabase start",
    "login": "supabase login",
    "analyze": "ANALYZE=true npm run build",
    "test": "NODE_ENV=test vitest run",
    "seed": "supabase db seed generate && supabase db seed run",
    "reset": "supabase db reset",
    "generate": "supabase gen types --lang=typescript --local --schema public > ../../packages/supabase/src/types/db.ts",
    "clean": "git clean -xdf .cache .turbo dist node_modules",
    "typecheck": "tsc --noEmit --emitDeclarationOnly false"
  },
  "dependencies": {
    "@zeke/analytics": "workspace:*",
    "@zeke/auth": "workspace:*",
    "@zeke/supabase": "workspace:*",
    "@zeke/next-config": "workspace:*",
    "@zeke/observability": "workspace:*",
    "@zeke/testing": "workspace:*",
    "@sentry/nextjs": "^9.22.0",
    "@t3-oss/env-nextjs": "0.13.8",
    "vitest": "^3.2.4",
    "zod": "^4.1.8"
  },
  "devDependencies": {
    "@zeke/typescript-config": "workspace:*",
    "@types/node": "^24.3.3",
    "@types/react": "19.1.13",
    "@types/react-dom": "19.1.9",
    "typescript": "^5.9.2"
  }
}

================
File: vitest.config.ts
================
export { default } from '@zeke/testing';
