This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-09-15T17:59:31.657Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
scripts/
  deploy-prod-worker.sh
  deploy-railway.sh
  logs.sh
  setup-railway-env.sh
  setup-worker-credentials.sh
  test-connection.sh
  test-integration.sh
  test-railway-deployment.sh
  test-supabase-connection.sh
  test-transcription.sh
src/
  __tests__/
    integration.test.js
  core/
    __tests__/
      job-definitions.test.js
      job-orchestrator.test.js
    job-definitions.ts
    job-orchestrator.ts
    worker-service.ts
  extract/
    build-raw-item-article.ts
    build-raw-item-youtube.ts
    check-ytdlp-availability.ts
    extract-youtube-audio.ts
    get-youtube-metadata.ts
    get-youtube-supported-formats.ts
    normalize-rss-item.ts
    parse-rss-feed.ts
  http/
    __tests__/
      routes.test.js
    routes.ts
  lib/
    openai/
      clean-json-response.ts
      constants.ts
      generate-analysis.ts
      generate-embedding.ts
      generate-stub-analysis.ts
      generate-stub-embedding.ts
      openai-client.ts
      types.ts
    youtube/
      check-quota-status.ts
      get-channel-uploads.ts
      get-video-details.ts
      search-channels.ts
      search-videos.ts
      types.ts
      youtube-client.ts
  storage/
    generate-vtt-content.ts
    prepare-youtube-transcript.ts
  tasks/
    analyze-story.ts
    extract-article.ts
    extract-youtube-audio.ts
    extract-youtube-content.ts
    fetch-youtube-channel-videos.ts
    fetch-youtube-search-videos.ts
    ingest-rss-source.ts
    ingest-youtube-source.ts
    preview-rss-source.ts
    preview-youtube-source.ts
    resolve-youtube-uploads-id.ts
    transcribe-audio.ts
  transcribe/
    queue.ts
    whisper.ts
  types/
    sources.ts
  utils/
    http.ts
    quota-tracker.ts
    retry.ts
    temp-files.ts
  db.ts
  log.ts
  util.ts
  worker-old.ts
  worker.ts
.dockerignore
.env.production
agents.md
ARCHITECTURE.md
Dockerfile
example-architecture.md
MIGRATION-COMPLETE.md
MIGRATION-SUMMARY.md
package.json
RAILWAY-CHECKLIST.md
RAILWAY-TROUBLESHOOTING.md
railway.toml
test-youtube-api.js
test-youtube-pipeline.js
tsconfig.json

================================================================
Files
================================================================

================
File: scripts/deploy-prod-worker.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Production deploy to Cloud Run using envs from worker/.env.production (preferred) or worker/.env.

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
ENV_FILE_PROD="$ROOT_DIR/.env.production"
ENV_FILE="$ROOT_DIR/.env"
if [[ -f "$ENV_FILE_PROD" ]]; then
  echo "[info] Using $ENV_FILE_PROD"
  set -a; source "$ENV_FILE_PROD"; set +a
elif [[ -f "$ENV_FILE" ]]; then
  echo "[info] Using $ENV_FILE"
  set -a; source "$ENV_FILE"; set +a
fi

: "${PROJECT_ID:?Set PROJECT_ID in worker/.env or env}" \
  "${REGION:=us-central1}" \
  "${SERVICE:=zeke-worker}" \
  "${DATABASE_URL:=}" \
  "${DATABASE_URL_POOLER:=}" \
  "${BOSS_SCHEMA:=pgboss}" \
  "${BOSS_CRON_TZ:=UTC}" \
  "${BOSS_MIGRATE:=false}" \
  "${OPENAI_API_KEY:=}" \
  "${YOUTUBE_API_KEY:=}" \
  "${YOUTUBE_QUOTA_LIMIT:=10000}" \
  "${YOUTUBE_QUOTA_RESET_HOUR:=0}" \
  "${YOUTUBE_RATE_LIMIT_BUFFER:=500}"

# Prefer DATABASE_URL_POOLER if provided
if [[ -n "$DATABASE_URL_POOLER" ]]; then
  DATABASE_URL="$DATABASE_URL_POOLER"
fi

if [[ -z "$DATABASE_URL" ]]; then
  echo "[error] DATABASE_URL (or DATABASE_URL_POOLER) not set"
  exit 1
fi

if [[ "${DATABASE_URL}" != *"pooler.supabase.com"* ]]; then
  echo "[warn] DATABASE_URL does not look like a Session Pooler URL."
  echo "       Expected host like <region>.pooler.supabase.com"
fi

ENV_VARS="DATABASE_URL=$DATABASE_URL,BOSS_SCHEMA=$BOSS_SCHEMA,BOSS_CRON_TZ=$BOSS_CRON_TZ,BOSS_MIGRATE=$BOSS_MIGRATE"

# Add OpenAI API key if provided
if [[ -n "$OPENAI_API_KEY" ]]; then
  ENV_VARS="$ENV_VARS,OPENAI_API_KEY=$OPENAI_API_KEY"
fi

# Add YouTube API configuration if provided
if [[ -n "$YOUTUBE_API_KEY" ]]; then
  ENV_VARS="$ENV_VARS,YOUTUBE_API_KEY=$YOUTUBE_API_KEY"
  ENV_VARS="$ENV_VARS,YOUTUBE_QUOTA_LIMIT=$YOUTUBE_QUOTA_LIMIT"
  ENV_VARS="$ENV_VARS,YOUTUBE_QUOTA_RESET_HOUR=$YOUTUBE_QUOTA_RESET_HOUR"
  ENV_VARS="$ENV_VARS,YOUTUBE_RATE_LIMIT_BUFFER=$YOUTUBE_RATE_LIMIT_BUFFER"
fi

echo "[info] Deploying $SERVICE to $REGION..."
gcloud run deploy "$SERVICE" \
  --source "$ROOT_DIR" \
  --project "$PROJECT_ID" \
  --region "$REGION" \
  --min-instances=1 \
  --cpu=2 --memory=4Gi \
  --timeout=1800 \
  --concurrency=1 \
  --set-env-vars "$ENV_VARS" \
  --no-allow-unauthenticated

================
File: scripts/deploy-railway.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üöÇ ZEKE Worker Railway Deployment${NC}"

# Check if Railway CLI is installed
if ! command -v railway >/dev/null 2>&1; then
    echo -e "${RED}‚ùå Railway CLI not found${NC}"
    echo -e "${YELLOW}üí° Install with: npm install -g @railway/cli${NC}"
    exit 1
fi

# Check if logged in to Railway
if ! railway whoami >/dev/null 2>&1; then
    echo -e "${YELLOW}üîê Not logged in to Railway${NC}"
    echo -e "${YELLOW}üí° Run: railway login${NC}"
    exit 1
fi

# Get current directory
ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
REPO_ROOT="$(cd "$ROOT_DIR/../.." && pwd)"

echo -e "${BLUE}üìÅ Working directory: $ROOT_DIR${NC}"
echo -e "${BLUE}üìÅ Repository root: $REPO_ROOT${NC}"

# Check if railway.toml exists
if [[ ! -f "$ROOT_DIR/railway.toml" ]]; then
    echo -e "${RED}‚ùå railway.toml not found${NC}"
    echo -e "${YELLOW}üí° Create railway.toml configuration first${NC}"
    exit 1
fi

# Build the application
echo -e "${YELLOW}üî® Building worker application...${NC}"
cd "$ROOT_DIR"

# Copy pnpm-lock.yaml temporarily for Docker build
echo -e "${YELLOW}üìã Copying pnpm-lock.yaml...${NC}"
cp "$REPO_ROOT/pnpm-lock.yaml" "$ROOT_DIR/pnpm-lock.yaml"

# Check for updated database types
echo -e "${YELLOW}üîç Checking database types...${NC}"
TYPES_FILE="$REPO_ROOT/packages/supabase/src/types/db.ts"
if [[ -f "$TYPES_FILE" ]]; then
    echo -e "${GREEN}‚úÖ Database types found${NC}"
    # Check if types are recent (modified within last hour)
    if [[ $(find "$TYPES_FILE" -mmin -60 2>/dev/null) ]]; then
        echo -e "${GREEN}‚úÖ Types are recent (modified within last hour)${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Database types may be outdated${NC}"
        echo -e "${YELLOW}üí° Run 'pnpm run types:generate' to update${NC}"
    fi
else
    echo -e "${RED}‚ùå Database types not found${NC}"
    echo -e "${YELLOW}üí° Generate types first: cd apps/api && supabase gen types typescript --local --schema public > ../../packages/supabase/src/types/db.ts${NC}"
    exit 1
fi

# Build TypeScript
echo -e "${YELLOW}üîß Building TypeScript...${NC}"
npm run build

# Clean up temporary file
rm -f "$ROOT_DIR/pnpm-lock.yaml"

echo -e "${GREEN}‚úÖ Build completed${NC}"

# Deploy to Railway
echo -e "${YELLOW}üöÇ Deploying to Railway...${NC}"

# Check if project exists
if railway status >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Railway project found${NC}"
else
    echo -e "${YELLOW}üÜï No Railway project found${NC}"
    echo -e "${YELLOW}üí° Run 'railway init' to create a new project${NC}"

    read -p "Create new Railway project? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        railway init
    else
        echo -e "${RED}‚ùå Deployment cancelled${NC}"
        exit 1
    fi
fi

# Set environment variables (if not already set)
echo -e "${YELLOW}üîß Checking environment variables...${NC}"

# Check required environment variables
REQUIRED_VARS=("DATABASE_URL" "OPENAI_API_KEY")
MISSING_VARS=()

for var in "${REQUIRED_VARS[@]}"; do
    if ! railway variables get "$var" >/dev/null 2>&1; then
        MISSING_VARS+=("$var")
    fi
done

if [[ ${#MISSING_VARS[@]} -gt 0 ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  Missing environment variables: ${MISSING_VARS[*]}${NC}"
    echo -e "${YELLOW}üí° Set them with: railway variables set KEY=value${NC}"
    echo -e "${YELLOW}üí° Or set them in the Railway dashboard${NC}"

    read -p "Continue deployment anyway? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo -e "${RED}‚ùå Deployment cancelled${NC}"
        exit 1
    fi
fi

# Deploy
echo -e "${YELLOW}üöÄ Starting deployment...${NC}"
railway up

# Check deployment status
echo -e "${YELLOW}‚è≥ Waiting for deployment to complete...${NC}"
sleep 10

if railway status | grep -q "Deployed"; then
    echo -e "${GREEN}üéâ Deployment successful!${NC}"

    # Get the deployment URL
    DEPLOYMENT_URL=$(railway domain 2>/dev/null || echo "No domain configured")
    if [[ "$DEPLOYMENT_URL" != "No domain configured" ]]; then
        echo -e "${BLUE}üåê Deployment URL: ${YELLOW}$DEPLOYMENT_URL${NC}"
        echo -e "${BLUE}üè• Health check: ${YELLOW}$DEPLOYMENT_URL/healthz${NC}"
        echo -e "${BLUE}üìä Status: ${YELLOW}$DEPLOYMENT_URL/debug/status${NC}"
    else
        echo -e "${YELLOW}üí° Configure a domain with: railway domain${NC}"
    fi

    # Show logs
    echo -e "\n${BLUE}üìã Recent logs:${NC}"
    railway logs --tail 20

else
    echo -e "${RED}‚ùå Deployment may have failed${NC}"
    echo -e "${YELLOW}üìã Check logs with: railway logs${NC}"
    exit 1
fi

echo -e "\n${GREEN}‚úÖ Railway deployment complete!${NC}"
echo -e "${BLUE}üí° Monitor with: railway logs -f${NC}"
echo -e "${BLUE}üí° Check status with: railway status${NC}"

================
File: scripts/logs.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Stream Cloud Run service logs to the console using gcloud.
# Usage:
#   bash scripts/logs.sh                 # info+ (default)
#   bash scripts/logs.sh errors          # warnings and errors only
#   SINCE=30m bash scripts/logs.sh       # change lookback window
#   bash scripts/logs.sh "jsonPayload.msg=fetch_content_start OR textPayload:extract_error"

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
ENV_FILE="$ROOT_DIR/.env"

if [[ -f "$ENV_FILE" ]]; then
  set -a
  # shellcheck disable=SC1090
  source "$ENV_FILE"
  set +a
fi

: "${PROJECT_ID:?Set PROJECT_ID in worker/.env or env}" \
  "${REGION:=us-central1}" \
  "${SERVICE:=zeke-worker}"

# Severity filter: default INFO; use 'errors' to get WARNING+
MIN_SEVERITY="${MIN_SEVERITY:-INFO}"
if [[ "${1:-}" == "errors" || "${1:-}" == "--errors" || "${1:-}" == "-e" ]]; then
  MIN_SEVERITY="WARNING"
  shift || true
fi

FRESHNESS="${FRESHNESS:-15m}"
USER_FILTER="${1:-}"

BASE_FILTER="resource.type=\"cloud_run_revision\" AND resource.labels.service_name=\"$SERVICE\" AND resource.labels.location=\"$REGION\" AND severity>=$MIN_SEVERITY"
if [[ -n "$USER_FILTER" ]]; then
  LOG_FILTER="$BASE_FILTER AND ($USER_FILTER)"
else
  LOG_FILTER="$BASE_FILTER"
fi

echo "[info] Tailing logs for service=$SERVICE region=$REGION project=$PROJECT_ID severity>=$MIN_SEVERITY freshness=$FRESHNESS"
if [[ -n "$USER_FILTER" ]]; then
  echo "[info] Extra filter: $USER_FILTER"
fi

# Simple, compatible poller using 'gcloud logging read' every INTERVAL seconds.
# Many gcloud versions accept FILTER as a positional argument and prefer --order=desc.
INTERVAL="${INTERVAL:-5}"
while true; do
  clear
  echo "[info] $(date -u) ‚Äì last $FRESHNESS ‚Äì severity>=$MIN_SEVERITY"
  gcloud logging read \
    "$LOG_FILTER" \
    --project "$PROJECT_ID" \
    --freshness="$FRESHNESS" \
    --order=desc \
    --limit=200 \
    --format="value(timestamp, severity, resource.labels.revision_name, jsonPayload.msg, textPayload)" | tail -n 200 || true
  sleep "$INTERVAL"
done

================
File: scripts/setup-railway-env.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üöÇ ZEKE Worker Railway Environment Setup${NC}"

# Check if Railway CLI is installed and logged in
if ! command -v railway >/dev/null 2>&1; then
    echo -e "${RED}‚ùå Railway CLI not found${NC}"
    echo -e "${YELLOW}üí° Install with: npm install -g @railway/cli${NC}"
    exit 1
fi

if ! railway whoami >/dev/null 2>&1; then
    echo -e "${YELLOW}üîê Not logged in to Railway${NC}"
    echo -e "${YELLOW}üí° Run: railway login${NC}"
    exit 1
fi

# Function to prompt for input with default
prompt_with_default() {
    local prompt="$1"
    local default="$2"
    local var_name="$3"

    echo -e "${YELLOW}$prompt${NC}"
    if [[ -n "$default" ]]; then
        echo -e "${BLUE}Default: $default${NC}"
    fi
    read -r input

    if [[ -z "$input" && -n "$default" ]]; then
        input="$default"
    fi

    eval "$var_name='$input'"
}

# Function to prompt for secret input
prompt_secret() {
    local prompt="$1"
    local var_name="$2"

    echo -e "${YELLOW}$prompt${NC}"
    read -s input
    echo
    eval "$var_name='$input'"
}

# Check if project is linked
if ! railway status >/dev/null 2>&1; then
    echo -e "${YELLOW}üÜï No Railway project linked${NC}"
    echo -e "${YELLOW}üí° Run 'railway init' or 'railway link' first${NC}"
    exit 1
fi

echo -e "${GREEN}‚úÖ Railway project linked${NC}"

# Get current environment variables
echo -e "${BLUE}üìã Checking existing environment variables...${NC}"
EXISTING_VARS=$(railway variables 2>/dev/null || echo "")

# Function to check if variable exists
var_exists() {
    local var_name="$1"
    echo "$EXISTING_VARS" | grep -q "^$var_name=" || return 1
}

# Function to set variable if not exists or if user wants to update
set_variable() {
    local var_name="$1"
    local var_value="$2"
    local is_secret="${3:-false}"

    if var_exists "$var_name"; then
        echo -e "${YELLOW}‚ö†Ô∏è  $var_name already exists${NC}"
        read -p "Update it? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            return 0
        fi
    fi

    if [[ "$is_secret" == "true" ]]; then
        railway variables set "$var_name=$var_value" --secret
    else
        railway variables set "$var_name=$var_value"
    fi

    echo -e "${GREEN}‚úÖ Set $var_name${NC}"
}

echo -e "\n${BLUE}üîß Setting up environment variables...${NC}"

# Core configuration
echo -e "\n${BLUE}üì¶ Core Configuration${NC}"

prompt_with_default "Node environment:" "production" NODE_ENV
set_variable "NODE_ENV" "$NODE_ENV"

prompt_with_default "Port:" "8080" PORT
set_variable "PORT" "$PORT"

prompt_with_default "pg-boss schema:" "pgboss" BOSS_SCHEMA
set_variable "BOSS_SCHEMA" "$BOSS_SCHEMA"

prompt_with_default "Enable pg-boss migrations:" "true" BOSS_MIGRATE
set_variable "BOSS_MIGRATE" "$BOSS_MIGRATE"

prompt_with_default "Use SSL for database:" "true" USE_SSL
set_variable "USE_SSL" "$USE_SSL"

# Supabase Cloud Database Configuration
echo -e "\n${BLUE}üóÑÔ∏è  Supabase Cloud Database Configuration${NC}"

echo -e "${GREEN}‚úÖ Using existing Supabase Cloud database${NC}"
echo -e "${BLUE}Project: hblelrtwdpukaymtpchv.supabase.co${NC}"
echo -e "${BLUE}This avoids creating duplicate database infrastructure.${NC}"

# Set Supabase configuration variables
echo -e "\n${YELLOW}üîß Setting Supabase configuration...${NC}"

# Supabase URL and keys (from your existing configuration)
set_variable "NEXT_PUBLIC_SUPABASE_URL" "https://hblelrtwdpukaymtpchv.supabase.co"
set_variable "NEXT_PUBLIC_SUPABASE_ANON_KEY" "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhibGVscnR3ZHB1a2F5bXRwY2h2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTY4Njg1MDMsImV4cCI6MjA3MjQ0NDUwM30.PooTgnM30B30on2FBbdri2_eKqIZoR3YZb8i-jtKdjo" true
set_variable "SUPABASE_SERVICE_ROLE_KEY" "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhibGVscnR3ZHB1a2F5bXRwY2h2Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1Njg2ODUwMywiZXhwIjoyMDcyNDQ0NTAzfQ.v9LrwYqgSqKb_K0kPrcthSK9-d8bFjWQZOwnN0bDuUw" true

# Worker role configuration
echo -e "\n${YELLOW}üîë Worker Role Configuration${NC}"
echo -e "${YELLOW}Configure the worker role for your Supabase database.${NC}"

prompt_secret "Enter worker role password for Supabase database:" WORKER_PASSWORD

# Supabase Session Pooler connection (recommended for production)
SUPABASE_DB_URL="postgresql://worker:$WORKER_PASSWORD@aws-0-us-east-1.pooler.supabase.com:5432/postgres"

set_variable "DATABASE_URL" "$SUPABASE_DB_URL" true
set_variable "WORKER_DB_PASSWORD" "$WORKER_PASSWORD" true

echo -e "\n${YELLOW}üîß Worker role setup in Supabase...${NC}"
echo -e "${BLUE}Connect to your Supabase database and run this SQL:${NC}"
echo -e "${GREEN}"
cat << EOF
-- Connect to Supabase via Dashboard > SQL Editor or psql
-- Create worker role and grant permissions
CREATE ROLE worker WITH LOGIN PASSWORD '$WORKER_PASSWORD';
GRANT CREATE ON DATABASE postgres TO worker;
GRANT USAGE ON SCHEMA public TO worker;
GRANT CREATE ON SCHEMA public TO worker;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO worker;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO worker;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO worker;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO worker;
EOF
echo -e "${NC}"

echo -e "\n${BLUE}üí° You can also run this via Supabase Dashboard > SQL Editor${NC}"
read -p "Press Enter after creating the worker role in Supabase..."

# API Keys
echo -e "\n${BLUE}üîë API Keys${NC}"

if ! var_exists "OPENAI_API_KEY"; then
    prompt_secret "Enter OpenAI API key:" OPENAI_API_KEY
    if [[ -n "$OPENAI_API_KEY" ]]; then
        set_variable "OPENAI_API_KEY" "$OPENAI_API_KEY" true
    fi
else
    echo -e "${GREEN}‚úÖ OPENAI_API_KEY already set${NC}"
fi

if ! var_exists "YOUTUBE_API_KEY"; then
    prompt_secret "Enter YouTube API key (optional):" YOUTUBE_API_KEY
    if [[ -n "$YOUTUBE_API_KEY" ]]; then
        set_variable "YOUTUBE_API_KEY" "$YOUTUBE_API_KEY" true
    fi
else
    echo -e "${GREEN}‚úÖ YOUTUBE_API_KEY already set${NC}"
fi

# Optional configuration
echo -e "\n${BLUE}‚öôÔ∏è  Optional Configuration${NC}"

prompt_with_default "Log level:" "info" LOG_LEVEL
set_variable "LOG_LEVEL" "$LOG_LEVEL"

prompt_with_default "Log format:" "json" LOG_FORMAT
set_variable "LOG_FORMAT" "$LOG_FORMAT"

# Summary
echo -e "\n${GREEN}üéâ Environment setup complete!${NC}"
echo -e "\n${BLUE}üìã Summary of configured variables:${NC}"
railway variables | grep -E "(NODE_ENV|PORT|DATABASE_URL|OPENAI_API_KEY|BOSS_SCHEMA)" || true

echo -e "\n${BLUE}üöÄ Next steps:${NC}"
echo -e "${YELLOW}1. Verify database worker role is created${NC}"
echo -e "${YELLOW}2. Test deployment: pnpm deploy:railway${NC}"
echo -e "${YELLOW}3. Check health: curl https://[your-domain]/healthz${NC}"
echo -e "${YELLOW}4. Monitor logs: railway logs -f${NC}"

echo -e "\n${GREEN}‚úÖ Ready for deployment!${NC}"

================
File: scripts/setup-worker-credentials.sh
================
#!/bin/bash

# Setup worker database credentials
# This script ensures the worker role has the correct password from environment variables

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üîß Setting up worker database credentials...${NC}"

# Load environment variables
if [ -f ".env.development" ]; then
    echo -e "${GREEN}üìÑ Loading .env.development${NC}"
    export $(grep -v '^#' .env.development | xargs)
elif [ -f ".env.local" ]; then
    echo -e "${GREEN}üìÑ Loading .env.local${NC}"
    export $(grep -v '^#' .env.local | xargs)
fi

# Check if required variables are set
if [ -z "${WORKER_DB_PASSWORD:-}" ]; then
    echo -e "${RED}‚ùå WORKER_DB_PASSWORD environment variable is not set${NC}"
    echo -e "${YELLOW}üí° Please set WORKER_DB_PASSWORD in your .env file${NC}"
    exit 1
fi

if [ -z "${DATABASE_URL:-}" ]; then
    echo -e "${RED}‚ùå DATABASE_URL environment variable is not set${NC}"
    echo -e "${YELLOW}üí° Please set DATABASE_URL in your .env file${NC}"
    exit 1
fi

# Extract connection details for admin connection
# We need to connect as postgres user to alter the worker role
ADMIN_URL=$(echo "$DATABASE_URL" | sed 's/worker:[^@]*@/postgres:postgres@/')

echo -e "${BLUE}üîë Setting worker role password...${NC}"

# Set the worker password
psql "$ADMIN_URL" -c "ALTER ROLE worker PASSWORD '$WORKER_DB_PASSWORD';" || {
    echo -e "${RED}‚ùå Failed to set worker password${NC}"
    exit 1
}

echo -e "${GREEN}‚úÖ Worker password updated successfully${NC}"

# Test the worker connection
echo -e "${BLUE}üß™ Testing worker connection...${NC}"

psql "$DATABASE_URL" -c "SELECT current_user, current_database();" || {
    echo -e "${RED}‚ùå Worker connection test failed${NC}"
    exit 1
}

echo -e "${GREEN}‚úÖ Worker connection test successful${NC}"

# Test source_health table access
echo -e "${BLUE}üß™ Testing source_health table access...${NC}"

psql "$DATABASE_URL" -c "SELECT COUNT(*) FROM public.source_health;" || {
    echo -e "${RED}‚ùå Worker cannot access source_health table${NC}"
    exit 1
}

echo -e "${GREEN}‚úÖ Worker can access source_health table${NC}"

echo -e "${GREEN}üéâ Worker credentials setup complete!${NC}"
echo -e "${BLUE}üìã Summary:${NC}"
echo -e "  ‚úÖ Worker role password updated"
echo -e "  ‚úÖ Database connection verified"
echo -e "  ‚úÖ Table access permissions confirmed"
echo -e "  ‚úÖ Ready for source health operations"

================
File: scripts/test-connection.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üîç Testing database connection...${NC}"

# Load environment variables (.env.development preferred, fallback to .env)
if [ -f .env.development ]; then
    set -a; source .env.development; set +a
elif [ -f .env ]; then
    set -a; source .env; set +a
fi

# Check required environment variables
if [ -z "${DATABASE_URL:-}" ]; then
    echo -e "${RED}‚ùå DATABASE_URL not set${NC}"
    exit 1
fi

BOSS_SCHEMA="${BOSS_SCHEMA:-pgboss}"

# If psql would prompt for a password (e.g., URL lacks password for `worker`),
# set PGPASSWORD from WORKER_PASS (default worker_password) for non-interactive use.
if [ -z "${PGPASSWORD:-}" ]; then
  if echo "$DATABASE_URL" | grep -Eq '^postgresql://worker@'; then
    export PGPASSWORD="${WORKER_PASS:-worker_password}"
  fi
fi

echo -e "${BLUE}üìç Schema: ${BOSS_SCHEMA}${NC}"
echo -e "${BLUE}üîó URL: ${DATABASE_URL//:*@/:***@}${NC}"

# Test 1: Basic PostgreSQL connection
echo -e "\n${YELLOW}1Ô∏è‚É£ Testing PostgreSQL connection...${NC}"

# Use psql to test connection
if echo "SELECT version();" | psql "$DATABASE_URL" -t -A 2>/dev/null | head -1 | grep -q "PostgreSQL"; then
    VERSION=$(echo "SELECT version();" | psql "$DATABASE_URL" -t -A 2>/dev/null | head -1 | cut -d' ' -f1)
    echo -e "${GREEN}‚úÖ PostgreSQL connected: $VERSION${NC}"
else
    echo -e "${RED}‚ùå PostgreSQL connection failed${NC}"
    exit 1
fi

# Test 2: Check PgBoss schema
echo -e "\n${YELLOW}2Ô∏è‚É£ Testing PgBoss schema...${NC}"

SCHEMA_EXISTS=$(echo "SELECT EXISTS(SELECT 1 FROM information_schema.schemata WHERE schema_name = '$BOSS_SCHEMA');" | psql "$DATABASE_URL" -t -A 2>/dev/null)

if [ "$SCHEMA_EXISTS" = "t" ]; then
    echo -e "${GREEN}‚úÖ PgBoss schema exists${NC}"

    # Count tables in schema
    TABLE_COUNT=$(echo "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = '$BOSS_SCHEMA';" | psql "$DATABASE_URL" -t -A 2>/dev/null)
    echo -e "${GREEN}üìä Found $TABLE_COUNT tables in $BOSS_SCHEMA schema${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  PgBoss schema not found - run migrations first${NC}"
fi

# Test 3: Test worker role permissions
echo -e "\n${YELLOW}3Ô∏è‚É£ Testing worker role permissions...${NC}"

# Check if we can create a test table (and clean it up)
TEST_TABLE="test_worker_permissions_$$"
if echo "CREATE TABLE $BOSS_SCHEMA.$TEST_TABLE (id SERIAL PRIMARY KEY); DROP TABLE $BOSS_SCHEMA.$TEST_TABLE;" | psql "$DATABASE_URL" >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Worker has sufficient permissions${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Limited permissions (may be read-only)${NC}"
fi

echo -e "\n${GREEN}üéâ All connection tests passed!${NC}"

================
File: scripts/test-integration.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üß™ Running Worker Integration Tests...${NC}"

# Load environment variables
if [ -f .env.development ]; then
    set -a; source .env.development; set +a
elif [ -f .env ]; then
    set -a; source .env; set +a
fi

# Check required environment variables
if [ -z "${DATABASE_URL:-}" ]; then
    echo -e "${RED}‚ùå DATABASE_URL not set${NC}"
    exit 1
fi

WORKER_PORT="${PORT:-8080}"
WORKER_URL="http://localhost:${WORKER_PORT}"

echo -e "${BLUE}üìç Testing against: ${WORKER_URL}${NC}"

# Test 1: Worker Health Check
echo -e "\n${YELLOW}1Ô∏è‚É£ Testing worker health...${NC}"

if curl -f -s "${WORKER_URL}/healthz" >/dev/null; then
    echo -e "${GREEN}‚úÖ Worker health check passed${NC}"
else
    echo -e "${RED}‚ùå Worker health check failed - is worker running?${NC}"
    echo -e "${YELLOW}üí° Try: npm run dev${NC}"
    exit 1
fi

# Test 2: System Status
echo -e "\n${YELLOW}2Ô∏è‚É£ Testing system status...${NC}"

STATUS_RESPONSE=$(curl -f -s "${WORKER_URL}/debug/status" || echo "failed")
if echo "$STATUS_RESPONSE" | grep -q '"ok":true'; then
    echo -e "${GREEN}‚úÖ System status check passed${NC}"

    # Extract some metrics
    if command -v jq >/dev/null 2>&1; then
        SOURCES=$(echo "$STATUS_RESPONSE" | jq -r '.sources.sources_rss // 0')
        RAW_TOTAL=$(echo "$STATUS_RESPONSE" | jq -r '.raw.raw_total // 0')
        CONTENTS=$(echo "$STATUS_RESPONSE" | jq -r '.contents.contents_total // 0')
        echo -e "${BLUE}üìä Sources: ${SOURCES}, Raw Items: ${RAW_TOTAL}, Contents: ${CONTENTS}${NC}"
    fi
else
    echo -e "${RED}‚ùå System status check failed${NC}"
    echo "Response: $STATUS_RESPONSE"
    exit 1
fi

# Test 3: Database Connection
echo -e "\n${YELLOW}3Ô∏è‚É£ Testing database connection...${NC}"

if bash scripts/test-connection.sh >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Database connection test passed${NC}"
else
    echo -e "${RED}‚ùå Database connection test failed${NC}"
    exit 1
fi

# Test 4: Manual Job Triggers
echo -e "\n${YELLOW}4Ô∏è‚É£ Testing manual job triggers...${NC}"

# Test RSS ingest trigger
RSS_RESPONSE=$(curl -f -s -X POST "${WORKER_URL}/debug/ingest-now" || echo "failed")
if echo "$RSS_RESPONSE" | grep -q '"ok":true'; then
    echo -e "${GREEN}‚úÖ RSS ingest trigger works${NC}"
else
    echo -e "${RED}‚ùå RSS ingest trigger failed${NC}"
    echo "Response: $RSS_RESPONSE"
fi

# Test YouTube ingest trigger (if API key available)
if [ -n "${YOUTUBE_API_KEY:-}" ] && [[ "${YOUTUBE_API_KEY}" != *"test"* ]]; then
    YT_RESPONSE=$(curl -f -s -X POST "${WORKER_URL}/debug/ingest-youtube" || echo "failed")
    if echo "$YT_RESPONSE" | grep -q '"ok":true'; then
        echo -e "${GREEN}‚úÖ YouTube ingest trigger works${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  YouTube ingest trigger failed (may be quota/API issue)${NC}"
    fi
else
    echo -e "${YELLOW}‚ö†Ô∏è  YouTube API key not available - skipping YouTube test${NC}"
fi

# Test 5: One-off URL Processing
echo -e "\n${YELLOW}5Ô∏è‚É£ Testing one-off URL processing...${NC}"

TEST_URLS='["https://example.com/test-article"]'
ONEOFF_RESPONSE=$(curl -f -s -X POST "${WORKER_URL}/debug/ingest-oneoff" \
    -H "Content-Type: application/json" \
    -d "{\"urls\": $TEST_URLS}" || echo "failed")

if echo "$ONEOFF_RESPONSE" | grep -q '"ok":true'; then
    echo -e "${GREEN}‚úÖ One-off URL processing works${NC}"

    # Check if we got results
    if command -v jq >/dev/null 2>&1; then
        RESULTS_COUNT=$(echo "$ONEOFF_RESPONSE" | jq -r '.results | length')
        echo -e "${BLUE}üìä Processed ${RESULTS_COUNT} URLs${NC}"
    fi
else
    echo -e "${RED}‚ùå One-off URL processing failed${NC}"
    echo "Response: $ONEOFF_RESPONSE"
fi

# Test 6: Job Queue Monitoring
echo -e "\n${YELLOW}6Ô∏è‚É£ Testing job queue monitoring...${NC}"

# Wait a moment for jobs to be processed
sleep 2

# Check status again to see job activity
STATUS_RESPONSE_2=$(curl -f -s "${WORKER_URL}/debug/status" || echo "failed")
if echo "$STATUS_RESPONSE_2" | grep -q '"ok":true'; then
    if command -v jq >/dev/null 2>&1; then
        JOB_COUNT=$(echo "$STATUS_RESPONSE_2" | jq -r '.jobs | length')
        if [ "$JOB_COUNT" -gt 0 ]; then
            echo -e "${GREEN}‚úÖ Job queue monitoring works (${JOB_COUNT} job types found)${NC}"

            # Show job summary
            echo -e "${BLUE}üìã Job Summary:${NC}"
            echo "$STATUS_RESPONSE_2" | jq -r '.jobs[] | "   \(.name): \(.count) (\(.state))"' | head -5
        else
            echo -e "${YELLOW}‚ö†Ô∏è  No jobs found in queue (may be normal)${NC}"
        fi
    else
        echo -e "${GREEN}‚úÖ Job queue monitoring endpoint works${NC}"
    fi
else
    echo -e "${RED}‚ùå Job queue monitoring failed${NC}"
fi

# Test 7: Error Handling
echo -e "\n${YELLOW}7Ô∏è‚É£ Testing error handling...${NC}"

# Test invalid source ID
INVALID_RESPONSE=$(curl -s -X POST "${WORKER_URL}/debug/ingest-source?sourceId=invalid-source-id" || echo "failed")
if echo "$INVALID_RESPONSE" | grep -q '"ok":false'; then
    echo -e "${GREEN}‚úÖ Error handling works (invalid source properly rejected)${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Error handling test inconclusive${NC}"
fi

# Test missing parameters
MISSING_RESPONSE=$(curl -s -X POST "${WORKER_URL}/debug/ingest-source" || echo "failed")
if echo "$MISSING_RESPONSE" | grep -q '"ok":false'; then
    echo -e "${GREEN}‚úÖ Parameter validation works${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Parameter validation test inconclusive${NC}"
fi

# Test 8: Architecture Consistency
echo -e "\n${YELLOW}8Ô∏è‚É£ Testing architecture consistency...${NC}"

# Verify new architecture is being used
if [ -f "dist/worker.js" ] && [ -f "src/core/worker-service.js" ]; then
    echo -e "${GREEN}‚úÖ New modular architecture is active${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Architecture files not found - run 'npm run build'${NC}"
fi

# Summary
echo -e "\n${GREEN}üéâ Integration tests completed!${NC}"
echo -e "\n${BLUE}üìã Test Summary:${NC}"
echo -e "   ‚úÖ Worker health and status"
echo -e "   ‚úÖ Database connectivity"
echo -e "   ‚úÖ Manual job triggering"
echo -e "   ‚úÖ URL processing pipeline"
echo -e "   ‚úÖ Job queue monitoring"
echo -e "   ‚úÖ Error handling"
echo -e "   ‚úÖ Architecture consistency"

echo -e "\n${GREEN}üöÄ Worker integration tests passed!${NC}"
echo -e "${BLUE}üí° The worker is ready for production use.${NC}"

================
File: scripts/test-railway-deployment.sh
================
#!/bin/bash

# Test Railway Deployment Configuration
# This script validates the Railway deployment setup before actual deployment

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
WORKER_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
TEST_PORT=8083
HEALTH_TIMEOUT=30
READY_TIMEOUT=60

echo -e "${BLUE}üß™ Testing Railway Deployment Configuration${NC}"
echo -e "${BLUE}============================================${NC}"

# Function to check if service is running
check_service() {
    local port=$1
    local endpoint=$2
    local timeout=$3
    local description=$4

    echo -e "${YELLOW}‚è≥ Testing $description (timeout: ${timeout}s)...${NC}"

    for i in $(seq 1 $timeout); do
        if curl -f -s "http://localhost:$port$endpoint" > /dev/null 2>&1; then
            echo -e "${GREEN}‚úÖ $description is responding${NC}"
            return 0
        fi
        sleep 1
    done

    echo -e "${RED}‚ùå $description failed to respond within ${timeout}s${NC}"
    return 1
}

# Function to test endpoint response
test_endpoint() {
    local port=$1
    local endpoint=$2
    local description=$3

    echo -e "${YELLOW}üîç Testing $description...${NC}"

    response=$(curl -s "http://localhost:$port$endpoint" || echo "FAILED")

    if [ "$response" = "FAILED" ]; then
        echo -e "${RED}‚ùå $description failed${NC}"
        return 1
    fi

    echo -e "${GREEN}‚úÖ $description response: $response${NC}"
    return 0
}

# Function to cleanup
cleanup() {
    echo -e "${YELLOW}üßπ Cleaning up...${NC}"
    if [ ! -z "$WORKER_PID" ]; then
        kill $WORKER_PID 2>/dev/null || true
        wait $WORKER_PID 2>/dev/null || true
    fi
    echo -e "${GREEN}‚úÖ Cleanup complete${NC}"
}

# Set trap for cleanup
trap cleanup EXIT

# Step 1: Validate Railway configuration
echo -e "${BLUE}üìã Step 1: Validating Railway Configuration${NC}"

cd "$WORKER_DIR"

if [ ! -f "railway.toml" ]; then
    echo -e "${RED}‚ùå railway.toml not found${NC}"
    exit 1
fi

if [ ! -f "Dockerfile" ]; then
    echo -e "${RED}‚ùå Dockerfile not found${NC}"
    exit 1
fi

echo -e "${GREEN}‚úÖ Railway configuration files found${NC}"

# Step 2: Build the application
echo -e "${BLUE}üì¶ Step 2: Building Application${NC}"

if [ ! -f "package.json" ]; then
    echo -e "${RED}‚ùå package.json not found${NC}"
    exit 1
fi

echo -e "${YELLOW}üîß Installing dependencies...${NC}"
npm install --silent

echo -e "${YELLOW}üîß Building TypeScript...${NC}"
npm run build

if [ ! -f "dist/worker.js" ]; then
    echo -e "${RED}‚ùå Build failed - dist/worker.js not found${NC}"
    exit 1
fi

echo -e "${GREEN}‚úÖ Build successful${NC}"

# Step 3: Test Docker build (optional, if Docker is available)
echo -e "${BLUE}üê≥ Step 3: Testing Docker Build (Optional)${NC}"

if command -v docker &> /dev/null; then
    echo -e "${YELLOW}üîß Building Docker image...${NC}"
    if docker build -t zeke-worker-test . --quiet; then
        echo -e "${GREEN}‚úÖ Docker build successful${NC}"

        # Clean up test image
        docker rmi zeke-worker-test --force > /dev/null 2>&1 || true
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Docker build failed (non-critical for Railway test)${NC}"
    fi
else
    echo -e "${YELLOW}‚ö†Ô∏è  Docker not available, skipping Docker build test${NC}"
fi

# Step 4: Test local startup with Railway-like environment
echo -e "${BLUE}üöÄ Step 4: Testing Local Startup${NC}"

# Set Railway-like environment variables
export PORT=$TEST_PORT
export NODE_ENV=production
export RAILWAY_ENVIRONMENT=test

# Check if required environment variables are set
if [ -z "$DATABASE_URL" ]; then
    echo -e "${YELLOW}‚ö†Ô∏è  DATABASE_URL not set - using test mode${NC}"
    export DATABASE_URL="postgresql://test:test@localhost:5432/test"
fi

echo -e "${YELLOW}üîß Starting worker service on port $TEST_PORT...${NC}"

# Start the worker in background
node dist/worker.js &
WORKER_PID=$!

# Wait a moment for startup
sleep 2

# Check if process is still running
if ! kill -0 $WORKER_PID 2>/dev/null; then
    echo -e "${RED}‚ùå Worker process died during startup${NC}"
    exit 1
fi

echo -e "${GREEN}‚úÖ Worker process started (PID: $WORKER_PID)${NC}"

# Step 5: Test health endpoints
echo -e "${BLUE}üè• Step 5: Testing Health Endpoints${NC}"

# Test immediate health check (should work right away)
if check_service $TEST_PORT "/healthz" $HEALTH_TIMEOUT "Health Check (/healthz)"; then
    test_endpoint $TEST_PORT "/healthz" "Health Check Response"
else
    echo -e "${RED}‚ùå Health check failed - this will cause Railway deployment to fail${NC}"
    exit 1
fi

# Test readiness check (may take longer)
if check_service $TEST_PORT "/ready" $READY_TIMEOUT "Readiness Check (/ready)"; then
    test_endpoint $TEST_PORT "/ready" "Readiness Check Response"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Readiness check failed - check database connectivity${NC}"
fi

# Test status endpoint if available
if check_service $TEST_PORT "/debug/status" 5 "Status Endpoint (/debug/status)"; then
    echo -e "${GREEN}‚úÖ Status endpoint is available${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Status endpoint not available (may require full initialization)${NC}"
fi

# Step 6: Railway CLI validation (if available)
echo -e "${BLUE}üöÇ Step 6: Railway CLI Validation${NC}"

if command -v railway &> /dev/null; then
    echo -e "${YELLOW}üîç Checking Railway CLI status...${NC}"

    if railway status > /dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ Railway project is linked${NC}"

        # Check environment variables
        echo -e "${YELLOW}üîç Checking Railway environment variables...${NC}"
        railway variables > /tmp/railway_vars.txt 2>/dev/null || true

        if [ -s /tmp/railway_vars.txt ]; then
            echo -e "${GREEN}‚úÖ Railway environment variables configured${NC}"
        else
            echo -e "${YELLOW}‚ö†Ô∏è  No Railway environment variables found${NC}"
        fi

        rm -f /tmp/railway_vars.txt
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Railway project not linked (run 'railway init' or 'railway link')${NC}"
    fi
else
    echo -e "${YELLOW}‚ö†Ô∏è  Railway CLI not installed${NC}"
    echo -e "${YELLOW}üí° Install with: npm install -g @railway/cli${NC}"
fi

# Final summary
echo -e "${BLUE}üìä Test Summary${NC}"
echo -e "${BLUE}===============${NC}"

echo -e "${GREEN}‚úÖ Configuration files validated${NC}"
echo -e "${GREEN}‚úÖ Application builds successfully${NC}"
echo -e "${GREEN}‚úÖ Worker process starts correctly${NC}"
echo -e "${GREEN}‚úÖ Health check endpoint responds${NC}"

echo -e "${BLUE}üéØ Railway Deployment Readiness${NC}"
echo -e "${GREEN}‚úÖ Your worker is ready for Railway deployment!${NC}"
echo ""
echo -e "${YELLOW}üìù Next Steps:${NC}"
echo -e "1. Ensure all environment variables are set in Railway dashboard"
echo -e "2. Run: ${BLUE}railway up${NC}"
echo -e "3. Monitor deployment: ${BLUE}railway logs -f${NC}"
echo -e "4. Test deployed health check: ${BLUE}curl https://[your-domain]/healthz${NC}"
echo ""
echo -e "${GREEN}üöÄ Ready for production deployment!${NC}"

================
File: scripts/test-supabase-connection.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üîç Supabase Connection Test for Railway Deployment${NC}"

# Supabase Cloud configuration
SUPABASE_URL="https://hblelrtwdpukaymtpchv.supabase.co"
SUPABASE_PROJECT_ID="hblelrtwdpukaymtpchv"
SUPABASE_POOLER_HOST="aws-0-us-east-1.pooler.supabase.com"
SUPABASE_POOLER_PORT="5432"
SUPABASE_DB_NAME="postgres"

# Function to test database connection
test_connection() {
    local connection_string="$1"
    local description="$2"

    echo -e "\n${BLUE}üîó Testing $description...${NC}"

    if PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -c "SELECT current_user, current_database(), version();" >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ $description connection successful${NC}"

        # Get connection details
        local result=$(PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -t -c "SELECT current_user || ' @ ' || current_database() || ' (' || split_part(version(), ' ', 2) || ')';")
        echo -e "${BLUE}   Connected as: ${result}${NC}"

        return 0
    else
        echo -e "${RED}‚ùå $description connection failed${NC}"
        return 1
    fi
}

# Function to test worker role permissions
test_permissions() {
    local connection_string="$1"
    local description="$2"

    echo -e "\n${BLUE}üîê Testing $description permissions...${NC}"

    # Test basic permissions
    if PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -c "
        SELECT
            has_database_privilege('worker', 'postgres', 'CREATE') as can_create_db,
            has_schema_privilege('worker', 'public', 'CREATE') as can_create_schema,
            has_schema_privilege('worker', 'public', 'USAGE') as can_use_schema;
    " >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ $description permissions verified${NC}"

        # Show permission details
        local perms=$(PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -t -c "
            SELECT
                'DB CREATE: ' || has_database_privilege('worker', 'postgres', 'CREATE') ||
                ', SCHEMA CREATE: ' || has_schema_privilege('worker', 'public', 'CREATE') ||
                ', SCHEMA USAGE: ' || has_schema_privilege('worker', 'public', 'USAGE');
        ")
        echo -e "${BLUE}   Permissions: ${perms}${NC}"

        return 0
    else
        echo -e "${RED}‚ùå $description permission check failed${NC}"
        return 1
    fi
}

# Function to test pg-boss schema
test_pgboss_schema() {
    local connection_string="$1"

    echo -e "\n${BLUE}üìã Testing pg-boss schema...${NC}"

    # Check if pgboss schema exists
    local schema_exists=$(PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -t -c "
        SELECT EXISTS(SELECT 1 FROM information_schema.schemata WHERE schema_name = 'pgboss');
    " 2>/dev/null || echo "f")

    if [[ "$schema_exists" == *"t"* ]]; then
        echo -e "${GREEN}‚úÖ pgboss schema exists${NC}"

        # Count pgboss tables
        local table_count=$(PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -t -c "
            SELECT count(*) FROM information_schema.tables WHERE table_schema = 'pgboss';
        " 2>/dev/null || echo "0")

        echo -e "${BLUE}   pgboss tables: ${table_count}${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  pgboss schema does not exist (will be created on first run)${NC}"
    fi
}

# Function to test main application tables
test_app_tables() {
    local connection_string="$1"

    echo -e "\n${BLUE}üìä Testing application tables...${NC}"

    local tables=("sources" "stories" "content" "story_overlays")
    local existing_tables=0

    for table in "${tables[@]}"; do
        local exists=$(PGPASSWORD="$WORKER_PASSWORD" psql "$connection_string" -t -c "
            SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = '$table');
        " 2>/dev/null || echo "f")

        if [[ "$exists" == *"t"* ]]; then
            echo -e "${GREEN}   ‚úÖ $table table exists${NC}"
            ((existing_tables++))
        else
            echo -e "${RED}   ‚ùå $table table missing${NC}"
        fi
    done

    if [[ $existing_tables -eq ${#tables[@]} ]]; then
        echo -e "${GREEN}‚úÖ All application tables exist${NC}"
        return 0
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Some application tables are missing${NC}"
        return 1
    fi
}

# Main execution
echo -e "${YELLOW}This script tests the Supabase Cloud database connection for Railway deployment.${NC}"
echo -e "${BLUE}Supabase Project: $SUPABASE_PROJECT_ID${NC}"

# Get worker password
if [[ -n "${WORKER_DB_PASSWORD:-}" ]]; then
    WORKER_PASSWORD="$WORKER_DB_PASSWORD"
    echo -e "${GREEN}‚úÖ Using WORKER_DB_PASSWORD from environment${NC}"
else
    echo -e "${YELLOW}üîë Enter worker role password:${NC}"
    read -s WORKER_PASSWORD
    echo
fi

if [[ -z "$WORKER_PASSWORD" ]]; then
    echo -e "${RED}‚ùå Worker password is required${NC}"
    exit 1
fi

# Test connections
echo -e "\n${BLUE}üß™ Running connection tests...${NC}"

# Test Session Pooler (recommended for production)
POOLER_URL="postgresql://worker:$WORKER_PASSWORD@$SUPABASE_POOLER_HOST:$SUPABASE_POOLER_PORT/$SUPABASE_DB_NAME"
if test_connection "$POOLER_URL" "Supabase Session Pooler"; then
    test_permissions "$POOLER_URL" "Session Pooler"
    test_pgboss_schema "$POOLER_URL"
    test_app_tables "$POOLER_URL"
    POOLER_SUCCESS=true
else
    POOLER_SUCCESS=false
fi

# Test Direct Connection (fallback)
DIRECT_URL="postgresql://worker:$WORKER_PASSWORD@db.hblelrtwdpukaymtpchv.supabase.co:5432/$SUPABASE_DB_NAME"
if test_connection "$DIRECT_URL" "Supabase Direct Connection"; then
    test_permissions "$DIRECT_URL" "Direct Connection"
    DIRECT_SUCCESS=true
else
    DIRECT_SUCCESS=false
fi

# Summary
echo -e "\n${BLUE}üìã Connection Test Summary${NC}"

if [[ "$POOLER_SUCCESS" == "true" ]]; then
    echo -e "${GREEN}‚úÖ Session Pooler: Ready for Railway deployment${NC}"
    echo -e "${BLUE}   Recommended DATABASE_URL: postgresql://worker:***@$SUPABASE_POOLER_HOST:$SUPABASE_POOLER_PORT/$SUPABASE_DB_NAME${NC}"
elif [[ "$DIRECT_SUCCESS" == "true" ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  Direct Connection: Working but Session Pooler preferred${NC}"
    echo -e "${BLUE}   Fallback DATABASE_URL: postgresql://worker:***@db.hblelrtwdpukaymtpchv.supabase.co:5432/$SUPABASE_DB_NAME${NC}"
else
    echo -e "${RED}‚ùå No working connections found${NC}"
    echo -e "${YELLOW}üí° Check worker role exists and password is correct${NC}"
    exit 1
fi

echo -e "\n${GREEN}üéâ Supabase connection test complete!${NC}"
echo -e "${BLUE}üí° Use the recommended DATABASE_URL in your Railway environment variables${NC}"

================
File: scripts/test-transcription.sh
================
#!/usr/bin/env bash
set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üé¨ Testing transcription pipeline...${NC}"

# Load environment variables
if [ -f .env.development ]; then
  set -a; source .env.development; set +a
elif [ -f .env ]; then
  set -a; source .env; set +a
fi

# Check required environment variables
if [ -z "${DATABASE_URL:-}" ]; then
    echo -e "${RED}‚ùå DATABASE_URL not set${NC}"
    exit 1
fi

BOSS_SCHEMA="${BOSS_SCHEMA:-pgboss}"
OPENAI_API_KEY="${OPENAI_API_KEY:-}"
YOUTUBE_API_KEY="${YOUTUBE_API_KEY:-}"

# Avoid interactive password prompts for worker role when URL lacks password
if [ -z "${PGPASSWORD:-}" ]; then
  if echo "$DATABASE_URL" | grep -Eq '^postgresql://worker@'; then
    export PGPASSWORD="${WORKER_PASS:-worker_password}"
  fi
fi

if [ -z "$OPENAI_API_KEY" ] || [[ "$OPENAI_API_KEY" == *"test"* ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  OPENAI_API_KEY not set or is test key - transcription will be mocked${NC}"
fi

if [ -z "$YOUTUBE_API_KEY" ] || [[ "$YOUTUBE_API_KEY" == *"test"* ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  YOUTUBE_API_KEY not set or is test key - YouTube API calls will be mocked${NC}"
fi

# Test 1: Check if PgBoss tables exist
echo -e "\n${YELLOW}1Ô∏è‚É£ Checking PgBoss tables...${NC}"

REQUIRED_TABLES=("job" "queue" "schedule" "subscription" "version")
MISSING_TABLES=()

for table in "${REQUIRED_TABLES[@]}"; do
    EXISTS=$(echo "SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_schema = '$BOSS_SCHEMA' AND table_name = '$table');" | psql "$DATABASE_URL" -t -A 2>/dev/null)
    if [ "$EXISTS" = "t" ]; then
        echo -e "${GREEN}‚úÖ Table $BOSS_SCHEMA.$table exists${NC}"
    else
        echo -e "${RED}‚ùå Table $BOSS_SCHEMA.$table missing${NC}"
        MISSING_TABLES+=("$table")
    fi
done

if [ ${#MISSING_TABLES[@]} -gt 0 ]; then
    echo -e "${RED}‚ùå Missing required PgBoss tables: ${MISSING_TABLES[*]}${NC}"
    echo -e "${YELLOW}üí° Run migrations to create PgBoss tables${NC}"
    exit 1
fi

# Test 2: Check job queue functionality
echo -e "\n${YELLOW}2Ô∏è‚É£ Testing job queue functionality...${NC}"

# Check if we can read from the job table
JOB_COUNT=$(echo "SELECT COUNT(*) FROM $BOSS_SCHEMA.job;" | psql "$DATABASE_URL" -t -A 2>/dev/null)

if [ -n "$JOB_COUNT" ]; then
    echo -e "${GREEN}‚úÖ Can read from job table (found $JOB_COUNT jobs)${NC}"
else
    echo -e "${RED}‚ùå Cannot read from job table${NC}"
    exit 1
fi

# Test basic job table structure
echo -e "${BLUE}üìã Checking job table structure...${NC}"
COLUMNS=$(echo "SELECT column_name FROM information_schema.columns WHERE table_schema = '$BOSS_SCHEMA' AND table_name = 'job' ORDER BY ordinal_position;" | psql "$DATABASE_URL" -t -A 2>/dev/null | tr '\n' ',' | sed 's/,$//')

if [ -n "$COLUMNS" ]; then
    echo -e "${GREEN}‚úÖ Job table columns: $COLUMNS${NC}"
else
    echo -e "${RED}‚ùå Cannot read job table structure${NC}"
    exit 1
fi

# Test 3: Check for failed jobs
echo -e "\n${YELLOW}3Ô∏è‚É£ Checking for failed jobs...${NC}"

FAILED_COUNT=$(echo "SELECT COUNT(*) FROM $BOSS_SCHEMA.job WHERE state = 'failed';" | psql "$DATABASE_URL" -t -A 2>/dev/null)

if [ "$FAILED_COUNT" -gt 0 ]; then
    echo -e "${YELLOW}‚ö†Ô∏è  Found $FAILED_COUNT failed job(s)${NC}"

    # Show recent failed jobs
    echo "Recent failed jobs:"
    echo "SELECT id, name, createdon, output FROM $BOSS_SCHEMA.job WHERE state = 'failed' ORDER BY createdon DESC LIMIT 5;" | psql "$DATABASE_URL" -t
else
    echo -e "${GREEN}‚úÖ No failed jobs found${NC}"
fi

# Test 4: Check queue statistics
echo -e "\n${YELLOW}4Ô∏è‚É£ Queue statistics...${NC}"

TOTAL_JOBS=$(echo "SELECT COUNT(*) FROM $BOSS_SCHEMA.job;" | psql "$DATABASE_URL" -t -A 2>/dev/null)
ACTIVE_JOBS=$(echo "SELECT COUNT(*) FROM $BOSS_SCHEMA.job WHERE state IN ('created', 'active', 'retry');" | psql "$DATABASE_URL" -t -A 2>/dev/null)
COMPLETED_JOBS=$(echo "SELECT COUNT(*) FROM $BOSS_SCHEMA.job WHERE state = 'completed';" | psql "$DATABASE_URL" -t -A 2>/dev/null)

echo -e "${BLUE}üìä Queue Statistics:${NC}"
echo -e "   Total jobs: $TOTAL_JOBS"
echo -e "   Active jobs: $ACTIVE_JOBS"
echo -e "   Completed jobs: $COMPLETED_JOBS"
echo -e "   Failed jobs: $FAILED_COUNT"

echo -e "\n${GREEN}üéâ Transcription pipeline test completed successfully!${NC}"

================
File: src/__tests__/integration.test.js
================
/**
 * Integration tests for the complete worker system
 *
 * These tests verify that the entire worker pipeline works correctly
 * from job triggering through to completion.
 */

import assert from 'node:assert/strict';
import { after, before, describe, test } from 'node:test';
import { WorkerService } from '../core/worker-service.js';

describe('Worker Integration Tests', () => {
  let workerService;
  let testPort;

  before(async () => {
    // Use a different port for testing to avoid conflicts
    testPort = 8081;
    process.env.PORT = testPort.toString();

    // Start worker service for testing
    workerService = new WorkerService();
    await workerService.start();

    // Wait a moment for everything to initialize
    await new Promise((resolve) => setTimeout(resolve, 1000));
  });

  after(async () => {
    if (workerService) {
      await workerService.stop();
    }
  });

  test('worker service should start and be healthy', async () => {
    const response = await fetch(`http://localhost:${testPort}/healthz`);
    assert.strictEqual(response.status, 200);

    const text = await response.text();
    assert.strictEqual(text, 'ok');
  });

  test('system status should return valid data', async () => {
    const response = await fetch(`http://localhost:${testPort}/debug/status`);
    assert.strictEqual(response.status, 200);

    const data = await response.json();
    assert.strictEqual(data.ok, true);
    assert.ok('sources' in data);
    assert.ok('raw' in data);
    assert.ok('contents' in data);
    assert.ok('jobs' in data);
  });

  test('RSS ingest trigger should work', async () => {
    const response = await fetch(
      `http://localhost:${testPort}/debug/ingest-now`,
      {
        method: 'POST',
      }
    );

    assert.strictEqual(response.status, 200);

    const data = await response.json();
    assert.strictEqual(data.ok, true);
  });

  test('YouTube ingest trigger should work', async () => {
    const response = await fetch(
      `http://localhost:${testPort}/debug/ingest-youtube`,
      {
        method: 'POST',
      }
    );

    // Should work even without API key (will just log warning)
    assert.strictEqual(response.status, 200);

    const data = await response.json();
    assert.strictEqual(data.ok, true);
  });

  test('one-off URL processing should work', async () => {
    const testUrls = ['https://example.com/test-article'];

    const response = await fetch(
      `http://localhost:${testPort}/debug/ingest-oneoff`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ urls: testUrls }),
      }
    );

    assert.strictEqual(response.status, 200);

    const data = await response.json();
    assert.strictEqual(data.ok, true);
    assert.ok(Array.isArray(data.results));
    assert.strictEqual(data.results.length, 1);
    assert.strictEqual(data.results[0].url, testUrls[0]);
    assert.strictEqual(data.results[0].type, 'article');
  });

  test('error handling should work correctly', async () => {
    // Test missing sourceId
    const response1 = await fetch(
      `http://localhost:${testPort}/debug/ingest-source`,
      {
        method: 'POST',
      }
    );

    assert.strictEqual(response1.status, 400);

    const data1 = await response1.json();
    assert.strictEqual(data1.ok, false);
    assert.strictEqual(data1.error, 'missing_sourceId');

    // Test invalid sourceId
    const response2 = await fetch(
      `http://localhost:${testPort}/debug/ingest-source?sourceId=invalid-id`,
      {
        method: 'POST',
      }
    );

    // Should return 404 for non-existent source
    assert.strictEqual(response2.status, 404);

    const data2 = await response2.json();
    assert.strictEqual(data2.ok, false);
    assert.strictEqual(data2.error, 'not_found');
  });

  test('parameter validation should work', async () => {
    // Test empty URLs array
    const response = await fetch(
      `http://localhost:${testPort}/debug/ingest-oneoff`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ urls: [] }),
      }
    );

    assert.strictEqual(response.status, 400);

    const data = await response.json();
    assert.strictEqual(data.ok, false);
    assert.strictEqual(data.error, 'no_urls');
  });

  test('legacy endpoints should return helpful messages', async () => {
    const response1 = await fetch(
      `http://localhost:${testPort}/debug/schedule-rss`,
      {
        method: 'POST',
      }
    );

    assert.strictEqual(response1.status, 200);

    const data1 = await response1.json();
    assert.strictEqual(data1.ok, true);
    assert.ok(data1.message.includes('automatic'));

    const response2 = await fetch(
      `http://localhost:${testPort}/debug/schedule-youtube`,
      {
        method: 'POST',
      }
    );

    assert.strictEqual(response2.status, 200);

    const data2 = await response2.json();
    assert.strictEqual(data2.ok, true);
    assert.ok(data2.message.includes('automatic'));
  });

  test('job orchestrator should be working', async () => {
    // Trigger multiple jobs and verify they're queued
    const promises = [
      fetch(`http://localhost:${testPort}/debug/ingest-now`, {
        method: 'POST',
      }),
      fetch(`http://localhost:${testPort}/debug/ingest-youtube`, {
        method: 'POST',
      }),
    ];

    const responses = await Promise.all(promises);

    for (const response of responses) {
      assert.strictEqual(response.status, 200);
      const data = await response.json();
      assert.strictEqual(data.ok, true);
    }

    // Wait a moment for jobs to be queued
    await new Promise((resolve) => setTimeout(resolve, 500));

    // Check that jobs are in the system
    const statusResponse = await fetch(
      `http://localhost:${testPort}/debug/status`
    );
    const statusData = await statusResponse.json();

    assert.strictEqual(statusData.ok, true);
    assert.ok(Array.isArray(statusData.jobs));
  });

  test('modular architecture should be active', async () => {
    // This test verifies that the new modular architecture is being used
    // by checking that the worker service is an instance of our new class
    assert.ok(workerService instanceof WorkerService);

    // Verify that the service has the expected methods
    assert.strictEqual(typeof workerService.start, 'function');
    assert.strictEqual(typeof workerService.stop, 'function');
  });
});

/**
 * Helper function to wait for a condition
 */
async function waitFor(condition, timeout = 5000) {
  const start = Date.now();
  while (Date.now() - start < timeout) {
    if (await condition()) {
      return true;
    }
    await new Promise((resolve) => setTimeout(resolve, 100));
  }
  return false;
}

================
File: src/core/__tests__/job-definitions.test.js
================
/**
 * Unit tests for Job Definitions
 *
 * Tests the job configuration system to ensure queues, workers,
 * and schedules are set up correctly.
 */

import assert from 'node:assert/strict';
import { describe, mock, test } from 'node:test';
import {
  JOB_CONFIG,
  QUEUES,
  createJobQueues,
  scheduleRecurringJobs,
  setupJobWorkers,
  triggerStartupJobs,
} from '../job-definitions.js';

describe('Job Definitions', () => {
  test('QUEUES should contain all expected queue names', () => {
    const expectedQueues = [
      'SYSTEM_HEARTBEAT',
      'INGEST_PULL',
      'INGEST_SOURCE',
      'INGEST_FETCH_CONTENT',
      'INGEST_FETCH_YOUTUBE_CONTENT',
      'ANALYZE_LLM',
    ];

    for (const queueKey of expectedQueues) {
      assert.ok(queueKey in QUEUES, `Missing queue: ${queueKey}`);
      assert.strictEqual(
        typeof QUEUES[queueKey],
        'string',
        `Queue ${queueKey} should be string`
      );
    }

    // Verify queue names follow expected pattern
    assert.strictEqual(QUEUES.SYSTEM_HEARTBEAT, 'system:heartbeat');
    assert.strictEqual(QUEUES.INGEST_PULL, 'ingest:pull');
    assert.strictEqual(QUEUES.INGEST_SOURCE, 'ingest:source');
    assert.strictEqual(QUEUES.INGEST_FETCH_CONTENT, 'ingest:fetch-content');
    assert.strictEqual(
      QUEUES.INGEST_FETCH_YOUTUBE_CONTENT,
      'ingest:fetch-youtube-content'
    );
    assert.strictEqual(QUEUES.ANALYZE_LLM, 'analyze:llm');
  });

  test('JOB_CONFIG should contain all expected configuration', () => {
    const expectedConfigs = [
      'HEARTBEAT_BATCH',
      'INGEST_PULL_BATCH',
      'CONTENT_FETCH_BATCH',
      'YT_FETCH_BATCH',
      'CRON_TZ',
    ];

    for (const configKey of expectedConfigs) {
      assert.ok(configKey in JOB_CONFIG, `Missing config: ${configKey}`);
    }

    // Verify types
    assert.strictEqual(typeof JOB_CONFIG.HEARTBEAT_BATCH, 'number');
    assert.strictEqual(typeof JOB_CONFIG.INGEST_PULL_BATCH, 'number');
    assert.strictEqual(typeof JOB_CONFIG.CONTENT_FETCH_BATCH, 'number');
    assert.strictEqual(typeof JOB_CONFIG.YT_FETCH_BATCH, 'number');
    assert.strictEqual(typeof JOB_CONFIG.CRON_TZ, 'string');
  });

  test('createJobQueues should create all queues', async () => {
    const mockBoss = createMockBoss();

    await createJobQueues(mockBoss);

    // Should create all queues
    assert.strictEqual(
      mockBoss.createQueueCalls.length,
      Object.keys(QUEUES).length
    );

    // Verify all queue names were created
    const createdQueues = mockBoss.createQueueCalls.map(
      (call) => call.queueName
    );
    const expectedQueues = Object.values(QUEUES);

    for (const expectedQueue of expectedQueues) {
      assert.ok(
        createdQueues.includes(expectedQueue),
        `Queue not created: ${expectedQueue}`
      );
    }
  });

  test('scheduleRecurringJobs should schedule all recurring jobs', async () => {
    const mockBoss = createMockBoss();

    await scheduleRecurringJobs(mockBoss);

    // Should schedule 3 recurring jobs: heartbeat, rss, youtube
    assert.strictEqual(mockBoss.scheduleCalls.length, 3);

    // Verify heartbeat schedule
    const heartbeatSchedule = mockBoss.scheduleCalls.find(
      (call) => call.queueName === QUEUES.SYSTEM_HEARTBEAT
    );
    assert.ok(heartbeatSchedule, 'Heartbeat schedule not found');
    assert.strictEqual(heartbeatSchedule.cron, '*/5 * * * *');
    assert.deepStrictEqual(heartbeatSchedule.data, { ping: 'ok' });

    // Verify RSS schedule
    const rssSchedule = mockBoss.scheduleCalls.find(
      (call) =>
        call.queueName === QUEUES.INGEST_PULL && call.data.source === 'rss'
    );
    assert.ok(rssSchedule, 'RSS schedule not found');
    assert.strictEqual(rssSchedule.cron, '*/5 * * * *');

    // Verify YouTube schedule
    const youtubeSchedule = mockBoss.scheduleCalls.find(
      (call) =>
        call.queueName === QUEUES.INGEST_PULL && call.data.source === 'youtube'
    );
    assert.ok(youtubeSchedule, 'YouTube schedule not found');
    assert.strictEqual(youtubeSchedule.cron, '*/15 * * * *');
  });

  test('triggerStartupJobs should send initial jobs', async () => {
    const mockBoss = createMockBoss();

    await triggerStartupJobs(mockBoss);

    // Should send 2 startup jobs: rss and youtube
    assert.strictEqual(mockBoss.sendCalls.length, 2);

    // Verify RSS startup job
    const rssJob = mockBoss.sendCalls.find(
      (call) => call.queue === QUEUES.INGEST_PULL && call.data.source === 'rss'
    );
    assert.ok(rssJob, 'RSS startup job not found');

    // Verify YouTube startup job
    const youtubeJob = mockBoss.sendCalls.find(
      (call) =>
        call.queue === QUEUES.INGEST_PULL && call.data.source === 'youtube'
    );
    assert.ok(youtubeJob, 'YouTube startup job not found');
  });

  test('setupJobWorkers should set up all workers', async () => {
    const mockBoss = createMockBoss();
    const mockOrchestrator = createMockOrchestrator();

    await setupJobWorkers(mockBoss, mockOrchestrator);

    // Should set up workers for all queues
    assert.strictEqual(mockBoss.workCalls.length, Object.keys(QUEUES).length);

    // Verify all queue names have workers
    const workerQueues = mockBoss.workCalls.map((call) => call.queueName);
    const expectedQueues = Object.values(QUEUES);

    for (const expectedQueue of expectedQueues) {
      assert.ok(
        workerQueues.includes(expectedQueue),
        `Worker not set up for: ${expectedQueue}`
      );
    }

    // Verify batch sizes are configured
    for (const workCall of mockBoss.workCalls) {
      assert.ok(
        'batchSize' in workCall.options,
        `Batch size not configured for ${workCall.queueName}`
      );
      assert.strictEqual(typeof workCall.options.batchSize, 'number');
    }
  });

  test('should handle errors in startup jobs gracefully', async () => {
    const mockBoss = {
      send: mock.fn(async () => {
        throw new Error('Mock send error');
      }),
    };

    // Should not throw - errors should be caught and logged
    await assert.doesNotReject(() => triggerStartupJobs(mockBoss));
  });
});

/**
 * Create a mock pg-boss instance for testing
 */
function createMockBoss() {
  const createQueueCalls = [];
  const scheduleCalls = [];
  const sendCalls = [];
  const workCalls = [];

  return {
    createQueueCalls,
    scheduleCalls,
    sendCalls,
    workCalls,

    createQueue: mock.fn(async (queueName) => {
      createQueueCalls.push({ queueName });
    }),

    schedule: mock.fn(async (queueName, cron, data, options) => {
      scheduleCalls.push({ queueName, cron, data, options });
    }),

    send: mock.fn(async (queue, data) => {
      sendCalls.push({ queue, data });
      return 'mock-job-id';
    }),

    work: mock.fn(async (queueName, options, handler) => {
      workCalls.push({ queueName, options, handler });
    }),
  };
}

/**
 * Create a mock orchestrator for testing
 */
function createMockOrchestrator() {
  return {
    triggerRssIngest: mock.fn(),
    triggerYouTubeIngest: mock.fn(),
    triggerContentExtraction: mock.fn(),
    triggerStoryAnalysis: mock.fn(),
  };
}

================
File: src/core/__tests__/job-orchestrator.test.js
================
/**
 * Unit tests for Job Orchestrator
 *
 * Tests the central job triggering system to ensure all methods
 * work correctly and consistently trigger the right jobs.
 */

import assert from 'node:assert/strict';
import { describe, mock, test } from 'node:test';
import { createJobOrchestrator } from '../job-orchestrator.js';

describe('Job Orchestrator', () => {
  test('should create orchestrator with all required methods', () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    // Verify all expected methods exist
    const expectedMethods = [
      'triggerRssIngest',
      'triggerRssSourceIngest',
      'triggerYouTubeIngest',
      'triggerYouTubeSourceIngest',
      'triggerContentExtraction',
      'triggerYouTubeContentExtraction',
      'triggerStoryAnalysis',
      'triggerOneOffIngest',
    ];

    for (const method of expectedMethods) {
      assert.strictEqual(
        typeof orchestrator[method],
        'function',
        `Missing method: ${method}`
      );
    }
  });

  test('triggerRssIngest should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    await orchestrator.triggerRssIngest();

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(mockBoss.sendCalls[0].queue, 'ingest:pull');
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, { source: 'rss' });
  });

  test('triggerYouTubeIngest should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    await orchestrator.triggerYouTubeIngest();

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(mockBoss.sendCalls[0].queue, 'ingest:pull');
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, { source: 'youtube' });
  });

  test('triggerRssSourceIngest should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    await orchestrator.triggerRssSourceIngest('source-123');

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(mockBoss.sendCalls[0].queue, 'ingest:source');
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, {
      sourceId: 'source-123',
      kind: 'rss',
    });
  });

  test('triggerYouTubeSourceIngest should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    await orchestrator.triggerYouTubeSourceIngest('yt-source-456');

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(mockBoss.sendCalls[0].queue, 'ingest:source');
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, {
      sourceId: 'yt-source-456',
      kind: 'youtube',
    });
  });

  test('triggerContentExtraction should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    const rawItemIds = ['item1', 'item2', 'item3'];
    await orchestrator.triggerContentExtraction(rawItemIds);

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(mockBoss.sendCalls[0].queue, 'ingest:fetch-content');
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, { rawItemIds });
  });

  test('triggerYouTubeContentExtraction should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    const data = {
      rawItemIds: ['yt-item1'],
      videoId: 'abc123',
      sourceKind: 'youtube_channel',
    };

    await orchestrator.triggerYouTubeContentExtraction(data);

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(
      mockBoss.sendCalls[0].queue,
      'ingest:fetch-youtube-content'
    );
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, data);
  });

  test('triggerStoryAnalysis should send correct job', async () => {
    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    await orchestrator.triggerStoryAnalysis('story-789');

    assert.strictEqual(mockBoss.sendCalls.length, 1);
    assert.strictEqual(mockBoss.sendCalls[0].queue, 'analyze:llm');
    assert.deepStrictEqual(mockBoss.sendCalls[0].data, {
      storyId: 'story-789',
    });
  });

  test('triggerOneOffIngest should process URLs correctly', async () => {
    // Mock the database functions
    const originalImport = global.import;
    global.import = mock.fn(async (path) => {
      if (path === '../db.js') {
        return {
          getOrCreateManualSource: mock.fn(async () => 'mock-source-id'),
          upsertRawItem: mock.fn(async () => 'mock-raw-id'),
        };
      }
      return originalImport(path);
    });

    const mockBoss = createMockBoss();
    const orchestrator = createJobOrchestrator(mockBoss);

    const urls = [
      'https://example.com/article',
      'https://youtube.com/watch?v=abc123',
    ];

    const results = await orchestrator.triggerOneOffIngest(urls);

    assert.strictEqual(results.length, 2);
    assert.strictEqual(results[0].type, 'article');
    assert.strictEqual(results[1].type, 'youtube');

    // Should have sent jobs for both items
    assert.strictEqual(mockBoss.sendCalls.length, 2);

    // Restore original import
    global.import = originalImport;
  });

  test('should handle errors gracefully', async () => {
    const mockBoss = {
      send: mock.fn(async () => {
        throw new Error('Mock boss error');
      }),
    };

    const orchestrator = createJobOrchestrator(mockBoss);

    // Should propagate errors
    await assert.rejects(
      () => orchestrator.triggerRssIngest(),
      /Mock boss error/
    );
  });
});

/**
 * Create a mock pg-boss instance for testing
 */
function createMockBoss() {
  const sendCalls = [];

  return {
    sendCalls, // Expose for assertions
    send: mock.fn(async (queue, data) => {
      sendCalls.push({ queue, data });
      return 'mock-job-id';
    }),
  };
}

================
File: src/core/job-definitions.ts
================
/**
 * Job Definitions - All pg-boss job configurations in one place
 *
 * This module defines all queues, workers, and scheduled jobs.
 * Each job follows a consistent pattern for easy understanding.
 *
 * Job Types:
 * - System jobs: heartbeat, monitoring
 * - Ingest jobs: RSS and YouTube source processing
 * - Content jobs: article and video content extraction
 * - Analysis jobs: LLM processing and embeddings
 */

import type PgBoss from 'pg-boss';
import { log } from '../log.js';
import type { JobOrchestrator } from './job-orchestrator.js';

// Job configuration constants
export const JOB_CONFIG = {
  HEARTBEAT_BATCH: 10,
  INGEST_PULL_BATCH: 5,
  CONTENT_FETCH_BATCH: 5,
  YT_FETCH_BATCH: 2,
  CRON_TZ: process.env.BOSS_CRON_TZ || 'UTC',
} as const;

// Queue names for type safety
export const QUEUES = {
  SYSTEM_HEARTBEAT: 'system:heartbeat',
  INGEST_PULL: 'ingest:pull',
  INGEST_SOURCE: 'ingest:source',
  INGEST_FETCH_CONTENT: 'ingest:fetch-content',
  INGEST_FETCH_YOUTUBE_CONTENT: 'ingest:fetch-youtube-content',
  ANALYZE_LLM: 'analyze:llm',
} as const;

// Job data types
export interface IngestPullJobData {
  source: 'rss' | 'youtube';
}

export interface IngestSourceJobData {
  sourceId: string;
  kind: 'rss' | 'youtube';
}

export interface FetchContentJobData {
  rawItemIds: string[];
}

export interface YouTubeExtractionJobData {
  rawItemIds: string[];
  videoId: string;
  sourceKind: string;
}

export interface AnalyzeJobData {
  storyId: string;
}

/**
 * Sets up all job queues
 */
export async function createJobQueues(boss: PgBoss): Promise<void> {
  log('job_queues_creating', { count: Object.keys(QUEUES).length });

  await Promise.all([
    boss.createQueue(QUEUES.SYSTEM_HEARTBEAT),
    boss.createQueue(QUEUES.INGEST_PULL),
    boss.createQueue(QUEUES.INGEST_SOURCE),
    boss.createQueue(QUEUES.INGEST_FETCH_CONTENT),
    boss.createQueue(QUEUES.INGEST_FETCH_YOUTUBE_CONTENT),
    boss.createQueue(QUEUES.ANALYZE_LLM),
  ]);

  log('job_queues_created', { queues: Object.values(QUEUES) });
}

/**
 * Sets up scheduled jobs (cron-based)
 */
export async function scheduleRecurringJobs(boss: PgBoss): Promise<void> {
  log('scheduled_jobs_creating', {});

  // System heartbeat every 5 minutes
  await boss.schedule(
    QUEUES.SYSTEM_HEARTBEAT,
    '*/5 * * * *',
    { ping: 'ok' },
    { tz: JOB_CONFIG.CRON_TZ }
  );

  // RSS ingest every 5 minutes
  await boss.schedule(
    QUEUES.INGEST_PULL,
    '*/5 * * * *',
    { source: 'rss' } as IngestPullJobData,
    { tz: JOB_CONFIG.CRON_TZ }
  );

  // YouTube ingest every 15 minutes
  await boss.schedule(
    QUEUES.INGEST_PULL,
    '*/15 * * * *',
    { source: 'youtube' } as IngestPullJobData,
    { tz: JOB_CONFIG.CRON_TZ }
  );

  log('scheduled_jobs_created', { timezone: JOB_CONFIG.CRON_TZ });
}

/**
 * Triggers initial jobs on startup for faster feedback
 */
export async function triggerStartupJobs(boss: PgBoss): Promise<void> {
  try {
    await boss.send(QUEUES.INGEST_PULL, { source: 'rss' } as IngestPullJobData);
    await boss.send(QUEUES.INGEST_PULL, {
      source: 'youtube',
    } as IngestPullJobData);
    log('startup_jobs_queued', { when: 'startup' });
  } catch (e) {
    log('startup_jobs_error', { err: String(e) }, 'warn');
  }
}

/**
 * Sets up all job workers
 */
export async function setupJobWorkers(
  boss: PgBoss,
  _orchestrator: JobOrchestrator
): Promise<void> {
  log('job_workers_setting_up', {});

  // System heartbeat worker
  await boss.work(
    QUEUES.SYSTEM_HEARTBEAT,
    { batchSize: JOB_CONFIG.HEARTBEAT_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        log('heartbeat', { jobId: job.id, data: job.data });
        await boss.complete(QUEUES.SYSTEM_HEARTBEAT, job.id);
      }
    }
  );

  // Ingest pull worker (handles scheduled RSS/YouTube ingests)
  await boss.work(
    QUEUES.INGEST_PULL,
    { batchSize: JOB_CONFIG.INGEST_PULL_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        await processIngestPullJob(boss, job);
      }
    }
  );

  // Individual source ingest worker
  await boss.work(
    QUEUES.INGEST_SOURCE,
    { batchSize: JOB_CONFIG.INGEST_PULL_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        await processIngestSourceJob(boss, job);
      }
    }
  );

  // Content extraction worker
  await boss.work(
    QUEUES.INGEST_FETCH_CONTENT,
    { batchSize: JOB_CONFIG.CONTENT_FETCH_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        await processContentExtractionJob(boss, job);
      }
    }
  );

  // YouTube content extraction worker
  await boss.work(
    QUEUES.INGEST_FETCH_YOUTUBE_CONTENT,
    { batchSize: JOB_CONFIG.YT_FETCH_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        await processYouTubeExtractionJob(boss, job);
      }
    }
  );

  // Story analysis worker
  await boss.work(
    QUEUES.ANALYZE_LLM,
    { batchSize: JOB_CONFIG.CONTENT_FETCH_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        await processAnalysisJob(boss, job);
      }
    }
  );

  log('job_workers_ready', { workers: Object.keys(QUEUES).length });
}

// Job processing functions (extracted from worker.ts for clarity)

async function processIngestPullJob(
  boss: PgBoss,
  job: { id: string; data?: unknown }
): Promise<void> {
  const { source } = (job.data || {}) as IngestPullJobData;
  log('ingest_pull_start', { jobId: job.id, source });

  if (source === 'rss') {
    await handleRssIngest(boss);
  } else if (source === 'youtube') {
    await handleYouTubeIngest(boss);
  }

  await boss.complete(QUEUES.INGEST_PULL, job.id);
  log('ingest_pull_done', { jobId: job.id, source });
}

async function processIngestSourceJob(
  boss: PgBoss,
  job: { id: string; data?: unknown }
): Promise<void> {
  const { sourceId, kind } = (job.data || {}) as IngestSourceJobData;
  log('ingest_source_start', { jobId: job.id, sourceId, kind });

  try {
    await ingestSourceById(boss, sourceId);
    await boss.complete(QUEUES.INGEST_SOURCE, job.id);
  } catch (err) {
    await boss.fail(QUEUES.INGEST_SOURCE, job.id, { error: String(err) });
  }

  log('ingest_source_done', { jobId: job.id, sourceId });
}

async function processContentExtractionJob(
  boss: PgBoss,
  job: { id: string; data?: unknown }
): Promise<void> {
  log('fetch_content_start', { jobId: job.id });
  try {
    const { extractArticle } = await import('../tasks/extract-article.js');
    await extractArticle(job.data as FetchContentJobData, boss);
    await boss.complete(QUEUES.INGEST_FETCH_CONTENT, job.id);
  } catch (err) {
    await boss.fail(QUEUES.INGEST_FETCH_CONTENT, job.id, {
      error: String(err),
    });
  }
  log('fetch_content_done', { jobId: job.id });
}

async function processYouTubeExtractionJob(
  boss: PgBoss,
  job: { id: string; data?: unknown }
): Promise<void> {
  log('fetch_youtube_content_start', { jobId: job.id });
  try {
    const { extractYouTubeContent } = await import(
      '../tasks/extract-youtube-content.js'
    );
    await extractYouTubeContent(job.data as YouTubeExtractionJobData, boss);
    await boss.complete(QUEUES.INGEST_FETCH_YOUTUBE_CONTENT, job.id);
  } catch (err) {
    await boss.fail(QUEUES.INGEST_FETCH_YOUTUBE_CONTENT, job.id, {
      error: String(err),
    });
  }
  log('fetch_youtube_content_done', { jobId: job.id });
}

async function processAnalysisJob(
  boss: PgBoss,
  job: { id: string; data?: unknown }
): Promise<void> {
  const { storyId } = (job.data as AnalyzeJobData) || {};
  log('analyze_llm_start', { jobId: job.id, storyId });
  try {
    if (!storyId) {
      throw new Error('Missing storyId in job data');
    }
    const { analyzeStory } = await import('../tasks/analyze-story.js');
    await analyzeStory(storyId);
    await boss.complete(QUEUES.ANALYZE_LLM, job.id);
  } catch (err) {
    await boss.fail(QUEUES.ANALYZE_LLM, job.id, { error: String(err) });
  }
  log('analyze_llm_done', { jobId: job.id, storyId });
}

// Helper functions (moved from worker.ts)

async function handleRssIngest(boss: PgBoss): Promise<void> {
  const { getRssSources } = await import('../db.js');
  const { ingestRssSource } = await import('../tasks/ingest-rss-source.js');

  const sources = await getRssSources();
  for (const src of sources) {
    if (!src.url) {
      continue;
    }
    await ingestRssSource(boss, { id: src.id, url: src.url });
  }
}

async function handleYouTubeIngest(boss: PgBoss): Promise<void> {
  if (!process.env.YOUTUBE_API_KEY) {
    log('ingest_youtube_skipped', { reason: 'missing_api_key' }, 'warn');
    return;
  }

  const { getYouTubeSources, upsertPlatformQuota } = await import('../db.js');
  const { ingestYouTubeSource } = await import(
    '../tasks/ingest-youtube-source.js'
  );
  const { quotaTracker } = await import('../utils/quota-tracker.js');

  const sources = await getYouTubeSources();
  log('ingest_youtube_start', { sourceCount: sources.length });

  for (const src of sources) {
    await ingestYouTubeSource(boss, src);
  }

  // Update quota tracking
  const status = quotaTracker.checkQuotaStatus();
  try {
    const resetHour = Number(process.env.YOUTUBE_QUOTA_RESET_HOUR || '0') || 0;
    const now = new Date();
    const resetAt = computeQuotaResetAt(now, resetHour);
    await upsertPlatformQuota('youtube', {
      limit: Number(process.env.YOUTUBE_QUOTA_LIMIT || '10000'),
      used: status.used,
      remaining: status.remaining,
      reset_at: resetAt.toISOString(),
    });
  } catch (e) {
    log('platform_quota_update_error', { error: String(e) }, 'debug');
  }

  log('ingest_youtube_complete', { sourcesProcessed: sources.length });
}

async function ingestSourceById(boss: PgBoss, sourceId: string): Promise<void> {
  const pg = (await import('../db.js')).default;
  const { rows } = await pg.query(
    'select id, kind from public.sources where id = $1',
    [sourceId]
  );
  const row = rows[0];

  if (!row) {
    throw new Error('not_found');
  }

  if (row.kind === 'rss' || row.kind === 'podcast') {
    const { ingestRssSource } = await import('../tasks/ingest-rss-source.js');
    const { getSourceById } = await import('../db.js');
    const srcFull = await getSourceById(sourceId);
    if (!srcFull?.url) {
      throw new Error('no_url_for_source');
    }
    await ingestRssSource(boss, { id: sourceId, url: srcFull.url });
    return;
  }

  if (row.kind === 'youtube_channel' || row.kind === 'youtube_search') {
    const { ingestYouTubeSource } = await import(
      '../tasks/ingest-youtube-source.js'
    );
    const { getSourceById } = await import('../db.js');
    const srcFull = await getSourceById(sourceId);
    if (!srcFull) {
      throw new Error('not_found');
    }
    await ingestYouTubeSource(boss, srcFull);
    return;
  }

  throw new Error('unsupported_kind');
}

function computeQuotaResetAt(now: Date, resetHour: number): Date {
  const resetAt = new Date(
    now.getFullYear(),
    now.getMonth(),
    now.getDate(),
    resetHour,
    0,
    0,
    0
  );
  if (resetAt.getTime() < now.getTime()) {
    resetAt.setDate(resetAt.getDate() + 1);
  }
  return resetAt;
}

================
File: src/core/job-orchestrator.ts
================
/**
 * Job Orchestrator - Central hub for triggering all jobs consistently
 *
 * This module eliminates the confusion between HTTP endpoints and queue jobs
 * by providing a single, consistent way to trigger any job in the system.
 *
 * Key principles:
 * - All jobs are triggered through this orchestrator
 * - HTTP endpoints only call orchestrator methods
 * - Scheduled jobs only call orchestrator methods
 * - No business logic duplication
 */

import type PgBoss from 'pg-boss';
import { log } from '../log.js';

export interface JobOrchestrator {
  // RSS Operations
  triggerRssIngest(): Promise<void>;
  triggerRssSourceIngest(sourceId: string): Promise<void>;

  // YouTube Operations
  triggerYouTubeIngest(): Promise<void>;
  triggerYouTubeSourceIngest(sourceId: string): Promise<void>;

  // Content Processing
  triggerContentExtraction(rawItemIds: string[]): Promise<void>;
  triggerYouTubeContentExtraction(data: YouTubeExtractionData): Promise<void>;

  // Analysis
  triggerStoryAnalysis(storyId: string): Promise<void>;

  // One-off Operations
  triggerOneOffIngest(urls: string[]): Promise<OneOffIngestResult[]>;
}

export interface YouTubeExtractionData {
  rawItemIds: string[];
  videoId: string;
  sourceKind: string;
}

export interface OneOffIngestResult {
  url: string;
  ok: boolean;
  raw_item_id?: string;
  error?: string;
  type: 'youtube' | 'article';
}

/**
 * Creates a job orchestrator that provides consistent job triggering
 */
export function createJobOrchestrator(boss: PgBoss): JobOrchestrator {
  return {
    async triggerRssIngest(): Promise<void> {
      log('orchestrator_trigger', { job: 'rss_ingest', trigger: 'manual' });
      await boss.send('ingest:pull', { source: 'rss' });
    },

    async triggerRssSourceIngest(sourceId: string): Promise<void> {
      log('orchestrator_trigger', {
        job: 'rss_source_ingest',
        sourceId,
        trigger: 'manual',
      });
      await boss.send('ingest:source', { sourceId, kind: 'rss' });
    },

    async triggerYouTubeIngest(): Promise<void> {
      log('orchestrator_trigger', { job: 'youtube_ingest', trigger: 'manual' });
      await boss.send('ingest:pull', { source: 'youtube' });
    },

    async triggerYouTubeSourceIngest(sourceId: string): Promise<void> {
      log('orchestrator_trigger', {
        job: 'youtube_source_ingest',
        sourceId,
        trigger: 'manual',
      });
      await boss.send('ingest:source', { sourceId, kind: 'youtube' });
    },

    async triggerContentExtraction(rawItemIds: string[]): Promise<void> {
      log('orchestrator_trigger', {
        job: 'content_extraction',
        count: rawItemIds.length,
      });
      await boss.send('ingest:fetch-content', { rawItemIds });
    },

    async triggerYouTubeContentExtraction(
      data: YouTubeExtractionData
    ): Promise<void> {
      log('orchestrator_trigger', {
        job: 'youtube_content_extraction',
        videoId: data.videoId,
      });
      await boss.send('ingest:fetch-youtube-content', data);
    },

    async triggerStoryAnalysis(storyId: string): Promise<void> {
      log('orchestrator_trigger', { job: 'story_analysis', storyId });
      await boss.send('analyze:llm', { storyId });
    },

    async triggerOneOffIngest(urls: string[]): Promise<OneOffIngestResult[]> {
      log('orchestrator_trigger', { job: 'oneoff_ingest', count: urls.length });

      const results: OneOffIngestResult[] = [];

      for (const url of urls) {
        const kind = classifyUrl(url);
        if (kind === 'youtube') {
          results.push(await processYouTubeUrl(url, boss));
        } else {
          results.push(await processArticleUrl(url, boss));
        }
      }

      return results;
    },
  };
}

// Helper functions (moved from worker.ts)
function classifyUrl(url: string): 'youtube' | 'article' {
  try {
    const parsedUrl = new URL(url);
    const hostname = parsedUrl.hostname || '';
    if (hostname.includes('youtube.com') || hostname.includes('youtu.be')) {
      return 'youtube';
    }
    return 'article';
  } catch {
    return 'article';
  }
}

function extractYouTubeVideoId(url: string): string {
  try {
    const parsedUrl = new URL(url);
    if (parsedUrl.hostname.includes('youtu.be')) {
      return parsedUrl.pathname.replace('/', '');
    }
    if (parsedUrl.pathname.startsWith('/shorts/')) {
      return parsedUrl.pathname.split('/')[2] || '';
    }
    return parsedUrl.searchParams.get('v') || '';
  } catch {
    return '';
  }
}

async function processYouTubeUrl(
  url: string,
  boss: PgBoss
): Promise<OneOffIngestResult> {
  const { getOrCreateManualSource, upsertRawItem } = await import('../db.js');

  const videoId = extractYouTubeVideoId(url);
  if (!videoId) {
    return { url, ok: false, error: 'no_video_id', type: 'youtube' };
  }

  const sourceId = await getOrCreateManualSource(
    'youtube_manual',
    'youtube.com',
    'YouTube Manual',
    null
  );

  const rawId = await upsertRawItem({
    source_id: sourceId,
    external_id: videoId,
    url,
    title: null,
    kind: 'youtube',
    metadata: { src: 'manual' },
  });

  if (rawId) {
    await boss.send('ingest:fetch-youtube-content', {
      rawItemIds: [rawId],
      videoId,
      sourceKind: 'youtube_manual',
    });
    return { url, ok: true, raw_item_id: rawId, type: 'youtube' };
  }

  return { url, ok: false, error: 'duplicate', type: 'youtube' };
}

async function processArticleUrl(
  url: string,
  boss: PgBoss
): Promise<OneOffIngestResult> {
  const { getOrCreateManualSource, upsertRawItem } = await import('../db.js');

  let domain: string | null = null;
  try {
    domain = new URL(url).hostname || null;
  } catch {
    domain = null;
  }

  const sourceId = await getOrCreateManualSource(
    'manual',
    domain,
    domain,
    null
  );

  const rawId = await upsertRawItem({
    source_id: sourceId,
    external_id: url,
    url,
    title: null,
    kind: 'article',
    metadata: { src: 'manual' },
  });

  if (rawId) {
    await boss.send('ingest:fetch-content', { rawItemIds: [rawId] });
    return { url, ok: true, raw_item_id: rawId, type: 'article' };
  }

  return { url, ok: false, error: 'duplicate', type: 'article' };
}

================
File: src/core/worker-service.ts
================
/**
 * Worker Service - Main service orchestrator
 *
 * This is the new main entry point that coordinates all worker components.
 * It replaces the monolithic worker.ts with a clean, modular architecture.
 *
 * Responsibilities:
 * - Initialize pg-boss connection
 * - Set up job queues and workers
 * - Start HTTP server
 * - Handle graceful shutdown
 * - Coordinate all subsystems
 */

import { config } from 'dotenv';

// Load environment variables from .env.development if it exists
config({ path: '.env.development' });
config({ path: '.env.local' });
config({ path: '.env' });
import type { ConnectionOptions as TlsConnectionOptions } from 'node:tls';
import express from 'express';
import PgBoss from 'pg-boss';
import { setupRoutes } from '../http/routes.js';
import { log } from '../log.js';
import {
  createJobQueues,
  scheduleRecurringJobs,
  setupJobWorkers,
  triggerStartupJobs,
} from './job-definitions.js';
import { createJobOrchestrator } from './job-orchestrator.js';

// Configuration from environment
const DATABASE_URL: string = process.env.DATABASE_URL ?? '';
const BOSS_SCHEMA = process.env.BOSS_SCHEMA || 'pgboss';
const BOSS_MIGRATE = process.env.BOSS_MIGRATE !== 'false';
const DEFAULT_PORT = 8080;
const RETRY_DELAY_MS = 10_000;
const METRICS_INTERVAL_MS = 3000;

// SSL configuration for database connection
const USE_SSL = !(
  DATABASE_URL.includes('127.0.0.1') ||
  DATABASE_URL.includes('localhost') ||
  DATABASE_URL.includes('host.docker.internal')
);

// Global state
let bossRef: PgBoss | null = null;

/**
 * Main worker service class
 */
export class WorkerService {
  private boss: PgBoss | null = null;
  private app: express.Express;
  private server: any = null;
  private isReady: boolean = false;

  constructor() {
    this.app = express();
    this.setupExpress();
    this.setupErrorHandlers();
  }

  /**
   * Start the worker service
   */
  async start(): Promise<void> {
    log('worker_service_starting', {
      nodeEnv: process.env.NODE_ENV,
      port: this.getPort(),
      databaseUrl: DATABASE_URL ? 'configured' : 'missing',
      bossSchema: BOSS_SCHEMA,
      bossMigrate: BOSS_MIGRATE,
      platform: process.platform,
      nodeVersion: process.version,
      pid: process.pid,
      railway: process.env.RAILWAY_ENVIRONMENT || 'not_detected'
    });

    try {
      // Validate environment variables
      await this.validateEnvironment();
      log('worker_service_env_validated', { message: 'Environment validation passed' });

      // Start HTTP server first for health checks
      await this.startHttpServer();
      log('worker_service_http_started', { port: this.getPort(), message: 'HTTP server ready for health checks' });

      // Initialize pg-boss with retry logic
      await this.initializeBossWithRetry();
      log('worker_service_boss_initialized', { message: 'pg-boss initialization completed' });

      // Start metrics collection
      this.startMetricsCollection();
      log('worker_service_metrics_started', { message: 'Metrics collection started' });

      // Mark service as ready
      this.isReady = true;

      log('worker_service_ready', {
        port: this.getPort(),
        message: 'All systems operational',
        healthEndpoint: `/healthz`,
        readyEndpoint: `/ready`,
        statusEndpoint: `/debug/status`
      });
    } catch (error) {
      log('worker_service_startup_failed', {
        error: String(error),
        stack: error instanceof Error ? error.stack : undefined,
        port: this.getPort(),
        databaseConfigured: !!DATABASE_URL,
        message: 'Worker service startup failed'
      }, 'error');
      throw error;
    }
  }

  /**
   * Stop the worker service gracefully
   */
  async stop(): Promise<void> {
    log('worker_service_stopping', {});

    try {
      if (this.boss) {
        await this.boss.stop({ graceful: true });
      }
      if (this.server) {
        this.server.close();
      }
    } catch (error) {
      log('worker_service_stop_error', { error: String(error) }, 'warn');
    }

    log('worker_service_stopped', {});
  }

  /**
   * Get the HTTP port
   */
  private getPort(): number {
    return Number(process.env.PORT || DEFAULT_PORT);
  }

  /**
   * Validate required environment variables
   */
  private async validateEnvironment(): Promise<void> {
    log('env_validation_starting', {});

    const requiredVars = ['DATABASE_URL'];
    const missingVars = requiredVars.filter(varName => !process.env[varName]);

    if (missingVars.length > 0) {
      log('env_validation_failed', {
        missingVars,
        message: 'Required environment variables are missing'
      }, 'error');
      throw new Error(`Missing required environment variables: ${missingVars.join(', ')}`);
    }

    const optionalVars = ['OPENAI_API_KEY', 'YOUTUBE_API_KEY'];
    const presentOptional = optionalVars.filter(varName => process.env[varName]);
    const missingOptional = optionalVars.filter(varName => !process.env[varName]);

    log('env_validation_complete', {
      requiredVars: requiredVars.length,
      optionalPresent: presentOptional,
      optionalMissing: missingOptional,
      message: 'Environment validation passed'
    });
  }

  /**
   * Test database connection before initializing pg-boss
   */
  private async testDatabaseConnection(): Promise<void> {
    log('db_connection_testing', {
      message: 'Testing database connectivity'
    });

    try {
      // Import pg client for connection test
      const { Client } = await import('pg');

      const client = new Client({
        connectionString: DATABASE_URL,
        ssl: USE_SSL ? { rejectUnauthorized: false } : false,
      });

      await client.connect();
      log('db_connection_established', { message: 'Database connection successful' });

      // Test basic query
      const result = await client.query('SELECT version() as version, current_database() as database');
      log('db_connection_verified', {
        version: result.rows[0]?.version?.split(' ')[0] || 'unknown',
        database: result.rows[0]?.database || 'unknown',
        message: 'Database query test successful'
      });

      await client.end();
      log('db_connection_closed', { message: 'Test connection closed' });

    } catch (error) {
      log('db_connection_failed', {
        error: String(error),
        databaseUrl: DATABASE_URL.replace(/:[^:@]*@/, ':***@'), // Hide password
        message: 'Database connection test failed'
      }, 'error');
      throw new Error(`Database connection failed: ${String(error)}`);
    }
  }

  /**
   * Set up Express middleware
   */
  private setupExpress(): void {
    this.app.use(express.json());

    // Basic health check (available immediately for Railway)
    this.app.get('/healthz', (_req, res) => {
      res.status(200).send('ok');
    });

    // Readiness check (only available after full initialization)
    this.app.get('/ready', (_req, res) => {
      if (this.isReady && this.boss) {
        res.status(200).json({
          status: 'ready',
          timestamp: new Date().toISOString(),
          boss: 'connected'
        });
      } else {
        res.status(503).json({
          status: 'not_ready',
          timestamp: new Date().toISOString(),
          boss: this.boss ? 'connected' : 'disconnected'
        });
      }
    });
  }

  /**
   * Set up global error handlers
   */
  private setupErrorHandlers(): void {
    process.on('uncaughtException', (err) => {
      log('uncaught_exception', { err: String(err) }, 'error');
    });

    process.on('unhandledRejection', (reason) => {
      log('unhandled_rejection', { reason: String(reason) }, 'error');
    });

    process.on('SIGINT', () => this.handleShutdown('SIGINT'));
    process.on('SIGTERM', () => this.handleShutdown('SIGTERM'));
  }

  /**
   * Start the HTTP server
   */
  private async startHttpServer(): Promise<void> {
    const port = this.getPort();

    return new Promise((resolve) => {
      this.server = this.app.listen(port, () => {
        log('http_server_started', { port });
        resolve();
      });
    });
  }

  /**
   * Initialize pg-boss with retry logic
   */
  private async initializeBossWithRetry(): Promise<void> {
    let attemptCount = 0;
    const maxAttempts = 5;

    const attempt = async (): Promise<void> => {
      attemptCount++;
      try {
        log('boss_init_attempt', {
          attempt: attemptCount,
          maxAttempts,
          databaseUrl: DATABASE_URL.replace(/:[^:@]*@/, ':***@'), // Hide password
          railway: process.env.RAILWAY_ENVIRONMENT || 'not_detected',
          ssl: USE_SSL
        });

        await this.initializeBoss();
      } catch (err) {
        const errorMessage = String(err);
        const isConnectionError = errorMessage.includes('ECONNREFUSED') ||
                                 errorMessage.includes('ENOTFOUND') ||
                                 errorMessage.includes('timeout');

        log('boss_init_error', {
          attempt: attemptCount,
          maxAttempts,
          err: errorMessage,
          errorType: isConnectionError ? 'connection' : 'other',
          willRetry: attemptCount < maxAttempts,
          railway: process.env.RAILWAY_ENVIRONMENT || 'not_detected'
        }, 'error');

        if (attemptCount >= maxAttempts) {
          const finalError = new Error(`Failed to initialize pg-boss after ${maxAttempts} attempts: ${errorMessage}`);
          log('boss_init_final_failure', {
            attempts: maxAttempts,
            lastError: errorMessage,
            databaseUrl: DATABASE_URL.replace(/:[^:@]*@/, ':***@'),
            suggestions: [
              'Check DATABASE_URL is correct',
              'Verify database is accessible',
              'Check worker role permissions',
              'Verify SSL configuration'
            ]
          }, 'error');
          throw finalError;
        }

        log('boss_init_retry', {
          retryIn: RETRY_DELAY_MS / 1000,
          nextAttempt: attemptCount + 1,
          errorType: isConnectionError ? 'connection' : 'other'
        });

        await new Promise(resolve => setTimeout(resolve, RETRY_DELAY_MS));
        await attempt();
      }
    };

    await attempt();
  }

  /**
   * Initialize pg-boss and set up all jobs
   */
  private async initializeBoss(): Promise<void> {
    if (!DATABASE_URL) {
      throw new Error('DATABASE_URL is required');
    }

    log('boss_initializing', {
      schema: BOSS_SCHEMA,
      ssl: USE_SSL,
      migrate: BOSS_MIGRATE,
      connectionPoolSize: 2
    });

    // Test database connectivity first
    await this.testDatabaseConnection();

    // Create pg-boss instance
    this.boss = new PgBoss({
      connectionString: DATABASE_URL,
      schema: BOSS_SCHEMA,
      ssl: USE_SSL
        ? ({ rejectUnauthorized: false } as TlsConnectionOptions)
        : false,
      application_name: 'zeke-worker',
      max: 2, // Keep connection pool small
      migrate: BOSS_MIGRATE,
    });

    this.boss.on('error', (err) =>
      log('boss_error', { err: String(err) }, 'error')
    );

    // Start pg-boss
    log('boss_starting', { message: 'Starting pg-boss instance' });
    await this.boss.start();
    log('boss_started', { schema: BOSS_SCHEMA });

    // Create job orchestrator
    log('boss_setup_orchestrator', { message: 'Creating job orchestrator' });
    const orchestrator = createJobOrchestrator(this.boss);

    // Set up all job infrastructure
    log('boss_setup_queues', { message: 'Creating job queues' });
    await createJobQueues(this.boss);

    log('boss_setup_recurring', { message: 'Scheduling recurring jobs' });
    await scheduleRecurringJobs(this.boss);

    log('boss_setup_workers', { message: 'Setting up job workers' });
    await setupJobWorkers(this.boss, orchestrator);

    log('boss_setup_startup', { message: 'Triggering startup jobs' });
    await triggerStartupJobs(this.boss);

    // Set up HTTP routes (now that orchestrator is ready)
    log('boss_setup_routes', { message: 'Setting up HTTP routes' });
    setupRoutes(this.app, orchestrator);

    // Store global reference for compatibility
    bossRef = this.boss;

    log('boss_ready', {
      queues: 'all queues and workers active',
      schema: BOSS_SCHEMA,
      message: 'pg-boss fully initialized'
    });
  }

  /**
   * Start metrics collection
   */
  private startMetricsCollection(): void {
    setInterval(async () => {
      try {
        const pg = (await import('../db.js')).default;
        const { upsertJobMetrics } = await import('../db.js');

        const { rows: jobStats } = await pg.query(
          'select name, state, count(*)::int as count from pgboss.job group by 1,2 order by 1,2'
        );

        await upsertJobMetrics(
          jobStats as Array<{ name: string; state: string; count: number }>
        );
      } catch {
        // Ignore metrics errors
      }
    }, METRICS_INTERVAL_MS);
  }

  /**
   * Handle shutdown signals
   */
  private async handleShutdown(signal: string): Promise<void> {
    log('shutdown_signal_received', { signal });
    try {
      await this.stop();
    } finally {
      process.exit(0);
    }
  }
}

/**
 * Create and start the worker service
 */
export async function startWorkerService(): Promise<WorkerService> {
  const service = new WorkerService();
  await service.start();
  return service;
}

// Export global boss reference for backward compatibility
export { bossRef };

================
File: src/extract/build-raw-item-article.ts
================
export function buildRawItemArticle(
  norm: {
    externalId: string;
    url: string;
    title: string | null;
    pubDate: string | null;
  },
  sourceId: string
) {
  return {
    source_id: sourceId,
    external_id: norm.externalId,
    url: norm.url,
    title: norm.title,
    kind: 'article' as const,
    metadata: {
      pubDate: norm.pubDate,
      src: norm.url,
    },
  };
}

================
File: src/extract/build-raw-item-youtube.ts
================
import type { YouTubeVideo } from '../lib/youtube/types.js';

type SourceWithMetadata = {
  id: string;
  url?: string | null;
  metadata?: { query?: string } | null;
};

export function buildRawItemYouTube(
  video: YouTubeVideo,
  src: SourceWithMetadata
) {
  const videoUrl = `https://www.youtube.com/watch?v=${video.videoId}`;
  const srcMeta = (src.metadata as { query?: string } | null) || {};
  const metadata = {
    videoId: video.videoId,
    channelId: video.channelId,
    channelTitle: video.channelTitle,
    publishedAt: video.publishedAt,
    duration: video.duration,
    viewCount: video.viewCount,
    likeCount: video.likeCount,
    commentCount: video.commentCount,
    thumbnails: video.thumbnails,
    tags: video.tags,
    categoryId: video.categoryId,
    defaultLanguage: video.defaultLanguage,
    defaultAudioLanguage: video.defaultAudioLanguage,
    src: src.url || `search:${srcMeta?.query}`,
  };

  return {
    source_id: src.id as string,
    external_id: video.videoId,
    url: videoUrl,
    title: video.title,
    kind: 'youtube' as const,
    metadata,
  };
}

================
File: src/extract/check-ytdlp-availability.ts
================
import { spawn } from 'node:child_process';
import { log } from '../log.js';

const YTDLP_CHECK_TIMEOUT_MS = 5000;

export async function checkYtDlpAvailability(): Promise<boolean> {
  try {
    const checkPromise = new Promise<boolean>((resolve, reject) => {
      const ytDlp = spawn('yt-dlp', ['--version']);
      ytDlp.on('close', (code) => resolve(code === 0));
      ytDlp.on('error', (error) => reject(error));
    });
    const timeoutPromise = new Promise<boolean>((resolve) => {
      setTimeout(() => resolve(false), YTDLP_CHECK_TIMEOUT_MS);
    });
    const isAvailable = await Promise.race([checkPromise, timeoutPromise]);
    log('youtube_ytdlp_availability_check', { available: isAvailable });
    return isAvailable;
  } catch (error) {
    log('youtube_ytdlp_availability_error', { error: String(error) }, 'error');
    return false;
  }
}

================
File: src/extract/extract-youtube-audio.ts
================
import { spawn } from 'node:child_process';
import { promises as fs } from 'node:fs';
import { join } from 'node:path';
import { log } from '../log.js';

const MS_PER_SECOND = 1000;
const SECONDS_PER_MINUTE = 60;
const MINUTES_TO_MS = SECONDS_PER_MINUTE * MS_PER_SECOND;
const AUDIO_EXTRACTION_TIMEOUT_MINUTES = 10;
const AUDIO_EXTRACTION_TIMEOUT_MS =
  AUDIO_EXTRACTION_TIMEOUT_MINUTES * MINUTES_TO_MS;

const KILOBYTE = 1024;
const BYTES_TO_MB = KILOBYTE * KILOBYTE;
const DECIMAL_PLACES = 100;

export type VideoMetadata = {
  videoId: string;
  title: string;
  duration: number; // seconds
  viewCount?: number;
  likeCount?: number;
  uploadDate: string;
  uploader: string;
  description: string;
  thumbnailUrl?: string;
  format: string;
  filesize?: number;
};

export type AudioExtractionResult = {
  audioPath: string;
  metadata: VideoMetadata;
  success: boolean;
  error?: string;
};

export async function extractAudio(
  videoUrl: string,
  videoId: string
): Promise<AudioExtractionResult> {
  const tempDir = '/tmp/youtube-processing';
  const audioPath = join(tempDir, `${videoId}.m4a`);

  try {
    await fs.mkdir(tempDir, { recursive: true });

    log('youtube_audio_extraction_start', { videoId, videoUrl, audioPath });

    const ytDlpArgs = [
      videoUrl,
      '--extract-audio',
      '--audio-format',
      'm4a',
      '--audio-quality',
      '0',
      '--output',
      audioPath.replace('.m4a', '.%(ext)s'),
      '--no-playlist',
      '--max-filesize',
      '500M',
      '--socket-timeout',
      '30',
      '--retries',
      '3',
      '--fragment-retries',
      '3',
    ];

    const audioExtractionPromise = new Promise<void>((resolve, reject) => {
      const ytDlp = spawn('yt-dlp', ytDlpArgs);
      let stderr = '';
      ytDlp.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      ytDlp.on('close', (code) =>
        code === 0
          ? resolve()
          : reject(new Error(`yt-dlp failed with code ${code}: ${stderr}`))
      );
      ytDlp.on('error', (error) =>
        reject(new Error(`Failed to spawn yt-dlp: ${error.message}`))
      );
    });

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(
        () => reject(new Error('Audio extraction timeout')),
        AUDIO_EXTRACTION_TIMEOUT_MS
      );
    });

    await Promise.race([audioExtractionPromise, timeoutPromise]);
    await fs.access(audioPath);

    const stats = await fs.stat(audioPath);
    const fileSizeBytes = stats.size;
    const fileSizeMb =
      Math.round((fileSizeBytes / BYTES_TO_MB) * DECIMAL_PLACES) /
      DECIMAL_PLACES;

    log('youtube_audio_extraction_complete', {
      videoId,
      audioPath,
      fileSizeMb,
    });

    return {
      audioPath,
      metadata: {
        videoId,
        title: '',
        duration: 0,
        uploadDate: new Date().toISOString().split('T')[0].replace(/-/g, ''),
        uploader: '',
        description: '',
        thumbnailUrl: undefined,
        viewCount: undefined,
        likeCount: undefined,
        format: 'm4a',
        filesize: fileSizeBytes,
      },
      success: true,
    };
  } catch (error) {
    log(
      'youtube_audio_extraction_error',
      { videoId, videoUrl, error: String(error) },
      'error'
    );
    try {
      await fs.unlink(audioPath);
    } catch (deleteError) {
      log(
        'youtube_audio_deletion_error',
        { videoId, audioPath, error: String(deleteError) },
        'error'
      );
    }
    return {
      audioPath: '',
      metadata: {
        videoId,
        title: '',
        duration: 0,
        uploadDate: '',
        uploader: '',
        description: '',
        format: 'm4a',
      },
      success: false,
      error: String(error),
    };
  }
}

================
File: src/extract/get-youtube-metadata.ts
================
import { spawn } from 'node:child_process';
import { log } from '../log.js';

export type VideoMetadata = {
  videoId: string;
  title: string;
  duration: number;
  viewCount?: number;
  likeCount?: number;
  uploadDate: string;
  uploader: string;
  description: string;
  thumbnailUrl?: string;
  format: string;
  filesize?: number;
};

const SECONDS_TO_MS = 1000;
const METADATA_EXTRACTION_TIMEOUT_SECONDS = 30;
const METADATA_EXTRACTION_TIMEOUT_MS =
  METADATA_EXTRACTION_TIMEOUT_SECONDS * SECONDS_TO_MS;

export async function getVideoMetadata(
  videoId: string
): Promise<VideoMetadata> {
  const videoUrl = `https://www.youtube.com/watch?v=${videoId}`;
  try {
    log('youtube_metadata_extraction_start', { videoId });

    const ytDlpArgs = [
      videoUrl,
      '--dump-json',
      '--no-playlist',
      '--socket-timeout',
      '30',
    ];

    const metadataPromise = new Promise<string>((resolve, reject) => {
      const ytDlp = spawn('yt-dlp', ytDlpArgs);
      let stdout = '';
      let stderr = '';
      ytDlp.stdout.on('data', (d) => {
        stdout += d.toString();
      });
      ytDlp.stderr.on('data', (d) => {
        stderr += d.toString();
      });
      ytDlp.on('close', (code) =>
        code === 0
          ? resolve(stdout)
          : reject(
              new Error(`yt-dlp metadata failed with code ${code}: ${stderr}`)
            )
      );
      ytDlp.on('error', (error) =>
        reject(
          new Error(`Failed to spawn yt-dlp for metadata: ${error.message}`)
        )
      );
    });

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(
        () => reject(new Error('Metadata extraction timeout')),
        METADATA_EXTRACTION_TIMEOUT_MS
      );
    });

    const jsonOutput = await Promise.race([metadataPromise, timeoutPromise]);
    const metadata = JSON.parse(jsonOutput.trim());

    const result: VideoMetadata = {
      videoId,
      title: metadata.title || '',
      duration: metadata.duration || 0,
      viewCount: metadata.view_count,
      likeCount: metadata.like_count,
      uploadDate: metadata.upload_date || '',
      uploader: metadata.uploader || metadata.channel || '',
      description: metadata.description || '',
      thumbnailUrl: metadata.thumbnail,
      format: metadata.ext || 'm4a',
      filesize: metadata.filesize,
    };

    log('youtube_metadata_extraction_complete', {
      videoId,
      title: result.title,
      duration: result.duration,
      viewCount: result.viewCount,
      uploader: result.uploader,
    });
    return result;
  } catch (error) {
    log(
      'youtube_metadata_extraction_error',
      { videoId, error: String(error) },
      'error'
    );
    return {
      videoId,
      title: `Video ${videoId}`,
      duration: 0,
      uploadDate: new Date().toISOString().split('T')[0].replace(/-/g, ''),
      uploader: 'Unknown',
      description: '',
      format: 'm4a',
    };
  }
}

================
File: src/extract/get-youtube-supported-formats.ts
================
import { spawn } from 'node:child_process';
import { log } from '../log.js';

const WHITESPACE_REGEX = /\s+/;

export async function getSupportedFormats(videoUrl: string): Promise<string[]> {
  try {
    const ytDlpArgs = [videoUrl, '--list-formats', '--no-playlist'];
    const formatsPromise = new Promise<string>((resolve, reject) => {
      const ytDlp = spawn('yt-dlp', ytDlpArgs);
      let stdout = '';
      ytDlp.stdout.on('data', (data) => {
        stdout += data.toString();
      });
      ytDlp.on('close', (code) =>
        code === 0
          ? resolve(stdout)
          : reject(new Error(`yt-dlp list-formats failed with code ${code}`))
      );
    });

    const output = await formatsPromise;
    const formats = output
      .split('\n')
      .filter((line) => line.includes('audio only'))
      .map((line) => line.split(WHITESPACE_REGEX)[0])
      .filter((format) => format && !format.includes('format'));
    return formats;
  } catch (error) {
    log(
      'youtube_formats_check_error',
      { videoUrl, error: String(error) },
      'error'
    );
    return [];
  }
}

================
File: src/extract/normalize-rss-item.ts
================
import { canonicalizeUrl } from '../util.js';

export type ParsedText =
  | string
  | { '#text'?: string; href?: string }
  | null
  | undefined;

export type RssItem = {
  guid?: string | { '#text'?: string };
  id?: string;
  link?: string | { href?: string };
  title?: string;
  pubDate?: string;
  updated?: string;
};

export type NormalizedRssItem = {
  externalId: string;
  url: string;
  title: string | null;
  pubDate: string | null;
};

export function normalizeRssItem(
  item: RssItem,
  _sourceUrl: string
): NormalizedRssItem | null {
  const guid = getText(item.guid) || item.id || getText(item.link) || '';
  const link = canonicalizeUrl(getText(item.link) || '');
  if (guid === '' && link === '') {
    return null;
  }

  const externalId = guid;
  const url = link || externalId;
  const title = item.title
    ? (getText(item.title as ParsedText) ?? String(item.title))
    : null;
  const pubDate = item.pubDate || item.updated || null;

  return { externalId, url, title, pubDate };
}

function getText(v: ParsedText): string | undefined {
  if (v == null) {
    return;
  }
  if (typeof v === 'string') {
    return v;
  }
  if (typeof v === 'object' && '#text' in v) {
    return v['#text'] as string;
  }
  if (typeof v === 'object' && 'href' in v) {
    return v.href as string;
  }
  return;
}

================
File: src/extract/parse-rss-feed.ts
================
import { XMLParser } from 'fast-xml-parser';

export type RssItem = {
  guid?: string | { '#text'?: string };
  id?: string; // atom
  link?: string | { href?: string };
  title?: string;
  pubDate?: string;
  updated?: string; // atom
};

export function parseRssFeed(xml: string): RssItem[] {
  const parser = new XMLParser({
    ignoreAttributes: false,
    attributeNamePrefix: '',
  });
  const doc = parser.parse(xml);
  const items: RssItem[] = asArray(doc?.rss?.channel?.item).concat(
    asArray(doc?.feed?.entry)
  );
  return items;
}

function asArray<T>(x: T | T[] | undefined): T[] {
  if (!x) {
    return [];
  }
  return Array.isArray(x) ? x : [x];
}

================
File: src/http/__tests__/routes.test.js
================
/**
 * Unit tests for HTTP Routes
 *
 * Tests the HTTP endpoint system to ensure all routes work correctly
 * and properly delegate to the job orchestrator.
 */

import assert from 'node:assert/strict';
import { describe, mock, test } from 'node:test';
import express from 'express';
import { setupRoutes } from '../routes.js';

describe('HTTP Routes', () => {
  test('should set up health routes', async () => {
    const { app, orchestrator } = createTestApp();

    // Test basic health check
    const healthResponse = await makeRequest(app, 'GET', '/healthz');
    assert.strictEqual(healthResponse.status, 200);
    assert.strictEqual(healthResponse.text, 'ok');
  });

  test('should set up debug status route', async () => {
    const { app, orchestrator } = createTestApp();

    // Mock the database import
    mockDatabaseImport();

    const statusResponse = await makeRequest(app, 'GET', '/debug/status');
    assert.strictEqual(statusResponse.status, 200);

    const statusData = JSON.parse(statusResponse.text);
    assert.strictEqual(statusData.ok, true);
    assert.ok('sources' in statusData);
    assert.ok('raw' in statusData);
    assert.ok('contents' in statusData);
    assert.ok('jobs' in statusData);
  });

  test('POST /debug/ingest-now should trigger RSS ingest', async () => {
    const { app, orchestrator } = createTestApp();

    const response = await makeRequest(app, 'POST', '/debug/ingest-now');
    assert.strictEqual(response.status, 200);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, true);

    // Verify orchestrator was called
    assert.strictEqual(orchestrator.triggerRssIngest.mock.callCount(), 1);
  });

  test('POST /debug/ingest-youtube should trigger YouTube ingest', async () => {
    const { app, orchestrator } = createTestApp();

    const response = await makeRequest(app, 'POST', '/debug/ingest-youtube');
    assert.strictEqual(response.status, 200);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, true);

    // Verify orchestrator was called
    assert.strictEqual(orchestrator.triggerYouTubeIngest.mock.callCount(), 1);
  });

  test('POST /debug/ingest-source should trigger specific source ingest', async () => {
    const { app, orchestrator } = createTestApp();

    // Mock database query for source type
    mockDatabaseImport({
      queryResult: [{ kind: 'rss' }],
    });

    const response = await makeRequest(
      app,
      'POST',
      '/debug/ingest-source?sourceId=test-source'
    );
    assert.strictEqual(response.status, 200);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, true);

    // Verify orchestrator was called with correct source ID
    assert.strictEqual(orchestrator.triggerRssSourceIngest.mock.callCount(), 1);
    assert.deepStrictEqual(
      orchestrator.triggerRssSourceIngest.mock.calls[0].arguments,
      ['test-source']
    );
  });

  test('POST /debug/ingest-source should handle YouTube sources', async () => {
    const { app, orchestrator } = createTestApp();

    // Mock database query for YouTube source
    mockDatabaseImport({
      queryResult: [{ kind: 'youtube_channel' }],
    });

    const response = await makeRequest(
      app,
      'POST',
      '/debug/ingest-source?sourceId=yt-source'
    );
    assert.strictEqual(response.status, 200);

    // Verify YouTube orchestrator was called
    assert.strictEqual(
      orchestrator.triggerYouTubeSourceIngest.mock.callCount(),
      1
    );
    assert.deepStrictEqual(
      orchestrator.triggerYouTubeSourceIngest.mock.calls[0].arguments,
      ['yt-source']
    );
  });

  test('POST /debug/ingest-source should return 404 for missing source', async () => {
    const { app, orchestrator } = createTestApp();

    // Mock database query with no results
    mockDatabaseImport({
      queryResult: [],
    });

    const response = await makeRequest(
      app,
      'POST',
      '/debug/ingest-source?sourceId=missing'
    );
    assert.strictEqual(response.status, 404);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, false);
    assert.strictEqual(data.error, 'not_found');
  });

  test('POST /debug/ingest-source should return 400 for missing sourceId', async () => {
    const { app, orchestrator } = createTestApp();

    const response = await makeRequest(app, 'POST', '/debug/ingest-source');
    assert.strictEqual(response.status, 400);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, false);
    assert.strictEqual(data.error, 'missing_sourceId');
  });

  test('POST /debug/ingest-oneoff should process URLs', async () => {
    const { app, orchestrator } = createTestApp();

    const urls = [
      'https://example.com/article',
      'https://youtube.com/watch?v=abc',
    ];
    const mockResults = [
      { url: urls[0], ok: true, raw_item_id: 'item1', type: 'article' },
      { url: urls[1], ok: true, raw_item_id: 'item2', type: 'youtube' },
    ];

    orchestrator.triggerOneOffIngest.mock.mockImplementation(
      async () => mockResults
    );

    const response = await makeRequest(app, 'POST', '/debug/ingest-oneoff', {
      urls,
    });
    assert.strictEqual(response.status, 200);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, true);
    assert.deepStrictEqual(data.results, mockResults);

    // Verify orchestrator was called with URLs
    assert.strictEqual(orchestrator.triggerOneOffIngest.mock.callCount(), 1);
    assert.deepStrictEqual(
      orchestrator.triggerOneOffIngest.mock.calls[0].arguments,
      [urls]
    );
  });

  test('POST /debug/ingest-oneoff should return 400 for no URLs', async () => {
    const { app, orchestrator } = createTestApp();

    const response = await makeRequest(app, 'POST', '/debug/ingest-oneoff', {
      urls: [],
    });
    assert.strictEqual(response.status, 400);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, false);
    assert.strictEqual(data.error, 'no_urls');
  });

  test('should handle orchestrator errors gracefully', async () => {
    const { app, orchestrator } = createTestApp();

    // Make orchestrator throw an error
    orchestrator.triggerRssIngest.mock.mockImplementation(async () => {
      throw new Error('Mock orchestrator error');
    });

    const response = await makeRequest(app, 'POST', '/debug/ingest-now');
    assert.strictEqual(response.status, 503);

    const data = JSON.parse(response.text);
    assert.strictEqual(data.ok, false);
    assert.ok(data.error.includes('Mock orchestrator error'));
  });

  test('legacy scheduling endpoints should return helpful messages', async () => {
    const { app, orchestrator } = createTestApp();

    const rssResponse = await makeRequest(app, 'POST', '/debug/schedule-rss');
    assert.strictEqual(rssResponse.status, 200);

    const rssData = JSON.parse(rssResponse.text);
    assert.strictEqual(rssData.ok, true);
    assert.ok(rssData.message.includes('automatic'));

    const youtubeResponse = await makeRequest(
      app,
      'POST',
      '/debug/schedule-youtube'
    );
    assert.strictEqual(youtubeResponse.status, 200);

    const youtubeData = JSON.parse(youtubeResponse.text);
    assert.strictEqual(youtubeData.ok, true);
    assert.ok(youtubeData.message.includes('automatic'));
  });
});

/**
 * Create a test Express app with mocked orchestrator
 */
function createTestApp() {
  const app = express();
  app.use(express.json());

  const orchestrator = {
    triggerRssIngest: mock.fn(async () => {}),
    triggerYouTubeIngest: mock.fn(async () => {}),
    triggerRssSourceIngest: mock.fn(async () => {}),
    triggerYouTubeSourceIngest: mock.fn(async () => {}),
    triggerOneOffIngest: mock.fn(async () => []),
  };

  setupRoutes(app, orchestrator);

  return { app, orchestrator };
}

/**
 * Make a request to the Express app
 */
async function makeRequest(app, method, path, body = null) {
  return new Promise((resolve) => {
    const req = {
      method,
      url: path,
      headers: { 'content-type': 'application/json' },
      body: body ? JSON.stringify(body) : undefined,
      query: {},
      params: {},
    };

    // Parse query string
    if (path.includes('?')) {
      const [pathname, queryString] = path.split('?');
      req.url = pathname;
      const params = new URLSearchParams(queryString);
      for (const [key, value] of params) {
        req.query[key] = value;
      }
    }

    // Parse body if present
    if (req.body) {
      try {
        req.body = JSON.parse(req.body);
      } catch {
        // Keep as string if not JSON
      }
    }

    const res = {
      text: '',
      headers: {},

      status(code) {
        this.status = code;
        return this;
      },

      send(data) {
        this.text = data;
        resolve(this);
        return this;
      },

      json(data) {
        this.text = JSON.stringify(data);
        resolve(this);
        return this;
      },
    };

    // Find matching route and call handler
    const routes = app._router?.stack || [];
    for (const layer of routes) {
      if (
        layer.route &&
        layer.route.path === req.url &&
        layer.route.methods[method.toLowerCase()]
      ) {
        try {
          layer.route.stack[0].handle(req, res);
        } catch (error) {
          res.status(500).json({ ok: false, error: error.message });
        }
        return;
      }
    }

    // No route found
    res.status(404).send('Not Found');
  });
}

/**
 * Mock the database import for testing
 */
function mockDatabaseImport(options = {}) {
  const { queryResult = [{ sources_rss: 5 }] } = options;

  const originalImport = global.import;
  global.import = mock.fn(async (path) => {
    if (path === '../db.js') {
      return {
        default: {
          query: mock.fn(async () => ({ rows: queryResult })),
        },
      };
    }
    return originalImport(path);
  });
}

================
File: src/http/routes.ts
================
/**
 * HTTP Routes - All worker HTTP endpoints in one place
 *
 * This module handles all HTTP endpoints for the worker service.
 * All endpoints delegate to the job orchestrator for consistency.
 *
 * Route Categories:
 * - Health: service health checks
 * - Debug: development and admin tools
 * - Preview: source preview functionality
 */

import type express from 'express';
import type { JobOrchestrator } from '../core/job-orchestrator.js';
import { log } from '../log.js';

// HTTP status codes
const HTTP_OK = 200;
const HTTP_BAD_REQUEST = 400;
const HTTP_NOT_FOUND = 404;
const HTTP_SERVICE_UNAVAILABLE = 503;

// Configuration
const DEFAULT_PREVIEW_LIMIT = 10;
const MAX_PREVIEW_LIMIT = 50;

/**
 * Sets up all HTTP routes for the worker service
 */
export function setupRoutes(
  app: express.Express,
  orchestrator: JobOrchestrator
): void {
  // Health check
  setupHealthRoutes(app);

  // Debug routes for development and admin
  setupDebugRoutes(app, orchestrator);

  // Preview routes for source testing
  setupPreviewRoutes(app);

  log('http_routes_setup', { routes: 'health, debug, preview' });
}

/**
 * Health check routes
 */
function setupHealthRoutes(app: express.Express): void {
  app.get('/healthz', (_req, res) => {
    res.status(HTTP_OK).send('ok');
  });

  app.get('/debug/status', async (_req, res) => {
    try {
      const pg = (await import('../db.js')).default;
      const [
        { rows: sources },
        { rows: rawCounts },
        { rows: contentCounts },
        { rows: jobStats },
      ] = await Promise.all([
        pg.query(
          "select count(*)::int as sources_rss from public.sources where kind = 'rss' and url is not null"
        ),
        pg.query(
          "select count(*)::int as raw_total, count(*) filter (where discovered_at > now() - interval '24 hours')::int as raw_24h from public.raw_items"
        ),
        pg.query('select count(*)::int as contents_total from public.contents'),
        pg.query(
          'select name, state, count(*)::int as count from pgboss.job group by 1,2 order by 1,2'
        ),
      ]);

      res.json({
        ok: true,
        sources: sources[0],
        raw: rawCounts[0],
        contents: contentCounts[0],
        jobs: jobStats,
      });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

/**
 * Debug routes for manual job triggering
 */
function setupDebugRoutes(
  app: express.Express,
  orchestrator: JobOrchestrator
): void {
  // Trigger RSS ingest
  app.post('/debug/ingest-now', async (_req, res) => {
    try {
      await orchestrator.triggerRssIngest();
      res.json({ ok: true });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });

  // Trigger YouTube ingest
  app.post('/debug/ingest-youtube', async (_req, res) => {
    try {
      await orchestrator.triggerYouTubeIngest();
      res.json({ ok: true });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });

  // Trigger specific source ingest
  app.post('/debug/ingest-source', async (req, res) => {
    try {
      const sourceId =
        (req.query.sourceId as string) || (req.body?.sourceId as string) || '';
      if (!sourceId) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'missing_sourceId' });
      }

      // Determine source type and trigger appropriate ingest
      const pg = (await import('../db.js')).default;
      const { rows } = await pg.query(
        'select kind from public.sources where id = $1',
        [sourceId]
      );
      const source = rows[0];

      if (!source) {
        return res
          .status(HTTP_NOT_FOUND)
          .json({ ok: false, error: 'not_found' });
      }

      if (source.kind === 'rss' || source.kind === 'podcast') {
        await orchestrator.triggerRssSourceIngest(sourceId);
      } else if (
        source.kind === 'youtube_channel' ||
        source.kind === 'youtube_search'
      ) {
        await orchestrator.triggerYouTubeSourceIngest(sourceId);
      } else {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'unsupported_kind' });
      }

      res.json({ ok: true });
    } catch (e: unknown) {
      const msg = String(e);
      if (msg.includes('not_found')) {
        return res
          .status(HTTP_NOT_FOUND)
          .json({ ok: false, error: 'not_found' });
      }
      if (msg.includes('unsupported_kind')) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'unsupported_kind' });
      }
      res.status(HTTP_SERVICE_UNAVAILABLE).json({ ok: false, error: msg });
    }
  });

  // One-off URL ingestion
  app.post('/debug/ingest-oneoff', async (req, res) => {
    try {
      const body = (req.body || {}) as { urls?: string[] };
      const urls = Array.isArray(body.urls) ? body.urls.filter(Boolean) : [];

      if (urls.length === 0) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'no_urls' });
      }

      const results = await orchestrator.triggerOneOffIngest(urls);
      res.json({ ok: true, results });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });

  // Legacy scheduling endpoints (kept for compatibility)
  app.post('/debug/schedule-rss', async (_req, res) => {
    res.json({
      ok: true,
      message:
        'RSS scheduling is automatic. Use /debug/ingest-now for manual trigger.',
    });
  });

  app.post('/debug/schedule-youtube', async (_req, res) => {
    res.json({
      ok: true,
      message:
        'YouTube scheduling is automatic. Use /debug/ingest-youtube for manual trigger.',
    });
  });
}

/**
 * Preview routes for testing sources
 */
function setupPreviewRoutes(app: express.Express): void {
  app.get('/debug/preview-source', async (req, res) => {
    try {
      const sourceId = (req.query.sourceId as string) || '';
      const rawLimit = Number.parseInt(
        String(req.query.limit ?? DEFAULT_PREVIEW_LIMIT),
        10
      );
      const limit = Math.min(
        Number.isNaN(rawLimit) ? DEFAULT_PREVIEW_LIMIT : rawLimit,
        MAX_PREVIEW_LIMIT
      );

      if (!sourceId) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'missing sourceId' });
      }

      const pg = (await import('../db.js')).default;
      const { rows } = await pg.query(
        'select id, kind, url, name, domain, metadata from public.sources where id = $1',
        [sourceId]
      );
      const src = rows[0];

      if (!src) {
        return res
          .status(HTTP_NOT_FOUND)
          .json({ ok: false, error: 'not_found' });
      }

      let result: unknown = null;
      try {
        result = await computePreviewResultForSource(src, limit);
      } catch (err) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: String(err) });
      }

      return res.json({ ok: true, ...(result as object) });
    } catch (e: unknown) {
      return res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

/**
 * Helper function to compute preview results for different source types
 */
async function computePreviewResultForSource(
  src: { id: string; url?: string; kind: string },
  limit: number
): Promise<unknown> {
  if (src.kind === 'rss' || src.kind === 'podcast') {
    const { previewRssSourceAction } = await import(
      '../tasks/preview-rss-source.js'
    );
    return previewRssSourceAction({ id: src.id, url: src.url ?? '' }, limit);
  }

  if (src.kind === 'youtube_channel' || src.kind === 'youtube_search') {
    const { previewYouTubeSourceAction } = await import(
      '../tasks/preview-youtube-source.js'
    );
    return previewYouTubeSourceAction(src, limit);
  }

  throw new Error('unsupported_kind');
}

================
File: src/lib/openai/clean-json-response.ts
================
const JSON_CODE_BLOCK_START = /^```json\s*/i;
const JSON_CODE_BLOCK_END = /\s*```$/i;

export function cleanAndParseJSON(text: string): unknown {
  const cleaned = text
    .replace(JSON_CODE_BLOCK_START, '')
    .replace(JSON_CODE_BLOCK_END, '')
    .trim();
  return JSON.parse(cleaned);
}

================
File: src/lib/openai/constants.ts
================
export const DEFAULT_CHAT_MODEL =
  process.env.OPENAI_CHAT_MODEL || 'gpt-4o-mini';
export const DEFAULT_EMBEDDING_MODEL =
  process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small';

export const MODEL_VERSION_LABELS = {
  chat: `${DEFAULT_CHAT_MODEL}-v1`,
  embedding: `${DEFAULT_EMBEDDING_MODEL}-v1`,
};

export const MAX_CONTENT_LENGTH_ANALYSIS = 8000;
export const MAX_CONTENT_LENGTH_EMBEDDING = 6000;

export const EMBEDDING_DIMENSIONS = 1536;

export const CHILI_MIN = 0;
export const CHILI_MAX = 5;
export const CHILI_DEFAULT = 1;
export const CHILI_KEYWORD_BOOST = 1;

export const CONFIDENCE_MIN = 0;
export const CONFIDENCE_MAX = 1;
export const CONFIDENCE_DEFAULT = 0.5;

export const CONFIDENCE_RELIABLE_SOURCE_BOOST = 0.3;
export const CONFIDENCE_LONG_TEXT_BOOST = 0.1;
export const CONFIDENCE_TITLE_BOOST = 0.1;

export const TEXT_LENGTH_THRESHOLD = 2000;
export const TITLE_MIN_LENGTH = 10;
export const TEXT_MIN_LENGTH = 1000;

export const EMBEDDING_NORMALIZATION_FACTOR = 0.1;
export const CHAR_NORMALIZATION = 255.0;

================
File: src/lib/openai/generate-analysis.ts
================
import { log } from '../../log.js';
import { withRetry } from '../../utils/retry.js';
import { cleanAndParseJSON } from './clean-json-response.js';
import {
  CHILI_DEFAULT,
  CHILI_MAX,
  CHILI_MIN,
  CONFIDENCE_DEFAULT,
  CONFIDENCE_MAX,
  CONFIDENCE_MIN,
} from './constants.js';
import type { OpenAIClient } from './openai-client.js';
import type { AnalysisInput, AnalysisResult } from './types.js';

export async function generateAnalysis(
  client: OpenAIClient,
  story: AnalysisInput
): Promise<AnalysisResult> {
  const domain = story.canonical_url
    ? new URL(story.canonical_url).hostname
    : 'unknown';
  const truncatedText =
    story.text.length > client.maxAnalysisLen
      ? `${story.text.substring(0, client.maxAnalysisLen)}...[truncated]`
      : story.text;

  const prompt = `Analyze this news article and provide insights in JSON format.

Title: ${story.title || 'No title'}
Source: ${domain}
Content: ${truncatedText}

Please respond with a JSON object containing:
1. "why_it_matters": A 2-3 bullet point explanation of why this story is significant (use ‚Ä¢ bullet points)
2. "chili": A score from 0-5 indicating how "hot" or important this story is (0=boring, 5=major breakthrough)
3. "confidence": A score from 0-1 indicating your confidence in the analysis (0=low confidence, 1=high confidence)

Consider factors like:
- Technological significance and innovation
- Potential impact on industry or society
- Source reliability (${domain})
- Timeliness and relevance
- Depth and quality of reporting

Respond only with valid JSON, no other text.`;

  try {
    const completion = await withRetry(
      () =>
        client.openai.chat.completions.create({
          model: client.chatModel,
          messages: [{ role: 'user', content: prompt }],
          temperature: 0.3,
          max_tokens: 500,
        }),
      { maxRetries: 3 }
    );

    const responseText = completion.choices[0]?.message?.content?.trim();
    if (!responseText) {
      throw new Error('Empty response from OpenAI');
    }

    type LLMAnalysisJSON = {
      why_it_matters?: unknown;
      chili?: unknown;
      confidence?: unknown;
      citations?: unknown;
    };

    const raw = cleanAndParseJSON(responseText) as LLMAnalysisJSON;

    const toNumber = (v: unknown): number | null => {
      if (typeof v === 'number') {
        return v;
      }
      if (typeof v === 'string') {
        return Number(v);
      }
      return null;
    };
    const clamp = (n: number, min: number, max: number) =>
      Math.max(min, Math.min(max, n));

    const why =
      typeof raw.why_it_matters === 'string' && raw.why_it_matters.trim()
        ? raw.why_it_matters
        : '‚Ä¢ Analysis not available';

    const chiliVal = clamp(
      Math.round(toNumber(raw.chili) ?? CHILI_DEFAULT),
      CHILI_MIN,
      CHILI_MAX
    );

    const confidenceVal = clamp(
      toNumber(raw.confidence) ?? CONFIDENCE_DEFAULT,
      CONFIDENCE_MIN,
      CONFIDENCE_MAX
    );

    const citationsVal =
      typeof raw.citations === 'object' &&
      raw.citations !== null &&
      !Array.isArray(raw.citations)
        ? (raw.citations as Record<string, unknown>)
        : {};

    return {
      why_it_matters: String(why),
      chili: chiliVal,
      confidence: confidenceVal,
      citations: citationsVal,
    };
  } catch (error) {
    log(
      'openai_analysis_error',
      { comp: 'analyze', error: String(error), story_title: story.title },
      'error'
    );
    throw error;
  }
}

================
File: src/lib/openai/generate-embedding.ts
================
import { log } from '../../log.js';
import { withRetry } from '../../utils/retry.js';
import type { OpenAIClient } from './openai-client.js';
import type { EmbeddingInput, EmbeddingResult } from './types.js';

export async function generateEmbedding(
  client: OpenAIClient,
  story: EmbeddingInput
): Promise<EmbeddingResult> {
  const content = `${story.title || ''}\n\n${story.text}`;
  const truncatedContent =
    content.length > client.maxEmbeddingLen
      ? `${content.substring(0, client.maxEmbeddingLen)}...[truncated]`
      : content;

  try {
    const response = await withRetry(
      () =>
        client.openai.embeddings.create({
          model: client.embeddingModel,
          input: truncatedContent,
          dimensions: client.embeddingDimensions,
        }),
      { maxRetries: 3 }
    );

    const embedding = response.data[0]?.embedding;
    if (!embedding || embedding.length !== client.embeddingDimensions) {
      throw new Error(
        `Invalid embedding response: expected ${client.embeddingDimensions} dimensions, got ${embedding?.length || 0}`
      );
    }
    return { embedding };
  } catch (error) {
    log(
      'openai_embedding_error',
      { comp: 'analyze', error: String(error), story_title: story.title },
      'error'
    );
    throw error;
  }
}

================
File: src/lib/openai/generate-stub-analysis.ts
================
import {
  CHILI_DEFAULT,
  CHILI_KEYWORD_BOOST,
  CHILI_MAX,
  CHILI_MIN,
  CONFIDENCE_DEFAULT,
  CONFIDENCE_LONG_TEXT_BOOST,
  CONFIDENCE_MAX,
  CONFIDENCE_RELIABLE_SOURCE_BOOST,
  CONFIDENCE_TITLE_BOOST,
  TEXT_LENGTH_THRESHOLD,
  TEXT_MIN_LENGTH,
  TITLE_MIN_LENGTH,
} from './constants.js';
import type { AnalysisInput, AnalysisResult } from './types.js';

const WORD_SPLIT_PATTERN = /\s+/;

export function generateStubAnalysis(
  story: AnalysisInput
): Promise<AnalysisResult> {
  const textLength = story.text.length;
  const titleWords = (story.title || '')
    .toLowerCase()
    .split(WORD_SPLIT_PATTERN);

  const hasImportantKeywords = titleWords.some((word) =>
    [
      'ai',
      'artificial',
      'intelligence',
      'breakthrough',
      'security',
      'privacy',
      'data',
    ].includes(word)
  );

  const domain = story.canonical_url
    ? new URL(story.canonical_url).hostname
    : 'unknown';
  const isReliableSource = ['arstechnica.com', 'news.ycombinator.com'].includes(
    domain
  );

  let why_it_matters = '';
  if (hasImportantKeywords) {
    why_it_matters =
      '‚Ä¢ This story covers emerging technology trends that could impact how we work and live\n' +
      '‚Ä¢ The developments discussed may influence industry standards and practices\n' +
      '‚Ä¢ Understanding these changes helps stay informed about technological progress';
  } else {
    why_it_matters =
      '‚Ä¢ This story provides insights into current industry developments\n' +
      '‚Ä¢ The information may be relevant for understanding market trends\n' +
      '‚Ä¢ Staying informed about these topics helps with professional awareness';
  }

  let chili = CHILI_DEFAULT;
  if (hasImportantKeywords) {
    chili += CHILI_KEYWORD_BOOST;
  }
  if (textLength > TEXT_LENGTH_THRESHOLD) {
    chili += 1;
  }
  if (isReliableSource) {
    chili += 1;
  }
  chili = Math.min(Math.max(chili, CHILI_MIN), CHILI_MAX);

  let confidence = CONFIDENCE_DEFAULT;
  if (isReliableSource) {
    confidence += CONFIDENCE_RELIABLE_SOURCE_BOOST;
  }
  if (textLength > TEXT_MIN_LENGTH) {
    confidence += CONFIDENCE_LONG_TEXT_BOOST;
  }
  if (story.title && story.title.length > TITLE_MIN_LENGTH) {
    confidence += CONFIDENCE_TITLE_BOOST;
  }
  confidence = Math.min(confidence, CONFIDENCE_MAX);

  return Promise.resolve({ why_it_matters, chili, confidence, citations: {} });
}

================
File: src/lib/openai/generate-stub-embedding.ts
================
import {
  CHAR_NORMALIZATION,
  CONFIDENCE_DEFAULT,
  EMBEDDING_DIMENSIONS,
  EMBEDDING_NORMALIZATION_FACTOR,
} from './constants.js';
import type { EmbeddingInput, EmbeddingResult } from './types.js';

export function generateStubEmbedding(
  story: EmbeddingInput
): Promise<EmbeddingResult> {
  const content = `${story.title || ''} ${story.text}`.toLowerCase();
  const embedding = new Array(EMBEDDING_DIMENSIONS).fill(0);

  for (let i = 0; i < content.length && i < EMBEDDING_DIMENSIONS; i++) {
    const charCode = content.charCodeAt(i);
    embedding[i % EMBEDDING_DIMENSIONS] +=
      (charCode / CHAR_NORMALIZATION - CONFIDENCE_DEFAULT) *
      EMBEDDING_NORMALIZATION_FACTOR;
  }

  const magnitude = Math.sqrt(
    embedding.reduce((sum, val) => sum + val * val, 0)
  );
  if (magnitude > 0) {
    for (let i = 0; i < embedding.length; i++) {
      embedding[i] /= magnitude;
    }
  }

  return Promise.resolve({ embedding });
}

================
File: src/lib/openai/openai-client.ts
================
import OpenAI from 'openai';
import {
  DEFAULT_CHAT_MODEL,
  DEFAULT_EMBEDDING_MODEL,
  EMBEDDING_DIMENSIONS,
  MAX_CONTENT_LENGTH_ANALYSIS,
  MAX_CONTENT_LENGTH_EMBEDDING,
} from './constants.js';

export type OpenAIClient = {
  openai: OpenAI;
  chatModel: string;
  embeddingModel: string;
  embeddingDimensions: number;
  maxAnalysisLen: number;
  maxEmbeddingLen: number;
};

export function createOpenAIClient(config?: {
  apiKey?: string;
  chatModel?: string;
  embeddingModel?: string;
  embeddingDimensions?: number;
  maxAnalysisLen?: number;
  maxEmbeddingLen?: number;
}): OpenAIClient {
  const apiKey = config?.apiKey ?? process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY environment variable is required');
  }

  const openai = new OpenAI({ apiKey });
  return {
    openai,
    chatModel: config?.chatModel ?? DEFAULT_CHAT_MODEL,
    embeddingModel: config?.embeddingModel ?? DEFAULT_EMBEDDING_MODEL,
    embeddingDimensions: config?.embeddingDimensions ?? EMBEDDING_DIMENSIONS,
    maxAnalysisLen: config?.maxAnalysisLen ?? MAX_CONTENT_LENGTH_ANALYSIS,
    maxEmbeddingLen: config?.maxEmbeddingLen ?? MAX_CONTENT_LENGTH_EMBEDDING,
  };
}

================
File: src/lib/openai/types.ts
================
export type AnalysisInput = {
  title: string | null;
  canonical_url: string | null;
  text: string;
};

export type AnalysisResult = {
  why_it_matters: string;
  chili: number;
  confidence: number;
  citations: Record<string, unknown>;
};

export type EmbeddingInput = {
  title: string | null;
  text: string;
};

export type EmbeddingResult = {
  embedding: number[];
};

================
File: src/lib/youtube/check-quota-status.ts
================
import { log } from '../../log.js';
import type { QuotaStatus } from './types.js';
import type { YouTubeClient } from './youtube-client.js';

const PAD_WIDTH = 2;

export function checkQuotaStatus(
  client: Pick<YouTubeClient, 'quotaLimit' | 'quotaBuffer' | 'quotaResetHour'>,
  currentUsage = 0
): QuotaStatus {
  const now = new Date();
  const today = now.toISOString().split('T')[0];

  const resetTime = new Date(
    `${today}T${String(client.quotaResetHour).padStart(PAD_WIDTH, '0')}:00:00.000Z`
  );
  const hasReset = now >= resetTime;

  const used = hasReset ? currentUsage : 0;
  const remaining = client.quotaLimit - used;
  const canProceed = remaining > client.quotaBuffer;

  log('youtube_quota_check', {
    used,
    remaining,
    canProceed,
    quotaLimit: client.quotaLimit,
    quotaBuffer: client.quotaBuffer,
    resetTime: resetTime.toISOString(),
  });

  return { used, remaining, canProceed };
}

================
File: src/lib/youtube/get-channel-uploads.ts
================
import { log } from '../../log.js';
import { withRetry } from '../../utils/retry.js';
import type { YouTubeVideo } from './types.js';
import type { YouTubeClient } from './youtube-client.js';

const PLAYLIST_ITEMS_QUOTA_COST = 1;
const MAX_VIDEOS_PER_REQUEST = 50;

export async function getChannelUploads(
  client: YouTubeClient,
  uploadsPlaylistId: string,
  maxResults = 10,
  publishedAfter?: string
): Promise<YouTubeVideo[]> {
  try {
    log('youtube_get_uploads_start', {
      uploadsPlaylistId,
      maxResults,
      publishedAfter,
    });

    const response = await withRetry(() =>
      client.youtube.playlistItems.list({
        part: ['snippet', 'contentDetails'],
        playlistId: uploadsPlaylistId,
        maxResults: Math.min(maxResults, MAX_VIDEOS_PER_REQUEST),
      })
    );

    let videos: YouTubeVideo[] = [];

    for (const item of response.data.items || []) {
      const videoId = item.contentDetails?.videoId;
      const snippet = item.snippet;
      if (videoId && snippet) {
        videos.push({
          videoId,
          title: snippet.title || '',
          description: snippet.description || '',
          publishedAt: snippet.publishedAt || '',
          channelId: snippet.channelId || '',
          channelTitle: snippet.channelTitle || '',
          thumbnailUrl: snippet.thumbnails?.medium?.url || undefined,
        });
      }
    }

    if (publishedAfter) {
      const cutoff = new Date(publishedAfter).toISOString();
      videos = videos.filter((v) => v.publishedAt >= cutoff);
    }

    log('youtube_get_uploads_complete', {
      uploadsPlaylistId,
      videosFound: videos.length,
      quotaUsed: PLAYLIST_ITEMS_QUOTA_COST,
    });

    return videos;
  } catch (error) {
    log(
      'youtube_get_uploads_error',
      {
        uploadsPlaylistId,
        error: String(error),
      },
      'error'
    );
    throw error;
  }
}

================
File: src/lib/youtube/get-video-details.ts
================
import type { youtube_v3 } from 'googleapis';
import { log } from '../../log.js';
import { withRetry } from '../../utils/retry.js';
import type { YouTubeVideo } from './types.js';
import type { YouTubeClient } from './youtube-client.js';

const VIDEO_DETAILS_QUOTA_COST = 1;
const MAX_VIDEOS_PER_REQUEST = 50;
const SAMPLE_VIDEO_IDS_COUNT = 5;

function chunkVideoIds(videoIds: string[]): string[][] {
  const chunks: string[][] = [];
  for (let i = 0; i < videoIds.length; i += MAX_VIDEOS_PER_REQUEST) {
    chunks.push(videoIds.slice(i, i + MAX_VIDEOS_PER_REQUEST));
  }
  return chunks;
}

function parseVideoItem(item: youtube_v3.Schema$Video): YouTubeVideo | null {
  if (!(item.snippet && item.id)) {
    return null;
  }

  return {
    videoId: item.id,
    title: item.snippet.title || '',
    description: item.snippet.description || '',
    publishedAt: item.snippet.publishedAt || '',
    channelId: item.snippet.channelId || '',
    channelTitle: item.snippet.channelTitle || '',
    thumbnailUrl: item.snippet.thumbnails?.medium?.url || undefined,
    duration: item.contentDetails?.duration || undefined,
    viewCount: item.statistics?.viewCount
      ? Number.parseInt(item.statistics.viewCount, 10)
      : undefined,
    likeCount: item.statistics?.likeCount
      ? Number.parseInt(item.statistics.likeCount, 10)
      : undefined,
  };
}

async function fetchVideoChunk(
  client: YouTubeClient,
  chunk: string[]
): Promise<YouTubeVideo[]> {
  const response = await withRetry(() =>
    client.youtube.videos.list({
      part: ['snippet', 'statistics', 'contentDetails'],
      id: chunk,
    })
  );

  const videos: YouTubeVideo[] = [];
  for (const item of response.data.items || []) {
    const video = parseVideoItem(item);
    if (video) {
      videos.push(video);
    }
  }
  return videos;
}

export async function getVideoDetails(
  client: YouTubeClient,
  videoIds: string[]
): Promise<YouTubeVideo[]> {
  if (videoIds.length === 0) {
    return [];
  }

  try {
    log('youtube_get_video_details_start', {
      videoCount: videoIds.length,
      videoIds: videoIds.slice(0, SAMPLE_VIDEO_IDS_COUNT),
    });

    const chunks = chunkVideoIds(videoIds);
    const allVideos: YouTubeVideo[] = [];

    for (const chunk of chunks) {
      const videos = await fetchVideoChunk(client, chunk);
      allVideos.push(...videos);
    }

    log('youtube_get_video_details_complete', {
      requestedCount: videoIds.length,
      foundCount: allVideos.length,
      quotaUsed: chunks.length * VIDEO_DETAILS_QUOTA_COST,
    });

    return allVideos;
  } catch (error) {
    log(
      'youtube_get_video_details_error',
      {
        videoCount: videoIds.length,
        error: String(error),
      },
      'error'
    );
    throw error;
  }
}

================
File: src/lib/youtube/search-channels.ts
================
import { log } from '../../log.js';
import { withRetry } from '../../utils/retry.js';
import type { YouTubeChannel } from './types.js';
import type { YouTubeClient } from './youtube-client.js';

const SEARCH_QUOTA_COST = 100;
const CHANNEL_DETAILS_QUOTA_COST = 1;

export async function searchChannels(
  client: YouTubeClient,
  query: string,
  maxResults = 10
): Promise<YouTubeChannel[]> {
  try {
    log('youtube_search_channels_start', { query, maxResults });

    const searchResponse = await withRetry(() =>
      client.youtube.search.list({
        part: ['snippet'],
        q: query,
        type: ['channel'],
        maxResults,
      })
    );

    const channels: YouTubeChannel[] = [];

    for (const item of searchResponse.data?.items || []) {
      if (item.snippet && item.id?.channelId) {
        const channelId = item.id.channelId;
        const channelDetails = await withRetry(() =>
          client.youtube.channels.list({
            part: ['contentDetails'],
            id: [channelId],
          })
        );

        const uploadsPlaylistId =
          channelDetails.data.items?.[0]?.contentDetails?.relatedPlaylists
            ?.uploads;

        if (uploadsPlaylistId) {
          channels.push({
            channelId,
            title: item.snippet.title || '',
            description: item.snippet.description || '',
            uploadsPlaylistId,
          });
        }
      }
    }

    log('youtube_search_channels_complete', {
      query,
      channelsFound: channels.length,
      quotaUsed:
        SEARCH_QUOTA_COST + channels.length * CHANNEL_DETAILS_QUOTA_COST,
    });

    return channels;
  } catch (error) {
    log(
      'youtube_search_channels_error',
      {
        query,
        error: String(error),
      },
      'error'
    );
    throw error;
  }
}

================
File: src/lib/youtube/search-videos.ts
================
import { log } from '../../log.js';
import { withRetry } from '../../utils/retry.js';
import type { VideoSearchOptions, YouTubeVideo } from './types.js';
import type { YouTubeClient } from './youtube-client.js';

const SEARCH_QUOTA_COST = 100;
const MAX_VIDEOS_PER_REQUEST = 50;

export async function searchVideos(
  client: YouTubeClient,
  options: VideoSearchOptions
): Promise<YouTubeVideo[]> {
  const {
    query,
    maxResults = 10,
    publishedAfter,
    order = 'relevance',
    duration = 'any',
  } = options;

  try {
    log('youtube_search_videos_start', {
      query,
      maxResults,
      publishedAfter,
      order,
      duration,
    });

    const searchResponse = await withRetry(() =>
      client.youtube.search.list({
        part: ['snippet'],
        q: query,
        type: ['video'],
        maxResults: Math.min(maxResults, MAX_VIDEOS_PER_REQUEST),
        publishedAfter,
        order,
        videoDuration: duration,
      })
    );

    const videos = extractVideosFromSearchResponse(searchResponse);

    log('youtube_search_videos_complete', {
      query,
      videosFound: videos.length,
      quotaUsed: SEARCH_QUOTA_COST,
    });

    return videos;
  } catch (error) {
    log(
      'youtube_search_videos_error',
      {
        query,
        error: String(error),
      },
      'error'
    );
    throw error;
  }
}

function extractVideosFromSearchResponse(searchResponse: {
  data: import('googleapis').youtube_v3.Schema$SearchListResponse;
}): YouTubeVideo[] {
  const videos: YouTubeVideo[] = [];
  for (const item of searchResponse.data?.items || []) {
    const videoId = item.id?.videoId;
    const snippet = item.snippet;
    if (videoId && snippet) {
      videos.push({
        videoId,
        title: snippet.title || '',
        description: snippet.description || '',
        publishedAt: snippet.publishedAt || '',
        channelId: snippet.channelId || '',
        channelTitle: snippet.channelTitle || '',
        thumbnailUrl: snippet.thumbnails?.medium?.url || undefined,
      });
    }
  }
  return videos;
}

================
File: src/lib/youtube/types.ts
================
export type YouTubeVideo = {
  videoId: string;
  title: string;
  description: string;
  publishedAt: string;
  channelId: string;
  channelTitle: string;
  thumbnailUrl?: string;
  duration?: string;
  viewCount?: number;
  likeCount?: number;
  // Optionals used by downstream metadata; may be undefined
  commentCount?: number;
  thumbnails?: Record<string, unknown>;
  tags?: string[];
  categoryId?: string;
  defaultLanguage?: string;
  defaultAudioLanguage?: string;
};

export type YouTubeChannel = {
  channelId: string;
  title: string;
  description: string;
  uploadsPlaylistId: string;
};

export type VideoSearchOptions = {
  query: string;
  maxResults?: number;
  publishedAfter?: string;
  order?: 'date' | 'relevance' | 'viewCount';
  duration?: 'short' | 'medium' | 'long' | 'any';
};

export type QuotaStatus = {
  used: number;
  remaining: number;
  canProceed: boolean;
};

================
File: src/lib/youtube/youtube-client.ts
================
import { google, type youtube_v3 } from 'googleapis';

export type YouTubeClient = {
  youtube: youtube_v3.Youtube;
  quotaLimit: number;
  quotaBuffer: number;
  quotaResetHour: number;
};

export function createYouTubeClient(config?: {
  apiKey?: string;
  quotaLimit?: number;
  quotaBuffer?: number;
  quotaResetHour?: number;
}): YouTubeClient {
  const apiKey = config?.apiKey ?? process.env.YOUTUBE_API_KEY;
  if (!apiKey) {
    throw new Error('YOUTUBE_API_KEY environment variable is required');
  }

  const youtube = google.youtube({ version: 'v3', auth: apiKey });

  const quotaLimit = Number.parseInt(
    String(config?.quotaLimit ?? process.env.YOUTUBE_QUOTA_LIMIT ?? '10000'),
    10
  );
  const quotaBuffer = Number.parseInt(
    String(
      config?.quotaBuffer ?? process.env.YOUTUBE_RATE_LIMIT_BUFFER ?? '500'
    ),
    10
  );
  const quotaResetHour = Number.parseInt(
    String(
      config?.quotaResetHour ?? process.env.YOUTUBE_QUOTA_RESET_HOUR ?? '0'
    ),
    10
  );

  return { youtube, quotaLimit, quotaBuffer, quotaResetHour };
}

================
File: src/storage/generate-vtt-content.ts
================
const SECONDS_PER_HOUR = 3600;
const SECONDS_PER_MINUTE = 60;
const PADDING_HOURS_MINUTES = 2;
const PADDING_SECONDS = 6;
const DECIMAL_PLACES = 3;

type TranscriptionSegment = { start?: number; end?: number; text?: string };
type TranscriptionResult = { segments?: TranscriptionSegment[] };

function formatVTTTime(seconds: number): string {
  const hours = Math.floor(seconds / SECONDS_PER_HOUR);
  const minutes = Math.floor((seconds % SECONDS_PER_HOUR) / SECONDS_PER_MINUTE);
  const secs = seconds % SECONDS_PER_MINUTE;
  const h = hours.toString().padStart(PADDING_HOURS_MINUTES, '0');
  const m = minutes.toString().padStart(PADDING_HOURS_MINUTES, '0');
  const s = secs.toFixed(DECIMAL_PLACES).padStart(PADDING_SECONDS, '0');
  return `${h}:${m}:${s}`;
}

export function generateVTTContent(
  transcriptionResult: TranscriptionResult
): string {
  const segments = transcriptionResult.segments || [];
  let vttContent = 'WEBVTT\n\n';
  for (let i = 0; i < segments.length; i++) {
    const segment = segments[i];
    const startTime = formatVTTTime(segment.start || 0);
    const endTime = formatVTTTime(segment.end || segment.start || 0);
    const text = (segment.text || '').trim();
    if (text) {
      vttContent += `${i + 1}\n`;
      vttContent += `${startTime} --> ${endTime}\n`;
      vttContent += `${text}\n\n`;
    }
  }
  return vttContent;
}

================
File: src/storage/prepare-youtube-transcript.ts
================
import { log } from '../log.js';

export type TranscriptData = {
  vttContent: string;
  transcriptUrl: string;
  success: boolean;
  error?: string;
};

export function prepareYouTubeTranscript(
  videoId: string,
  vttContent: string,
  plainText: string
): TranscriptData {
  try {
    log('youtube_transcript_prepare_start', {
      videoId,
      vttSize: vttContent.length,
      txtSize: plainText.length,
    });
    if (!(vttContent.trim() && plainText.trim())) {
      throw new Error('Empty transcript content');
    }
    const transcriptUrl = `youtube://${videoId}`;
    log('youtube_transcript_prepare_success', {
      videoId,
      transcriptUrl,
      vttSize: vttContent.length,
      txtSize: plainText.length,
    });
    return { vttContent, transcriptUrl, success: true };
  } catch (error) {
    const errorMessage = String(error);
    log(
      'youtube_transcript_prepare_error',
      { videoId, error: errorMessage },
      'error'
    );
    return {
      vttContent: '',
      transcriptUrl: '',
      success: false,
      error: errorMessage,
    };
  }
}

================
File: src/tasks/analyze-story.ts
================
import {
  getStoryWithContent,
  upsertStoryEmbedding,
  upsertStoryOverlay,
} from '../db.js';
import { MODEL_VERSION_LABELS } from '../lib/openai/constants.js';
import { generateAnalysis as oaGenerateAnalysis } from '../lib/openai/generate-analysis.js';
import { generateEmbedding as oaGenerateEmbedding } from '../lib/openai/generate-embedding.js';
import { generateStubAnalysis } from '../lib/openai/generate-stub-analysis.js';
import { generateStubEmbedding } from '../lib/openai/generate-stub-embedding.js';
import { createOpenAIClient } from '../lib/openai/openai-client.js';
import type {
  AnalysisInput,
  AnalysisResult,
  EmbeddingResult,
} from '../lib/openai/types.js';
import { log } from '../log.js';

const USE_OPENAI = !!process.env.OPENAI_API_KEY;

export async function analyzeStory(storyId: string): Promise<void> {
  const story = await getStoryWithContent(storyId);
  if (!story) {
    throw new Error(`Story not found: ${storyId}`);
  }

  log('analyze_story_start', {
    comp: 'analyze',
    story_id: storyId,
    title: story.title,
    text_length: story.text.length,
  });

  try {
    const input: AnalysisInput = {
      title: story.title,
      canonical_url: story.canonical_url,
      text: story.text,
    };

    let analysis: AnalysisResult;
    let embedding: EmbeddingResult;
    if (USE_OPENAI) {
      const client = createOpenAIClient();
      analysis = await oaGenerateAnalysis(client, input).catch(async () => {
        return await generateStubAnalysis(input);
      });
      embedding = await oaGenerateEmbedding(client, input).catch(async () => {
        return await generateStubEmbedding(input);
      });
    } else {
      [analysis, embedding] = await Promise.all([
        generateStubAnalysis(input),
        generateStubEmbedding(input),
      ]);
    }

    await Promise.all([
      upsertStoryOverlay({
        story_id: storyId,
        why_it_matters: analysis.why_it_matters,
        chili: analysis.chili,
        confidence: analysis.confidence,
        citations: analysis.citations,
        model_version: USE_OPENAI ? MODEL_VERSION_LABELS.chat : 'stub-v1',
      }),
      upsertStoryEmbedding({
        story_id: storyId,
        embedding: embedding.embedding,
        model_version: USE_OPENAI ? MODEL_VERSION_LABELS.embedding : 'stub-v1',
      }),
    ]);

    log('analyze_story_success', {
      comp: 'analyze',
      story_id: storyId,
      chili: analysis.chili,
      confidence: analysis.confidence,
      embedding_dim: embedding.embedding.length,
    });
  } catch (error) {
    log(
      'analyze_story_error',
      { comp: 'analyze', story_id: storyId, error: String(error) },
      'error'
    );
    throw error;
  }
}

================
File: src/tasks/extract-article.ts
================
import { Readability } from '@mozilla/readability';
import { JSDOM } from 'jsdom';
import type PgBoss from 'pg-boss';
import {
  findRawItemsByIds,
  findStoryIdByContentHash,
  insertContents,
  insertStory,
} from '../db.js';
import { log } from '../log.js';
import { canonicalizeUrl, hashText } from '../util.js';
import { fetchWithTimeout } from '../utils/http.js';

const FETCH_TIMEOUT_MS = 15_000;

export async function extractArticle(
  jobData: { rawItemIds: string[] },
  boss: PgBoss
) {
  const rows = await findRawItemsByIds(jobData.rawItemIds || []);
  for (const row of rows) {
    try {
      const t0 = Date.now();
      const resp = await fetchWithTimeout(
        row.url,
        { redirect: 'follow' },
        FETCH_TIMEOUT_MS
      );
      if (!resp.ok) {
        throw new Error(`HTTP ${resp.status}`);
      }
      const finalUrl = canonicalizeUrl(resp.url || row.url);
      const html = await resp.text();
      const dom = new JSDOM(html, { url: finalUrl });
      const reader = new Readability(dom.window.document);
      const parsed = reader.parse();
      const text = parsed?.textContent?.trim() || '';
      if (!text) {
        throw new Error('no_text_extracted');
      }

      const content_hash = hashText(text);
      const content_id = await insertContents({
        raw_item_id: row.id,
        text,
        html_url: finalUrl,
        lang: null,
        content_hash,
      });

      const existingStoryId = await findStoryIdByContentHash(content_hash);
      const storyId =
        existingStoryId ??
        (await insertStory({
          content_id,
          title: parsed?.title || row.title || null,
          canonical_url: finalUrl,
          primary_url: finalUrl,
          kind: 'article',
          published_at: null,
        }));

      await boss.send('analyze:llm', { storyId });
      log('extract_success', {
        comp: 'extract',
        raw_item_id: row.id,
        content_id,
        story_id: storyId,
        content_hash,
        url: finalUrl,
        text_len: text.length,
        duration_ms: Date.now() - t0,
      });
    } catch (err) {
      log(
        'extract_error',
        {
          comp: 'extract',
          raw_item_id: row.id,
          url: row.url,
          err: String(err),
        },
        'error'
      );
    }
  }
}

================
File: src/tasks/extract-youtube-audio.ts
================
import { extractAudio as vanillaExtractAudio } from '../extract/extract-youtube-audio.js';
export const extractYouTubeAudio = vanillaExtractAudio;

================
File: src/tasks/extract-youtube-content.ts
================
import type PgBoss from 'pg-boss';
import {
  findRawItemsByIds,
  findStoryIdByContentHash,
  insertContents,
  insertStory,
} from '../db.js';
import { extractAudio } from '../extract/extract-youtube-audio.js';
import { log } from '../log.js';
import { generateVTTContent } from '../storage/generate-vtt-content.js';
import { prepareYouTubeTranscript } from '../storage/prepare-youtube-transcript.js';
import { transcribeAudio } from '../transcribe/whisper.js';
import { hashText } from '../util.js';
import { cleanupVideoTempFiles } from '../utils/temp-files.js';

// Constants
const SECONDS_PER_HOUR = 3600;
const SECONDS_PER_MINUTE = 60;
const MILLISECONDS_PER_SECOND = 1000;
const BYTES_PER_KB = 1024;
const BYTES_PER_MB = BYTES_PER_KB * BYTES_PER_KB;
const DESCRIPTION_MAX_LENGTH = 500;
const MAX_PREVIEW_SEGMENTS = 10;
const TIME_PAD_LENGTH = 2;
const UPLOAD_DATE_LENGTH = 8;
const UPLOAD_DATE_YEAR_END = 4;
const UPLOAD_DATE_MONTH_START = 4;
const UPLOAD_DATE_MONTH_END = 6;
const UPLOAD_DATE_DAY_START = 6;
const UPLOAD_DATE_DAY_END = 8;
const ROUNDING_FACTOR = 100;

const UPLOAD_DATE_REGEX = /(\d{4})(\d{2})(\d{2})/;

export type YouTubeExtractionJobData = {
  rawItemIds: string[];
  videoId: string;
  sourceKind: string;
};

type VideoMetadata = {
  title?: string;
  uploader?: string;
  duration?: number;
  uploadDate?: string;
  viewCount?: number | string;
  description?: string;
  filesize?: number;
};

type TranscriptionResult = {
  language: string;
  modelUsed: string;
  segments: Array<{ start: number; end: number; text: string }>;
  processingTimeMs: number;
  text: string;
  success?: boolean;
  error?: string;
};

export async function extractYouTubeContent(
  jobData: YouTubeExtractionJobData,
  boss: PgBoss
) {
  const { rawItemIds, videoId, sourceKind } = jobData;
  const rows = await findRawItemsByIds(rawItemIds);

  for (const row of rows) {
    await processYouTubeRow(row, videoId, sourceKind, boss);
  }
}

type RawItemRow = { id: string; url: string; title: string | null };

async function processYouTubeRow(
  row: RawItemRow,
  videoId: string,
  sourceKind: string,
  boss: PgBoss
): Promise<void> {
  const t0 = Date.now();
  const videoUrl = row.url;

  try {
    log('youtube_extract_start', {
      comp: 'extract',
      raw_item_id: row.id,
      video_id: videoId,
      source_kind: sourceKind,
      url: row.url,
    });

    const audioResult = await ensureAudioExtracted(videoUrl, videoId);

    log('youtube_audio_extracted', {
      comp: 'extract',
      raw_item_id: row.id,
      video_id: videoId,
      audio_path: audioResult.audioPath,
      duration: audioResult.metadata.duration,
      file_size_mb:
        Math.round(
          ((audioResult.metadata.filesize || 0) / BYTES_PER_MB) *
            ROUNDING_FACTOR
        ) / ROUNDING_FACTOR,
    });

    const transcriptionResult = await transcribeOrThrow(
      audioResult.audioPath,
      videoId
    );

    log('youtube_transcription_complete', {
      comp: 'extract',
      raw_item_id: row.id,
      video_id: videoId,
      language: transcriptionResult.language,
      text_length: transcriptionResult.text.length,
      segments: transcriptionResult.segments.length,
      processing_time_ms: transcriptionResult.processingTimeMs,
    });

    const transcriptText = ensureTranscriptText(transcriptionResult);

    const vttContent = generateVTTContent(transcriptionResult);
    const transcriptData = await prepareYouTubeTranscript(
      videoId,
      vttContent,
      transcriptText
    );
    if (!transcriptData.success) {
      throw new Error(`Transcript preparation failed: ${transcriptData.error}`);
    }

    const enhancedText = formatYouTubeContent(
      transcriptText,
      audioResult.metadata,
      transcriptionResult
    );

    const content_hash = hashText(enhancedText);
    const content_id = await insertContents({
      raw_item_id: row.id,
      text: enhancedText,
      html_url: videoUrl,
      transcript_url: transcriptData.transcriptUrl,
      transcript_vtt: transcriptData.vttContent,
      lang: transcriptionResult.language,
      content_hash,
    });

    const existingStoryId = await findStoryIdByContentHash(content_hash);
    const storyId =
      existingStoryId ??
      (await insertStory({
        content_id,
        title: audioResult.metadata.title || row.title || null,
        canonical_url: videoUrl,
        primary_url: videoUrl,
        kind: 'youtube',
        published_at: audioResult.metadata.uploadDate
          ? new Date(
              audioResult.metadata.uploadDate.replace(
                UPLOAD_DATE_REGEX,
                '$1-$2-$3'
              )
            ).toISOString()
          : null,
      }));

    await boss.send('analyze:llm', { storyId });
    await cleanupVideoTempFiles(videoId);

    log('youtube_extract_success', {
      comp: 'extract',
      raw_item_id: row.id,
      content_id,
      story_id: storyId,
      video_id: videoId,
      content_hash,
      url: videoUrl,
      text_len: enhancedText.length,
      duration: audioResult.metadata.duration,
      language: transcriptionResult.language,
      duration_ms: Date.now() - t0,
    });
  } catch (err) {
    log(
      'youtube_extract_error',
      {
        comp: 'extract',
        raw_item_id: row.id,
        video_id: videoId,
        url: row.url,
        err: String(err),
      },
      'error'
    );
    try {
      await cleanupVideoTempFiles(videoId);
    } catch {
      // ignore cleanup errors
    }
  }
}

async function ensureAudioExtracted(videoUrl: string, videoId: string) {
  const audioResult = await extractAudio(videoUrl, videoId);
  if (!audioResult.success) {
    throw new Error(`Audio extraction failed: ${audioResult.error}`);
  }
  return audioResult;
}

async function transcribeOrThrow(audioPath: string, videoId: string) {
  const transcriptionResult = await transcribeAudio(audioPath, videoId, {
    model: 'base',
    language: undefined,
    wordTimestamps: true,
  });
  if (!transcriptionResult.success) {
    throw new Error(
      `Transcription failed: ${String(transcriptionResult.error || 'unknown')}`
    );
  }
  return transcriptionResult as TranscriptionResult;
}

function ensureTranscriptText(
  transcriptionResult: TranscriptionResult
): string {
  const transcriptText = transcriptionResult.text.trim();
  if (!transcriptText) {
    throw new Error('No text extracted from transcription');
  }
  return transcriptText;
}

function formatYouTubeContent(
  transcriptText: string,
  metadata: VideoMetadata,
  transcriptionResult: TranscriptionResult
): string {
  const sections: string[] = [];

  sections.push('=== VIDEO INFORMATION ===');
  sections.push(`Title: ${metadata.title}`);
  sections.push(`Channel: ${metadata.uploader}`);
  sections.push(`Duration: ${formatDuration(metadata.duration)}`);
  sections.push(`Upload Date: ${formatUploadDate(metadata.uploadDate)}`);
  if (metadata.viewCount) {
    const viewsNum =
      typeof metadata.viewCount === 'string'
        ? Number.parseInt(metadata.viewCount, 10)
        : metadata.viewCount;
    if (Number.isFinite(viewsNum)) {
      sections.push(`Views: ${viewsNum.toLocaleString()}`);
    }
  }
  if (metadata.description) {
    sections.push(
      `Description: ${metadata.description.substring(0, DESCRIPTION_MAX_LENGTH)}${metadata.description.length > DESCRIPTION_MAX_LENGTH ? '...' : ''}`
    );
  }

  sections.push('');
  sections.push('=== TRANSCRIPTION INFORMATION ===');
  sections.push(`Language: ${transcriptionResult.language}`);
  sections.push(`Model: ${transcriptionResult.modelUsed}`);
  sections.push(`Segments: ${transcriptionResult.segments.length}`);
  sections.push(
    `Processing Time: ${Math.round(transcriptionResult.processingTimeMs / MILLISECONDS_PER_SECOND)}s`
  );

  sections.push('');
  sections.push('=== TRANSCRIPT ===');
  sections.push(transcriptText);

  if (transcriptionResult.segments && transcriptionResult.segments.length > 0) {
    sections.push('');
    sections.push('=== TIMESTAMPED SEGMENTS ===');
    const maxSegments = Math.min(
      MAX_PREVIEW_SEGMENTS,
      transcriptionResult.segments.length
    );
    for (let i = 0; i < maxSegments; i++) {
      const segment = transcriptionResult.segments[i];
      const startTime = formatTimestamp(segment.start);
      const endTime = formatTimestamp(segment.end);
      sections.push(`[${startTime} - ${endTime}] ${segment.text.trim()}`);
    }
    if (transcriptionResult.segments.length > maxSegments) {
      sections.push(
        `... and ${transcriptionResult.segments.length - maxSegments} more segments`
      );
    }
  }

  return sections.join('\n');
}

function formatDuration(seconds?: number): string {
  if (!seconds) {
    return '0:00';
  }
  const hours = Math.floor(seconds / SECONDS_PER_HOUR);
  const minutes = Math.floor((seconds % SECONDS_PER_HOUR) / SECONDS_PER_MINUTE);
  const secs = Math.floor(seconds % SECONDS_PER_MINUTE);
  if (hours > 0) {
    return `${hours}:${minutes.toString().padStart(TIME_PAD_LENGTH, '0')}:${secs.toString().padStart(TIME_PAD_LENGTH, '0')}`;
  }
  return `${minutes}:${secs.toString().padStart(TIME_PAD_LENGTH, '0')}`;
}

function formatUploadDate(uploadDate?: string): string {
  if (!uploadDate || uploadDate.length !== UPLOAD_DATE_LENGTH) {
    return 'Unknown';
  }
  const year = uploadDate.substring(0, UPLOAD_DATE_YEAR_END);
  const month = uploadDate.substring(
    UPLOAD_DATE_MONTH_START,
    UPLOAD_DATE_MONTH_END
  );
  const day = uploadDate.substring(UPLOAD_DATE_DAY_START, UPLOAD_DATE_DAY_END);
  return `${year}-${month}-${day}`;
}

function formatTimestamp(seconds?: number): string {
  const minutes = Math.floor((seconds || 0) / SECONDS_PER_MINUTE);
  const secs = Math.floor((seconds || 0) % SECONDS_PER_MINUTE);
  return `${minutes}:${secs.toString().padStart(TIME_PAD_LENGTH, '0')}`;
}

================
File: src/tasks/fetch-youtube-channel-videos.ts
================
import { getChannelUploads } from '../lib/youtube/get-channel-uploads.js';
import { getVideoDetails } from '../lib/youtube/get-video-details.js';
import type { YouTubeVideo } from '../lib/youtube/types.js';
import { createYouTubeClient } from '../lib/youtube/youtube-client.js';
import { log } from '../log.js';
import { quotaTracker } from '../utils/quota-tracker.js';

const QUOTA_COST_PER_PLAYLIST_PAGE = 1;
const MAX_ITEMS_PER_PAGE = 50;

export async function fetchYouTubeChannelVideos(input: {
  uploadsPlaylistId: string;
  maxResults?: number;
  publishedAfter?: string;
}): Promise<YouTubeVideo[]> {
  const { uploadsPlaylistId, maxResults = 50, publishedAfter } = input;
  const client = createYouTubeClient();

  log('youtube_fetch_channel_videos_start', {
    uploadsPlaylistId,
    maxResults,
    publishedAfter,
  });

  const estimatedCost =
    QUOTA_COST_PER_PLAYLIST_PAGE + Math.ceil(maxResults / MAX_ITEMS_PER_PAGE);
  if (!quotaTracker.reserveQuota('channel_videos', estimatedCost)) {
    const status = quotaTracker.checkQuotaStatus();
    throw new Error(
      `Insufficient quota. Need ${estimatedCost}, have ${status.remaining}`
    );
  }

  const videos = await getChannelUploads(
    client,
    uploadsPlaylistId,
    maxResults,
    publishedAfter
  );
  const videoIds = videos.map((v) => v.videoId);
  const detailedVideos = await getVideoDetails(client, videoIds);

  const actualCost =
    QUOTA_COST_PER_PLAYLIST_PAGE +
    Math.ceil(videoIds.length / MAX_ITEMS_PER_PAGE);
  quotaTracker.consumeQuota('channel_videos', actualCost);

  log('youtube_fetch_channel_videos_complete', {
    uploadsPlaylistId,
    videosFound: detailedVideos.length,
    quotaUsed: actualCost,
    quotaStatus: quotaTracker.checkQuotaStatus(),
  });

  return detailedVideos;
}

================
File: src/tasks/fetch-youtube-search-videos.ts
================
import { searchVideos as ytSearchVideos } from '../lib/youtube/search-videos.js';
import type { YouTubeVideo } from '../lib/youtube/types.js';
import { createYouTubeClient } from '../lib/youtube/youtube-client.js';
import { log } from '../log.js';
import { quotaTracker } from '../utils/quota-tracker.js';

const QUOTA_COST_SEARCH = 100;

export async function fetchYouTubeSearchVideos(input: {
  query: string;
  maxResults?: number;
  publishedAfter?: string;
  order?: 'date' | 'relevance' | 'viewCount';
  duration?: 'short' | 'medium' | 'long' | 'any';
}): Promise<YouTubeVideo[]> {
  const {
    query,
    maxResults = 25,
    publishedAfter,
    order = 'relevance',
    duration = 'any',
  } = input;
  const client = createYouTubeClient();

  log('youtube_search_videos_start', {
    query,
    maxResults,
    publishedAfter,
    order,
    duration,
  });

  if (!quotaTracker.reserveQuota('search_videos', QUOTA_COST_SEARCH)) {
    const status = quotaTracker.checkQuotaStatus();
    throw new Error(
      `Insufficient quota. Need ${QUOTA_COST_SEARCH}, have ${status.remaining}`
    );
  }

  const results: YouTubeVideo[] = await ytSearchVideos(client, {
    query,
    maxResults,
    publishedAfter,
    order,
    duration,
  });

  quotaTracker.consumeQuota('search_videos', QUOTA_COST_SEARCH);

  log('youtube_search_videos_complete', {
    query,
    resultsFound: results.length,
    quotaUsed: QUOTA_COST_SEARCH,
    quotaStatus: quotaTracker.checkQuotaStatus(),
  });

  return results;
}

================
File: src/tasks/ingest-rss-source.ts
================
import type PgBoss from 'pg-boss';
import { upsertRawItem, upsertSourceHealth } from '../db.js';
import { buildRawItemArticle } from '../extract/build-raw-item-article.js';
import { normalizeRssItem } from '../extract/normalize-rss-item.js';
import { parseRssFeed } from '../extract/parse-rss-feed.js';
import { log } from '../log.js';
import { fetchWithTimeout } from '../utils/http.js';

const FETCH_TIMEOUT_MS = 15_000;

export async function ingestRssSource(
  boss: PgBoss,
  src: { id: string; url: string }
): Promise<{ seen: number; newCount: number }> {
  const t0 = Date.now();
  log('ingest_source_start', {
    comp: 'ingest',
    kind: 'rss',
    source_id: src.id,
    url: src.url,
  });

  const res = await fetchWithTimeout(
    src.url,
    { redirect: 'follow' },
    FETCH_TIMEOUT_MS
  );
  if (!res.ok) {
    throw new Error(`HTTP ${res.status}`);
  }
  const xml = await res.text();

  const items = parseRssFeed(xml);
  let seen = 0;
  let newCount = 0;

  for (const it of items) {
    const norm = normalizeRssItem(it, src.url);
    if (!norm) {
      continue;
    }
    seen++;
    const payload = buildRawItemArticle(norm, src.id);
    const id = await upsertRawItem(payload);
    if (id) {
      await boss.send('ingest:fetch-content', { rawItemIds: [id] });
      newCount++;
    }
  }

  await upsertSourceHealth(src.id, 'ok', null);
  log('ingest_source_done', {
    comp: 'ingest',
    kind: 'rss',
    source_id: src.id,
    url: src.url,
    items_seen: seen,
    items_new: newCount,
    duration_ms: Date.now() - t0,
  });
  return { seen, newCount };
}

================
File: src/tasks/ingest-youtube-source.ts
================
import type PgBoss from 'pg-boss';
import { upsertRawItem, upsertSourceHealth } from '../db.js';
import { buildRawItemYouTube } from '../extract/build-raw-item-youtube.js';
import type { YouTubeVideo } from '../lib/youtube/types.js';
import { log } from '../log.js';
import type { SourceBase } from '../types/sources.js';
import { fetchYouTubeChannelVideos } from './fetch-youtube-channel-videos.js';
import { fetchYouTubeSearchVideos } from './fetch-youtube-search-videos.js';
import { resolveYouTubeUploadsId } from './resolve-youtube-uploads-id.js';

const DEFAULT_MAX_VIDEOS = 10;
const DAYS_LOOKBACK = 7;
const HOURS_PER_DAY = 24;
const MINUTES_PER_HOUR = 60;
const SECONDS_PER_MINUTE = 60;
const MS_PER_SECOND = 1000;
const MS_PER_DAY =
  HOURS_PER_DAY * MINUTES_PER_HOUR * SECONDS_PER_MINUTE * MS_PER_SECOND;

export async function ingestYouTubeSource(boss: PgBoss, src: SourceBase) {
  const t0 = Date.now();
  log('ingest_source_start', {
    comp: 'ingest',
    kind: src.kind,
    source_id: src.id,
    url: src.url,
    name: src.name,
  });

  let seen = 0;
  let newCount = 0;
  let errorMessage: string | null = null;

  try {
    const videos = await getVideosForSource(src);
    const result = await processVideos(boss, src, videos);
    seen = result.seen;
    newCount = result.newCount;

    log('ingest_source_done', {
      comp: 'ingest',
      kind: src.kind,
      source_id: src.id,
      videos_seen: seen,
      videos_new: newCount,
      duration_ms: Date.now() - t0,
    });
  } catch (err) {
    errorMessage = String(err);
    log(
      'ingest_source_error',
      {
        comp: 'ingest',
        kind: src.kind,
        source_id: src.id,
        url: src.url,
        err: errorMessage,
      },
      'error'
    );
  } finally {
    try {
      await upsertSourceHealth(
        src.id,
        errorMessage ? 'error' : 'ok',
        errorMessage
      );
    } catch (error) {
      log('source_health_update_error', { error: String(error) }, 'error');
    }
  }
}

async function getVideosForSource(src: SourceBase): Promise<YouTubeVideo[]> {
  const metadata = (src.metadata ?? {}) as Record<string, unknown>;
  if (src.kind === 'youtube_channel') {
    const uploadsPlaylistId = await resolveYouTubeUploadsId({
      sourceId: src.id,
      url: src.url || undefined,
      name: src.name || undefined,
      currentUploadsPlaylistId:
        (metadata.upload_playlist_id as string | undefined) || undefined,
    });
    const maxResults = parsePositiveInt(
      (metadata as Record<string, unknown>).max_videos_per_run,
      DEFAULT_MAX_VIDEOS
    );
    const publishedAfter = new Date(
      Date.now() - DAYS_LOOKBACK * MS_PER_DAY
    ).toISOString();
    return fetchYouTubeChannelVideos({
      uploadsPlaylistId,
      maxResults,
      publishedAfter,
    });
  }

  if (src.kind === 'youtube_search') {
    const maxResults = parsePositiveInt(
      metadata.max_results,
      DEFAULT_MAX_VIDEOS
    );
    const publishedAfter =
      parseOptionalString(metadata.published_after) ||
      new Date(Date.now() - DAYS_LOOKBACK * MS_PER_DAY).toISOString();
    const order = parseOrder(metadata.order);
    const duration = parseDuration(metadata.duration);
    const query = typeof metadata.query === 'string' ? metadata.query : '';
    if (!query) {
      throw new Error('No query in metadata');
    }
    return fetchYouTubeSearchVideos({
      query,
      maxResults,
      publishedAfter,
      order,
      duration,
    });
  }

  throw new Error(`unsupported_kind: ${src.kind}`);
}

async function processVideos(
  boss: PgBoss,
  src: SourceBase,
  videos: YouTubeVideo[]
): Promise<{ seen: number; newCount: number }> {
  let seen = 0;
  let newCount = 0;
  for (const v of videos) {
    seen++;
    const id = await upsertRawItem(buildRawItemYouTube(v, src));
    if (id) {
      await boss.send('ingest:fetch-youtube-content', {
        rawItemIds: [id],
        videoId: v.videoId,
        sourceKind: src.kind,
      });
      newCount++;
    }
  }
  return { seen, newCount };
}

type SearchOrder = 'date' | 'relevance' | 'viewCount';
type SearchDuration = 'short' | 'medium' | 'long' | 'any' | undefined;

function parseOrder(value: unknown): SearchOrder {
  const allowed: SearchOrder[] = ['date', 'relevance', 'viewCount'];
  return typeof value === 'string' && (allowed as string[]).includes(value)
    ? (value as SearchOrder)
    : 'relevance';
}

function parseDuration(value: unknown): SearchDuration {
  const allowed = new Set(['short', 'medium', 'long', 'any']);
  return typeof value === 'string' && allowed.has(value)
    ? (value as SearchDuration)
    : undefined;
}

function parsePositiveInt(value: unknown, fallback: number): number {
  if (typeof value === 'number' && Number.isFinite(value) && value > 0) {
    return Math.floor(value);
  }
  if (typeof value === 'string') {
    const n = Number.parseInt(value, 10);
    if (Number.isFinite(n) && n > 0) {
      return n;
    }
  }
  return fallback;
}

function parseOptionalString(value: unknown): string | undefined {
  return typeof value === 'string' && value ? value : undefined;
}

================
File: src/tasks/preview-rss-source.ts
================
import { normalizeRssItem } from '../extract/normalize-rss-item.js';
import { parseRssFeed } from '../extract/parse-rss-feed.js';
import { fetchWithTimeout } from '../utils/http.js';

const FETCH_TIMEOUT_MS = 15_000;

export async function previewRssSourceAction(
  src: { id: string; url: string },
  limit = 10
): Promise<{
  items: Array<{ title: string | null; url: string; external_id: string }>;
  quota: null;
}> {
  const res = await fetchWithTimeout(
    src.url,
    { redirect: 'follow' },
    FETCH_TIMEOUT_MS
  );
  if (!res.ok) {
    throw new Error(`HTTP ${res.status}`);
  }
  const xml = await res.text();
  const items = parseRssFeed(xml);
  const mapped = items
    .map((it) => normalizeRssItem(it, src.url))
    .filter((x): x is NonNullable<ReturnType<typeof normalizeRssItem>> =>
      Boolean(x)
    )
    .slice(0, limit)
    .map((n) => ({ title: n.title, url: n.url, external_id: n.externalId }))
    .filter((x) => x.url);
  return { items: mapped, quota: null };
}

================
File: src/tasks/preview-youtube-source.ts
================
import type { SourceBase } from '../types/sources.js';
import { quotaTracker } from '../utils/quota-tracker.js';
import { fetchYouTubeChannelVideos } from './fetch-youtube-channel-videos.js';
import { fetchYouTubeSearchVideos } from './fetch-youtube-search-videos.js';
import { resolveYouTubeUploadsId } from './resolve-youtube-uploads-id.js';

const DEFAULT_MAX_VIDEOS = 10;
const DAYS_LOOKBACK = 7;
const HOURS_PER_DAY = 24;
const MINUTES_PER_HOUR = 60;
const SECONDS_PER_MINUTE = 60;
const MS_PER_SECOND = 1000;
const MS_PER_DAY =
  HOURS_PER_DAY * MINUTES_PER_HOUR * SECONDS_PER_MINUTE * MS_PER_SECOND;

export async function previewYouTubeSourceAction(
  src: SourceBase,
  limit = DEFAULT_MAX_VIDEOS
) {
  if (src.kind === 'youtube_channel') {
    const metadata = (src.metadata ?? {}) as Record<string, unknown>;
    const maxResults = Math.min(
      parsePositiveInt(metadata.max_videos_per_run, DEFAULT_MAX_VIDEOS),
      limit
    );
    const publishedAfter = new Date(
      Date.now() - DAYS_LOOKBACK * MS_PER_DAY
    ).toISOString();
    const uploadsPlaylistId = await resolveYouTubeUploadsId({
      sourceId: src.id,
      url: src.url || undefined,
      name: src.name || undefined,
      currentUploadsPlaylistId:
        (metadata.upload_playlist_id as string | undefined) || undefined,
    });
    const videos = await fetchYouTubeChannelVideos({
      uploadsPlaylistId,
      maxResults,
      publishedAfter,
    });
    return {
      items: videos.slice(0, limit).map(toPreviewItem),
      quota: quotaTracker.checkQuotaStatus(),
    };
  }
  if (src.kind === 'youtube_search') {
    const metadata = (src.metadata ?? {}) as Record<string, unknown>;
    const query = typeof metadata.query === 'string' ? metadata.query : '';
    const maxResults = Math.min(
      parsePositiveInt(metadata.max_results, DEFAULT_MAX_VIDEOS),
      limit
    );
    const publishedAfter =
      parseOptionalString(metadata.published_after) ||
      new Date(Date.now() - DAYS_LOOKBACK * MS_PER_DAY).toISOString();
    const order = parseOrder(metadata.order);
    const duration = parseDuration(metadata.duration);
    const videos = await fetchYouTubeSearchVideos({
      query,
      maxResults,
      publishedAfter,
      order,
      duration,
    });
    return {
      items: videos.slice(0, limit).map(toPreviewItem),
      quota: quotaTracker.checkQuotaStatus(),
    };
  }
  throw new Error(`Unsupported kind for preview: ${src.kind}`);
}

function toPreviewItem(v: {
  title: string;
  videoId: string;
  publishedAt: string;
}) {
  return {
    title: v.title,
    url: `https://www.youtube.com/watch?v=${v.videoId}`,
    external_id: v.videoId,
    published_at: v.publishedAt,
  };
}

type SearchOrder = 'date' | 'relevance' | 'viewCount';
type SearchDuration = 'short' | 'medium' | 'long' | 'any' | undefined;

function parseOrder(value: unknown): SearchOrder {
  const allowed: SearchOrder[] = ['date', 'relevance', 'viewCount'];
  return typeof value === 'string' && (allowed as string[]).includes(value)
    ? (value as SearchOrder)
    : 'relevance';
}

function parseDuration(value: unknown): SearchDuration {
  const allowed = new Set(['short', 'medium', 'long', 'any']);
  return typeof value === 'string' && allowed.has(value)
    ? (value as SearchDuration)
    : undefined;
}

function parsePositiveInt(value: unknown, fallback: number): number {
  if (typeof value === 'number' && Number.isFinite(value) && value > 0) {
    return Math.floor(value);
  }
  if (typeof value === 'string') {
    const n = Number.parseInt(value, 10);
    if (Number.isFinite(n) && n > 0) {
      return n;
    }
  }
  return fallback;
}

function parseOptionalString(value: unknown): string | undefined {
  return typeof value === 'string' && value ? value : undefined;
}

================
File: src/tasks/resolve-youtube-uploads-id.ts
================
import { updateSourceMetadata } from '../db.js';
import { searchChannels } from '../lib/youtube/search-channels.js';
import { createYouTubeClient } from '../lib/youtube/youtube-client.js';
import { log } from '../log.js';

const YOUTUBE_HANDLE_REGEX = /@([A-Za-z0-9_\-.]+)/;

export async function resolveYouTubeUploadsId(input: {
  sourceId: string;
  url?: string | null;
  name?: string | null;
  currentUploadsPlaylistId?: string | null;
}): Promise<string> {
  // If we already have an ID, prefer it (validation happens when fetching videos)
  if (input.currentUploadsPlaylistId) {
    return input.currentUploadsPlaylistId;
  }

  const client = createYouTubeClient();
  let query = '';
  if (typeof input.url === 'string' && input.url.includes('youtube.com/')) {
    const match = input.url.match(YOUTUBE_HANDLE_REGEX);
    if (match) {
      query = match[1];
    }
  }
  if (!query && typeof input.name === 'string') {
    query = input.name;
  }

  log('youtube_derive_uploads_id_start', { source_id: input.sourceId, query });
  const candidates = await searchChannels(client, query);
  const chosen = candidates[0];
  if (!chosen?.uploadsPlaylistId) {
    throw new Error(
      `Could not derive uploads playlist id for source ${input.sourceId}`
    );
  }

  // Persist for next runs
  try {
    await updateSourceMetadata(input.sourceId, {
      upload_playlist_id: chosen.uploadsPlaylistId,
    });
  } catch (e) {
    log(
      'youtube_derive_uploads_id_persist_error',
      { source_id: input.sourceId, err: String(e) },
      'warn'
    );
  }

  log('youtube_derive_uploads_id_success', {
    source_id: input.sourceId,
    uploadsPlaylistId: chosen.uploadsPlaylistId,
  });
  return chosen.uploadsPlaylistId;
}

================
File: src/tasks/transcribe-audio.ts
================
import { transcribeAudio as vanillaTranscribeAudio } from '../transcribe/whisper.js';
export const transcribeAudioAction = vanillaTranscribeAudio;

================
File: src/transcribe/queue.ts
================
import { log } from '../log.js';
import { cleanupVideoTempFiles } from '../utils/temp-files.js';
import {
  type TranscriptionResult,
  type WhisperOptions,
  transcribeAudio,
} from './whisper.js';

// Constants
const SECONDS_PER_MINUTE = 60;
const MINUTES_PER_HOUR = 60;
const MILLISECONDS_PER_SECOND = 1000;
const MILLISECONDS_PER_HOUR =
  MINUTES_PER_HOUR * SECONDS_PER_MINUTE * MILLISECONDS_PER_SECOND;
const DEFAULT_CLEANUP_AGE_HOURS = 24;
const PROCESS_QUEUE_DELAY_MS = 100;
const CHECK_JOB_INTERVAL_MS = 1000;
const DEFAULT_WAIT_TIMEOUT_MINUTES = 30;
const DEFAULT_WAIT_TIMEOUT_MS =
  DEFAULT_WAIT_TIMEOUT_MINUTES * SECONDS_PER_MINUTE * MILLISECONDS_PER_SECOND;
const MIN_RETRY_DELAY_MS = 1000;
const MAX_RETRY_DELAY_MS = 30_000;
const RETRY_DELAY_BASE = 2;
const RECENT_JOBS_LIMIT = 10;
const CLEANUP_INTERVAL_MS =
  MINUTES_PER_HOUR * SECONDS_PER_MINUTE * MILLISECONDS_PER_SECOND;
const DEFAULT_MAX_RETRIES = 2;
const DEFAULT_MAX_CONCURRENT_JOBS = 1; // Whisper is CPU intensive

// Priority order values (lower value = higher priority)
const PRIORITY_HIGH = 0;
const PRIORITY_MEDIUM = 1;
const PRIORITY_LOW = 2;

export type TranscriptionJob = {
  id: string;
  videoId: string;
  audioPath: string;
  options: Partial<WhisperOptions>;
  priority: 'high' | 'medium' | 'low';
  createdAt: Date;
  startedAt?: Date;
  completedAt?: Date;
  result?: TranscriptionResult;
  error?: string;
  retryCount: number;
  maxRetries: number;
};

export type AddJobParams = {
  videoId: string;
  audioPath: string;
  options?: Partial<WhisperOptions>;
  priority?: 'high' | 'medium' | 'low';
  maxRetries?: number;
};

export type QueueStats = {
  pending: number;
  processing: number;
  completed: number;
  failed: number;
  totalProcessingTimeMs: number;
  averageProcessingTimeMs: number;
};

class TranscriptionQueue {
  private readonly jobs: Map<string, TranscriptionJob> = new Map();
  private readonly processingJobs: Set<string> = new Set();
  private readonly maxConcurrentJobs: number; // Whisper is CPU intensive
  private isProcessing = false;

  constructor(maxConcurrentJobs = DEFAULT_MAX_CONCURRENT_JOBS) {
    this.maxConcurrentJobs = maxConcurrentJobs;
  }

  /**
   * Add a transcription job to the queue
   */
  addJob(params: AddJobParams): string {
    const {
      videoId,
      audioPath,
      options = {},
      priority = 'medium',
      maxRetries = DEFAULT_MAX_RETRIES,
    } = params;
    const jobId = `${videoId}_${Date.now()}`;

    const job: TranscriptionJob = {
      id: jobId,
      videoId,
      audioPath,
      options,
      priority,
      createdAt: new Date(),
      retryCount: 0,
      maxRetries,
    };

    this.jobs.set(jobId, job);

    log('transcription_job_added', {
      jobId,
      videoId,
      priority,
      maxRetries,
      queueSize: this.jobs.size,
    });

    // Start processing if not already running
    this.processQueue();

    return jobId;
  }

  /**
   * Get job status
   */
  getJob(jobId: string): TranscriptionJob | undefined {
    return this.jobs.get(jobId);
  }

  /**
   * Get all jobs for a video
   */
  getJobsForVideo(videoId: string): TranscriptionJob[] {
    return Array.from(this.jobs.values()).filter(
      (job) => job.videoId === videoId
    );
  }

  /**
   * Cancel a job
   */
  cancelJob(jobId: string): boolean {
    const job = this.jobs.get(jobId);
    if (!job) {
      return false;
    }

    if (this.processingJobs.has(jobId)) {
      log(
        'transcription_job_cancel_processing',
        {
          jobId,
          videoId: job.videoId,
        },
        'warn'
      );
      return false; // Cannot cancel processing jobs
    }

    this.jobs.delete(jobId);

    log('transcription_job_cancelled', {
      jobId,
      videoId: job.videoId,
    });

    return true;
  }

  /**
   * Get queue statistics
   */
  getStats(): QueueStats {
    const jobs = Array.from(this.jobs.values());

    const pending = jobs.filter((job) => !job.startedAt).length;
    const processing = this.processingJobs.size;
    const completed = jobs.filter(
      (job) => job.completedAt && job.result?.success
    ).length;
    const failed = jobs.filter(
      (job) => job.completedAt && !job.result?.success
    ).length;

    const completedJobs = jobs.filter((job) => job.result?.processingTimeMs);
    const totalProcessingTimeMs = completedJobs.reduce(
      (sum, job) => sum + (job.result?.processingTimeMs || 0),
      0
    );
    const averageProcessingTimeMs =
      completedJobs.length > 0
        ? totalProcessingTimeMs / completedJobs.length
        : 0;

    return {
      pending,
      processing,
      completed,
      failed,
      totalProcessingTimeMs,
      averageProcessingTimeMs,
    };
  }

  /**
   * Clean up old completed jobs
   */
  cleanupOldJobs(maxAgeHours = DEFAULT_CLEANUP_AGE_HOURS): void {
    const cutoffTime = Date.now() - maxAgeHours * MILLISECONDS_PER_HOUR;
    let cleanedCount = 0;

    for (const [jobId, job] of this.jobs.entries()) {
      if (job.completedAt && job.completedAt.getTime() < cutoffTime) {
        this.jobs.delete(jobId);
        cleanedCount++;
      }
    }

    if (cleanedCount > 0) {
      log('transcription_jobs_cleaned', {
        cleanedCount,
        maxAgeHours,
        remainingJobs: this.jobs.size,
      });
    }
  }

  /**
   * Process the queue
   */
  private processQueue(): void {
    if (this.isProcessing) {
      return;
    }

    this.isProcessing = true;

    try {
      while (this.processingJobs.size < this.maxConcurrentJobs) {
        const nextJob = this.getNextJob();
        if (!nextJob) {
          break; // No more jobs to process
        }

        // Start processing the job
        this.processJob(nextJob);
      }
    } finally {
      this.isProcessing = false;
    }
  }

  /**
   * Get the next job to process (priority-based)
   */
  private getNextJob(): TranscriptionJob | null {
    const pendingJobs = Array.from(this.jobs.values())
      .filter((job) => !(job.startedAt || this.processingJobs.has(job.id)))
      .sort((a, b) => {
        // Sort by priority first, then by creation time
        const priorityOrder = {
          high: PRIORITY_HIGH,
          medium: PRIORITY_MEDIUM,
          low: PRIORITY_LOW,
        };
        const priorityDiff =
          priorityOrder[a.priority] - priorityOrder[b.priority];
        if (priorityDiff !== 0) {
          return priorityDiff;
        }
        return a.createdAt.getTime() - b.createdAt.getTime();
      });

    return pendingJobs[0] || null;
  }

  /**
   * Process a single job
   */
  private async processJob(job: TranscriptionJob): Promise<void> {
    this.processingJobs.add(job.id);
    job.startedAt = new Date();

    log('transcription_job_started', {
      jobId: job.id,
      videoId: job.videoId,
      priority: job.priority,
      retryCount: job.retryCount,
      audioPath: job.audioPath,
    });

    try {
      // Perform transcription
      const result = await transcribeAudio(
        job.audioPath,
        job.videoId,
        job.options
      );

      job.result = result;
      job.completedAt = new Date();

      if (result.success) {
        log('transcription_job_completed', {
          jobId: job.id,
          videoId: job.videoId,
          language: result.language,
          duration: result.duration,
          textLength: result.text.length,
          processingTimeMs: result.processingTimeMs,
        });
      } else {
        log(
          'transcription_job_failed',
          {
            jobId: job.id,
            videoId: job.videoId,
            error: result.error,
            retryCount: job.retryCount,
          },
          'error'
        );

        // Retry if possible
        if (job.retryCount < job.maxRetries) {
          this.retryJob(job);
          return;
        }
      }

      // Clean up temp files for this video
      await cleanupVideoTempFiles(job.videoId);
    } catch (error) {
      job.error = String(error);
      job.completedAt = new Date();

      log(
        'transcription_job_error',
        {
          jobId: job.id,
          videoId: job.videoId,
          error: String(error),
          retryCount: job.retryCount,
        },
        'error'
      );

      // Retry if possible
      if (job.retryCount < job.maxRetries) {
        this.retryJob(job);
        return;
      }

      // Clean up temp files for this video
      await cleanupVideoTempFiles(job.videoId);
    } finally {
      this.processingJobs.delete(job.id);

      // Continue processing queue
      setTimeout(() => this.processQueue(), PROCESS_QUEUE_DELAY_MS);
    }
  }

  /**
   * Retry a failed job
   */
  private retryJob(job: TranscriptionJob): void {
    job.retryCount++;
    job.startedAt = undefined;
    job.completedAt = undefined;
    job.result = undefined;
    job.error = undefined;

    log('transcription_job_retry', {
      jobId: job.id,
      videoId: job.videoId,
      retryCount: job.retryCount,
      maxRetries: job.maxRetries,
    });

    // Add delay before retry (exponential backoff)
    const delayMs = Math.min(
      MIN_RETRY_DELAY_MS * RETRY_DELAY_BASE ** (job.retryCount - 1),
      MAX_RETRY_DELAY_MS
    );
    setTimeout(() => this.processQueue(), delayMs);
  }

  /**
   * Wait for a job to complete
   */
  async waitForJob(
    jobId: string,
    timeoutMs: number = DEFAULT_WAIT_TIMEOUT_MS
  ): Promise<TranscriptionJob | null> {
    const startTime = Date.now();

    while (Date.now() - startTime < timeoutMs) {
      const job = this.jobs.get(jobId);
      if (!job) {
        return null; // Job not found
      }

      if (job.completedAt) {
        return job; // Job completed
      }

      // Wait a bit before checking again
      await new Promise((resolve) =>
        setTimeout(resolve, CHECK_JOB_INTERVAL_MS)
      );
    }

    log(
      'transcription_job_wait_timeout',
      {
        jobId,
        timeoutMs,
      },
      'warn'
    );

    return null; // Timeout
  }

  /**
   * Get queue status summary
   */
  getQueueStatus(): {
    stats: QueueStats;
    recentJobs: Array<{
      id: string;
      videoId: string;
      status: 'pending' | 'processing' | 'completed' | 'failed';
      createdAt: Date;
      processingTimeMs?: number;
    }>;
  } {
    const stats = this.getStats();
    const jobs = Array.from(this.jobs.values())
      .sort((a, b) => b.createdAt.getTime() - a.createdAt.getTime())
      .slice(0, RECENT_JOBS_LIMIT); // Last 10 jobs

    const recentJobs = jobs.map((job) => {
      let status: 'pending' | 'processing' | 'completed' | 'failed';

      if (this.processingJobs.has(job.id)) {
        status = 'processing';
      } else if (job.completedAt) {
        status = job.result?.success ? 'completed' : 'failed';
      } else {
        status = 'pending';
      }

      return {
        id: job.id,
        videoId: job.videoId,
        status,
        createdAt: job.createdAt,
        processingTimeMs: job.result?.processingTimeMs,
      };
    });

    return { stats, recentJobs };
  }
}

// Global queue instance
export const transcriptionQueue = new TranscriptionQueue(
  DEFAULT_MAX_CONCURRENT_JOBS
);

// Cleanup old jobs every hour
setInterval(() => {
  transcriptionQueue.cleanupOldJobs(DEFAULT_CLEANUP_AGE_HOURS);
}, CLEANUP_INTERVAL_MS);

================
File: src/transcribe/whisper.ts
================
import { spawn } from 'node:child_process';
import { promises as fs } from 'node:fs';
import { log } from '../log.js';
import { cleanupTempFiles, createTempPath } from '../utils/temp-files.js';

export type TranscriptionSegment = {
  start: number;
  end: number;
  text: string;
  confidence?: number;
};

export type TranscriptionResult = {
  text: string;
  segments: TranscriptionSegment[];
  language: string;
  duration: number;
  success: boolean;
  error?: string;
  modelUsed: string;
  processingTimeMs: number;
};

export type WhisperModel =
  | 'tiny'
  | 'base'
  | 'small'
  | 'medium'
  | 'large'
  | 'large-v2'
  | 'large-v3';

export type WhisperOptions = {
  model: WhisperModel;
  language?: string; // Auto-detect if not specified
  task?: 'transcribe' | 'translate'; // Default: transcribe
  temperature?: number; // 0.0 to 1.0, default: 0
  bestOf?: number; // Number of candidates, default: 5
  beamSize?: number; // Beam search size, default: 5
  patience?: number; // Beam search patience, default: 1.0
  lengthPenalty?: number; // Length penalty, default: 1.0
  suppressTokens?: string; // Comma-separated token IDs to suppress
  initialPrompt?: string; // Initial prompt to guide transcription
  conditionOnPreviousText?: boolean; // Default: true
  fp16?: boolean; // Use FP16 precision, default: true
  compressionRatioThreshold?: number; // Default: 2.4
  logprobThreshold?: number; // Default: -1.0
  noSpeechThreshold?: number; // Default: 0.6
  wordTimestamps?: boolean; // Include word-level timestamps
  prepend_punctuations?: string; // Default: "\"'"¬ø([{-"
  append_punctuations?: string; // Default: "\"'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö")]}„ÄÅ"
};

// Constants for file size calculations
const BYTES_PER_KB = 1024;
const BYTES_PER_MB = BYTES_PER_KB * BYTES_PER_KB;
const MS_PER_SECOND = 1000;
const MS_PER_MINUTE = 60 * MS_PER_SECOND;
const DECIMAL_PLACES = 100;

const DEFAULT_WHISPER_OPTIONS: WhisperOptions = {
  model: 'base',
  task: 'transcribe',
  temperature: 0,
  bestOf: 5,
  beamSize: 5,
  patience: 1.0,
  lengthPenalty: 1.0,
  conditionOnPreviousText: true,
  fp16: true,
  compressionRatioThreshold: 2.4,
  logprobThreshold: -1.0,
  noSpeechThreshold: 0.6,
  wordTimestamps: true,
};

/**
 * Transcribe audio file using OpenAI Whisper
 */
export async function transcribeAudio(
  audioPath: string,
  videoId: string,
  options: Partial<WhisperOptions> = {}
): Promise<TranscriptionResult> {
  const startTime = Date.now();
  const opts = { ...DEFAULT_WHISPER_OPTIONS, ...options };

  // Create temp file for output
  const outputPath = createTempPath(videoId, 'json');

  try {
    log('whisper_transcription_start', {
      videoId,
      audioPath,
      outputPath,
      model: opts.model,
      language: opts.language || 'auto-detect',
      task: opts.task,
    });

    // Verify audio file exists
    await fs.access(audioPath);
    const audioStats = await fs.stat(audioPath);

    log('whisper_audio_file_info', {
      videoId,
      audioPath,
      fileSizeBytes: audioStats.size,
      fileSizeMB:
        Math.round((audioStats.size / BYTES_PER_MB) * DECIMAL_PLACES) /
        DECIMAL_PLACES,
    });

    // Build Whisper command
    const whisperArgs = buildWhisperArgs(audioPath, outputPath, opts);

    // Run Whisper transcription
    const transcriptionPromise = new Promise<void>((resolve, reject) => {
      const whisper = spawn('whisper', whisperArgs);
      let stderr = '';
      let stdout = '';

      whisper.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      whisper.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      whisper.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(
            new Error(
              `Whisper failed with code ${code}. stderr: ${stderr}. stdout: ${stdout}`
            )
          );
        }
      });

      whisper.on('error', (error) => {
        reject(new Error(`Failed to spawn whisper: ${error.message}`));
      });
    });

    // Set timeout based on audio file size (roughly 1 minute per MB, minimum 5 minutes)
    const FIVE_MINUTES = 5;
    const FIVE_MINUTES_MS = FIVE_MINUTES * MS_PER_MINUTE;
    const timeoutMs = Math.max(
      FIVE_MINUTES_MS,
      (audioStats.size / BYTES_PER_MB) * MS_PER_MINUTE
    );
    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(
        () =>
          reject(
            new Error(`Whisper transcription timeout after ${timeoutMs}ms`)
          ),
        timeoutMs
      );
    });

    await Promise.race([transcriptionPromise, timeoutPromise]);

    // Read and parse the output JSON
    const outputContent = await fs.readFile(outputPath, 'utf-8');
    const whisperOutput = JSON.parse(outputContent);

    // Extract segments with proper typing
    const segments: TranscriptionSegment[] = (whisperOutput.segments || []).map(
      (seg: {
        start?: number;
        end?: number;
        text?: string;
        avg_logprob?: number;
      }) => ({
        start: seg.start || 0,
        end: seg.end || 0,
        text: (seg.text || '').trim(),
        confidence: seg.avg_logprob ? Math.exp(seg.avg_logprob) : undefined,
      })
    );

    const processingTimeMs = Date.now() - startTime;
    const result: TranscriptionResult = {
      text: whisperOutput.text || '',
      segments,
      language: whisperOutput.language || 'unknown',
      duration: whisperOutput.duration || 0,
      success: true,
      modelUsed: opts.model,
      processingTimeMs,
    };

    log('whisper_transcription_complete', {
      videoId,
      model: opts.model,
      language: result.language,
      duration: result.duration,
      segmentCount: segments.length,
      textLength: result.text.length,
      processingTimeMs,
      processingTimeMinutes:
        Math.round((processingTimeMs / MS_PER_MINUTE) * DECIMAL_PLACES) /
        DECIMAL_PLACES,
    });

    // Clean up temp files
    await cleanupTempFiles([outputPath]);

    return result;
  } catch (error) {
    const processingTimeMs = Date.now() - startTime;

    log(
      'whisper_transcription_error',
      {
        videoId,
        audioPath,
        model: opts.model,
        error: String(error),
        processingTimeMs,
      },
      'error'
    );

    // Clean up temp files
    try {
      await cleanupTempFiles([outputPath]);
    } catch {
      // Ignore cleanup errors
    }

    return {
      text: '',
      segments: [],
      language: 'unknown',
      duration: 0,
      success: false,
      error: String(error),
      modelUsed: opts.model,
      processingTimeMs,
    };
  }
}

// Regex pattern for extracting directory from path
const DIRECTORY_REGEX = /\/[^/]+$/;

/**
 * Build basic Whisper command arguments
 */
function buildBasicWhisperArgs(
  audioPath: string,
  outputPath: string,
  options: WhisperOptions
): string[] {
  return [
    audioPath,
    '--output_format',
    'json',
    '--output_dir',
    outputPath.replace(DIRECTORY_REGEX, ''), // Directory only
    '--model',
    options.model,
    '--task',
    options.task || 'transcribe',
    '--temperature',
    String(options.temperature || 0),
    '--best_of',
    String(options.bestOf || DEFAULT_WHISPER_OPTIONS.bestOf),
    '--beam_size',
    String(options.beamSize || DEFAULT_WHISPER_OPTIONS.beamSize),
    '--patience',
    String(options.patience || DEFAULT_WHISPER_OPTIONS.patience),
    '--length_penalty',
    String(options.lengthPenalty || DEFAULT_WHISPER_OPTIONS.lengthPenalty),
    '--compression_ratio_threshold',
    String(
      options.compressionRatioThreshold ||
        DEFAULT_WHISPER_OPTIONS.compressionRatioThreshold
    ),
    '--logprob_threshold',
    String(
      options.logprobThreshold || DEFAULT_WHISPER_OPTIONS.logprobThreshold
    ),
    '--no_speech_threshold',
    String(
      options.noSpeechThreshold || DEFAULT_WHISPER_OPTIONS.noSpeechThreshold
    ),
  ];
}

/**
 * Add optional parameters to Whisper arguments
 */
function addOptionalWhisperArgs(args: string[], options: WhisperOptions): void {
  if (options.language) {
    args.push('--language', options.language);
  }

  if (options.initialPrompt) {
    args.push('--initial_prompt', options.initialPrompt);
  }

  if (options.suppressTokens) {
    args.push('--suppress_tokens', options.suppressTokens);
  }

  if (options.conditionOnPreviousText === false) {
    args.push('--condition_on_previous_text', 'False');
  }

  if (options.fp16 === false) {
    args.push('--fp16', 'False');
  }

  if (options.wordTimestamps) {
    args.push('--word_timestamps', 'True');
  }

  if (options.prepend_punctuations) {
    args.push('--prepend_punctuations', options.prepend_punctuations);
  }

  if (options.append_punctuations) {
    args.push('--append_punctuations', options.append_punctuations);
  }
}

/**
 * Build Whisper command arguments
 */
function buildWhisperArgs(
  audioPath: string,
  outputPath: string,
  options: WhisperOptions
): string[] {
  const args = buildBasicWhisperArgs(audioPath, outputPath, options);
  addOptionalWhisperArgs(args, options);
  return args;
}

/**
 * Check if Whisper is available and working
 */
export async function checkWhisperAvailability(): Promise<boolean> {
  try {
    const checkPromise = new Promise<boolean>((resolve, reject) => {
      const whisper = spawn('whisper', ['--help']);

      whisper.on('close', (code) => {
        resolve(code === 0);
      });

      whisper.on('error', (error) => {
        reject(error);
      });
    });

    const WHISPER_CHECK_TIMEOUT_SECONDS = 10;
    const WHISPER_CHECK_TIMEOUT_MS =
      WHISPER_CHECK_TIMEOUT_SECONDS * MS_PER_SECOND;
    const timeoutPromise = new Promise<boolean>((resolve) => {
      setTimeout(() => resolve(false), WHISPER_CHECK_TIMEOUT_MS);
    });

    const isAvailable = await Promise.race([checkPromise, timeoutPromise]);

    log('whisper_availability_check', {
      available: isAvailable,
    });

    return isAvailable;
  } catch (error) {
    log(
      'whisper_availability_error',
      {
        error: String(error),
      },
      'error'
    );
    return false;
  }
}

/**
 * Get available Whisper models
 */
export function getAvailableModels(): WhisperModel[] {
  return ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3'];
}

// Duration thresholds for model selection
const SECONDS_PER_MINUTE = 60;
const SECONDS_PER_HOUR = 60 * SECONDS_PER_MINUTE;
const FIVE = 5;
const THIRTY = 30;

const DURATION_THRESHOLDS = {
  FIVE_MINUTES: FIVE * SECONDS_PER_MINUTE,
  THIRTY_MINUTES: THIRTY * SECONDS_PER_MINUTE,
  ONE_HOUR: SECONDS_PER_HOUR,
} as const;

/**
 * Get recommended model based on audio duration and quality requirements
 */
export function getRecommendedModel(
  durationSeconds: number,
  prioritizeSpeed = false
): WhisperModel {
  if (prioritizeSpeed) {
    return durationSeconds > DURATION_THRESHOLDS.ONE_HOUR ? 'tiny' : 'base'; // Use faster models for long content
  }

  // Balance quality and speed based on duration
  if (durationSeconds < DURATION_THRESHOLDS.FIVE_MINUTES) {
    // < 5 minutes
    return 'small';
  }
  if (durationSeconds < DURATION_THRESHOLDS.THIRTY_MINUTES) {
    // < 30 minutes
    return 'base';
  }
  if (durationSeconds < DURATION_THRESHOLDS.ONE_HOUR) {
    // < 1 hour
    return 'base';
  }
  return 'tiny'; // Use fastest model for very long content
}

/**
 * Estimate transcription time based on model and audio duration
 */
export function estimateTranscriptionTime(
  durationSeconds: number,
  model: WhisperModel
): number {
  // Rough estimates based on CPU performance (in seconds)
  const modelMultipliers: Record<WhisperModel, number> = {
    tiny: 0.1,
    base: 0.2,
    small: 0.4,
    medium: 0.8,
    large: 1.2,
    'large-v2': 1.2,
    'large-v3': 1.2,
  };

  return Math.ceil(durationSeconds * modelMultipliers[model]);
}

================
File: src/types/sources.ts
================
// Local database types for worker (self-contained for Docker builds)
export type Json =
  | string
  | number
  | boolean
  | null
  | { [key: string]: Json | undefined }
  | Json[]

export type SourceKind =
  | 'rss'
  | 'podcast'
  | 'youtube_channel'
  | 'youtube_search';

// Essential database table types (subset needed by worker)
export type SourceRow = {
  id: string;
  kind: string;
  name: string | null;
  url: string | null;
  active: boolean;
  domain: string | null;
  authority_score: number | null;
  created_at: string;
  updated_at: string;
  last_checked: string | null;
  last_cursor: Json | null;
  metadata: Json | null;
};

export type RawItemRow = {
  id: string;
  source_id: string;
  external_id: string | null;
  url: string | null;
  title: string | null;
  description: string | null;
  published_at: string | null;
  metadata: Json | null;
  created_at: string;
  updated_at: string;
};

export type ContentRow = {
  id: string;
  raw_item_id: string;
  content_hash: string;
  text: string | null;
  html_url: string | null;
  pdf_url: string | null;
  audio_url: string | null;
  transcript_url: string | null;
  transcript_vtt: string | null;
  duration_seconds: number | null;
  view_count: number | null;
  lang: string | null;
  extracted_at: string | null;
};

export type StoryRow = {
  id: string;
  content_id: string;
  canonical_url: string | null;
  kind: string | null;
  title: string | null;
  primary_url: string | null;
  published_at: string | null;
  cluster_key: string | null;
  created_at: string;
};

export type SourceBase = {
  id: string;
  kind: SourceKind | string;
  name?: string | null;
  url?: string | null;
  metadata?: Record<string, unknown> | null;
};

// Legacy compatibility - gradually migrate to SourceRow
export type SourceRowLegacy = SourceBase;

================
File: src/utils/http.ts
================
export async function fetchWithTimeout(
  url: string,
  init: RequestInit = {},
  timeoutMs = 15_000
): Promise<Response> {
  const ac = new AbortController();
  const timer = setTimeout(() => ac.abort(), timeoutMs);
  try {
    const res = await fetch(url, { ...init, signal: ac.signal });
    return res;
  } finally {
    clearTimeout(timer);
  }
}

================
File: src/utils/quota-tracker.ts
================
import { log } from '../log.js';

// Constants for quota allocation percentages
const CHANNEL_INGESTION_ALLOCATION = 0.7;
const SEARCH_QUERIES_ALLOCATION = 0.2;
const VIDEO_DETAILS_ALLOCATION = 0.05;

// Constants for calculations
const PERCENTAGE_MULTIPLIER = 100;
const MILLISECONDS_PER_SECOND = 1000;
const SECONDS_PER_MINUTE = 60;
const MINUTES_PER_HOUR = 60;
const MILLISECONDS_PER_HOUR =
  MILLISECONDS_PER_SECOND * SECONDS_PER_MINUTE * MINUTES_PER_HOUR;
const DECIMAL_PRECISION = 10;

export type QuotaStatus = {
  used: number;
  remaining: number;
  limit: number;
  resetAt: string;
  canProceed: boolean;
};

export type QuotaUsage = {
  operation: string;
  units: number;
  timestamp: string;
  videoId?: string;
  channelId?: string;
};

/**
 * YouTube API Quota Management Utility
 *
 * Tracks daily quota usage and provides methods to check availability
 * and reserve quota for operations.
 */
export class QuotaTracker {
  private readonly dailyLimit: number;
  private readonly reserveBuffer: number;
  private readonly resetHour: number;
  private usageLog: QuotaUsage[] = [];

  constructor() {
    this.dailyLimit = Number.parseInt(
      process.env.YOUTUBE_QUOTA_LIMIT || '10000',
      10
    );
    this.reserveBuffer = Number.parseInt(
      process.env.YOUTUBE_RATE_LIMIT_BUFFER || '500',
      10
    );
    this.resetHour = Number.parseInt(
      process.env.YOUTUBE_QUOTA_RESET_HOUR || '0',
      10
    );
  }

  /**
   * Check current quota status
   */
  checkQuotaStatus(currentUsage = 0): QuotaStatus {
    const now = new Date();
    const resetAt = this.getNextResetTime(now);
    const hasReset = this.hasQuotaReset(now);

    // Reset usage if we've passed the reset time
    const used = hasReset ? currentUsage : this.getTodayUsage();
    const remaining = this.dailyLimit - used;
    const canProceed = remaining > this.reserveBuffer;

    const status: QuotaStatus = {
      used,
      remaining,
      limit: this.dailyLimit,
      resetAt: resetAt.toISOString(),
      canProceed,
    };

    log('quota_status_check', {
      ...status,
      reserveBuffer: this.reserveBuffer,
      hasReset,
    });

    return status;
  }

  /**
   * Reserve quota for an operation
   */
  reserveQuota(
    operation: string,
    units: number,
    metadata?: { videoId?: string; channelId?: string }
  ): boolean {
    const status = this.checkQuotaStatus();

    if (status.remaining < units + this.reserveBuffer) {
      log(
        'quota_reservation_failed',
        {
          operation,
          requestedUnits: units,
          remaining: status.remaining,
          reserveBuffer: this.reserveBuffer,
        },
        'warn'
      );
      return false;
    }

    // Log the reservation (not actual usage yet)
    log('quota_reserved', {
      operation,
      units,
      remaining: status.remaining - units,
      ...metadata,
    });

    return true;
  }

  /**
   * Consume quota after successful operation
   */
  consumeQuota(
    operation: string,
    units: number,
    metadata?: { videoId?: string; channelId?: string }
  ): void {
    const usage: QuotaUsage = {
      operation,
      units,
      timestamp: new Date().toISOString(),
      ...metadata,
    };

    this.usageLog.push(usage);

    // Keep only today's usage
    this.cleanupOldUsage();

    log('quota_consumed', {
      operation,
      units,
      totalUsedToday: this.getTodayUsage(),
      remaining: this.dailyLimit - this.getTodayUsage(),
      ...metadata,
    });
  }

  /**
   * Get total quota usage for today
   */
  getTodayUsage(): number {
    const today = new Date().toISOString().split('T')[0];

    return this.usageLog
      .filter((usage) => usage.timestamp.startsWith(today))
      .reduce((total, usage) => total + usage.units, 0);
  }

  /**
   * Get quota usage breakdown by operation type
   */
  getUsageBreakdown(): Record<string, { count: number; units: number }> {
    const today = new Date().toISOString().split('T')[0];
    const todayUsage = this.usageLog.filter((usage) =>
      usage.timestamp.startsWith(today)
    );

    const breakdown: Record<string, { count: number; units: number }> = {};

    for (const usage of todayUsage) {
      if (!breakdown[usage.operation]) {
        breakdown[usage.operation] = { count: 0, units: 0 };
      }
      breakdown[usage.operation].count++;
      breakdown[usage.operation].units += usage.units;
    }

    return breakdown;
  }

  /**
   * Check if quota has reset since last check
   */
  private hasQuotaReset(now: Date): boolean {
    const today = now.toISOString().split('T')[0];
    const resetTime = new Date(
      `${today}T${String(this.resetHour).padStart(2, '0')}:00:00.000Z`
    );

    return now >= resetTime;
  }

  /**
   * Get the next quota reset time
   */
  private getNextResetTime(now: Date): Date {
    const today = now.toISOString().split('T')[0];
    const todayReset = new Date(
      `${today}T${String(this.resetHour).padStart(2, '0')}:00:00.000Z`
    );

    if (now >= todayReset) {
      // Next reset is tomorrow
      const tomorrow = new Date(now);
      tomorrow.setDate(tomorrow.getDate() + 1);
      const tomorrowStr = tomorrow.toISOString().split('T')[0];
      return new Date(
        `${tomorrowStr}T${String(this.resetHour).padStart(2, '0')}:00:00.000Z`
      );
    }
    // Next reset is today
    return todayReset;
  }

  /**
   * Remove old usage entries to prevent memory leaks
   */
  private cleanupOldUsage(): void {
    const twoDaysAgo = new Date();
    twoDaysAgo.setDate(twoDaysAgo.getDate() - 2);
    const cutoff = twoDaysAgo.toISOString();

    const originalLength = this.usageLog.length;
    this.usageLog = this.usageLog.filter((usage) => usage.timestamp >= cutoff);

    if (this.usageLog.length < originalLength) {
      log('quota_usage_cleanup', {
        removed: originalLength - this.usageLog.length,
        remaining: this.usageLog.length,
        cutoff,
      });
    }
  }

  /**
   * Get quota allocation recommendations
   */
  getQuotaAllocation(): {
    channelIngestion: number;
    searchQueries: number;
    videoDetails: number;
    reserve: number;
  } {
    const available = this.dailyLimit - this.reserveBuffer;

    return {
      channelIngestion: Math.floor(available * CHANNEL_INGESTION_ALLOCATION), // 70% for channel-based ingestion (efficient)
      searchQueries: Math.floor(available * SEARCH_QUERIES_ALLOCATION), // 20% for search queries (expensive)
      videoDetails: Math.floor(available * VIDEO_DETAILS_ALLOCATION), // 5% for video details
      reserve: this.reserveBuffer, // 5% reserve buffer
    };
  }

  /**
   * Log current quota status and recommendations
   */
  logQuotaReport(): void {
    const status = this.checkQuotaStatus();
    const breakdown = this.getUsageBreakdown();
    const allocation = this.getQuotaAllocation();

    log('quota_daily_report', {
      status,
      breakdown,
      allocation,
      efficiency: {
        utilizationRate: (status.used / status.limit) * PERCENTAGE_MULTIPLIER,
        remainingHours: this.getHoursUntilReset(),
      },
    });
  }

  /**
   * Get hours until next quota reset
   */
  private getHoursUntilReset(): number {
    const now = new Date();
    const nextReset = this.getNextResetTime(now);
    const diffMs = nextReset.getTime() - now.getTime();
    return (
      Math.round((diffMs / MILLISECONDS_PER_HOUR) * DECIMAL_PRECISION) /
      DECIMAL_PRECISION
    ); // Round to 1 decimal
  }
}

// Export singleton instance
export const quotaTracker = new QuotaTracker();

================
File: src/utils/retry.ts
================
export type RetryOptions = {
  maxRetries?: number;
  baseDelayMs?: number;
  jitter?: boolean;
  isRetryable?: (error: unknown) => boolean;
};

const SERVER_ERROR_MIN = 500;
const SERVER_ERROR_MAX = 600;

export function isRetryableDefault(error: unknown): boolean {
  const err = error as { code?: string; message?: string; status?: number };
  // Common transient network errors
  if (err?.code === 'ECONNRESET' || err?.code === 'ETIMEDOUT') {
    return true;
  }

  // HTTP 429 and 5xx
  if (typeof err?.status === 'number') {
    if (err.status === 429) {
      return true;
    }
    if (err.status >= SERVER_ERROR_MIN && err.status < SERVER_ERROR_MAX) {
      return true;
    }
  }

  // Generic hints in error message
  const msg = String(err?.message || '');
  if (msg.includes('rateLimitExceeded')) {
    return true;
  }

  // Known non-retryable hints
  if (msg.includes('quotaExceeded')) {
    return false;
  }

  return false;
}

export async function withRetry<T>(
  operation: () => Promise<T>,
  opts: RetryOptions = {}
): Promise<T> {
  const {
    maxRetries = 3,
    baseDelayMs = 1000,
    jitter = true,
    isRetryable = isRetryableDefault,
  } = opts;

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      const last = attempt === maxRetries;
      if (!isRetryable(error) || last) {
        throw error;
      }

      const backoff = baseDelayMs * 2 ** (attempt - 1);
      const delay = jitter
        ? Math.round(backoff * (0.5 + Math.random()))
        : backoff;
      await new Promise((r) => setTimeout(r, delay));
    }
  }

  // Unreachable due to early returns/throws
  // but required for type completeness
  throw new Error('Max retries exceeded');
}

================
File: src/utils/temp-files.ts
================
import { promises as fs } from 'node:fs';
import { dirname, join } from 'node:path';
import { log } from '../log.js';

const TEMP_BASE_DIR = '/tmp/youtube-processing';

// Constants for byte conversions
const BYTES_PER_KB = 1024;
const BYTES_PER_MB = BYTES_PER_KB * BYTES_PER_KB;
const ROUNDING_FACTOR = 100;

// Time constants
const SECONDS_PER_MINUTE = 60;
const MINUTES_PER_HOUR = 60;
const MILLIS_PER_SECOND = 1000;
const MILLIS_PER_HOUR =
  SECONDS_PER_MINUTE * MINUTES_PER_HOUR * MILLIS_PER_SECOND;
const DEFAULT_MAX_AGE_HOURS = 24;

export type TempFileInfo = {
  path: string;
  videoId: string;
  extension: string;
  createdAt: Date;
  sizeBytes?: number;
};

/**
 * Create a temporary file path for YouTube processing
 */
export function createTempPath(videoId: string, extension: string): string {
  // Sanitize videoId to prevent path traversal
  const sanitizedVideoId = videoId.replace(/[^a-zA-Z0-9_-]/g, '_');
  const filename = `${sanitizedVideoId}.${extension}`;
  return join(TEMP_BASE_DIR, filename);
}

/**
 * Ensure temp directory exists with proper permissions
 */
export async function ensureTempDirectory(): Promise<void> {
  try {
    await fs.mkdir(TEMP_BASE_DIR, { recursive: true, mode: 0o755 });

    log('temp_directory_created', {
      path: TEMP_BASE_DIR,
      permissions: '755',
    });
  } catch (error) {
    log(
      'temp_directory_creation_error',
      {
        path: TEMP_BASE_DIR,
        error: String(error),
      },
      'error'
    );
    throw error;
  }
}

/**
 * Clean up temporary files
 */
export async function cleanupTempFiles(filePaths: string[]): Promise<void> {
  const results = {
    cleaned: 0,
    errors: 0,
    totalSizeFreed: 0,
  };

  for (const filePath of filePaths) {
    try {
      // Get file size before deletion
      const stats = await fs.stat(filePath);
      results.totalSizeFreed += stats.size;

      // Delete the file
      await fs.unlink(filePath);
      results.cleaned++;

      log('temp_file_cleaned', {
        path: filePath,
        sizeBytes: stats.size,
        sizeMB:
          Math.round((stats.size / BYTES_PER_MB) * ROUNDING_FACTOR) /
          ROUNDING_FACTOR,
      });
    } catch (error) {
      results.errors++;

      log(
        'temp_file_cleanup_error',
        {
          path: filePath,
          error: String(error),
        },
        'warn'
      );
    }
  }

  log('temp_files_cleanup_complete', {
    filesRequested: filePaths.length,
    filesCleaned: results.cleaned,
    errors: results.errors,
    totalSizeFreedMB:
      Math.round((results.totalSizeFreed / BYTES_PER_MB) * ROUNDING_FACTOR) /
      ROUNDING_FACTOR,
  });
}

/**
 * Clean up all temporary files for a specific video
 */
export async function cleanupVideoTempFiles(videoId: string): Promise<void> {
  try {
    const sanitizedVideoId = videoId.replace(/[^a-zA-Z0-9_-]/g, '_');
    const files = await fs.readdir(TEMP_BASE_DIR);

    const videoFiles = files
      .filter((file) => file.startsWith(sanitizedVideoId))
      .map((file) => join(TEMP_BASE_DIR, file));

    if (videoFiles.length > 0) {
      await cleanupTempFiles(videoFiles);
    }
  } catch (error) {
    log(
      'video_temp_cleanup_error',
      {
        videoId,
        error: String(error),
      },
      'warn'
    );
  }
}

/**
 * Clean up old temporary files (older than specified hours)
 */
export async function cleanupOldTempFiles(
  maxAgeHours = DEFAULT_MAX_AGE_HOURS
): Promise<void> {
  try {
    const files = await fs.readdir(TEMP_BASE_DIR);
    const cutoffTime = Date.now() - maxAgeHours * MILLIS_PER_HOUR;
    const oldFiles: string[] = [];

    for (const file of files) {
      const filePath = join(TEMP_BASE_DIR, file);

      try {
        const stats = await fs.stat(filePath);

        if (stats.mtime.getTime() < cutoffTime) {
          oldFiles.push(filePath);
        }
      } catch {
        // Ignore stat errors for individual files
      }
    }

    if (oldFiles.length > 0) {
      log('cleaning_old_temp_files', {
        count: oldFiles.length,
        maxAgeHours,
      });

      await cleanupTempFiles(oldFiles);
    }
  } catch (error) {
    log(
      'old_temp_cleanup_error',
      {
        maxAgeHours,
        error: String(error),
      },
      'error'
    );
  }
}

/**
 * Get information about all temporary files
 */
export async function getTempFileInfo(): Promise<TempFileInfo[]> {
  try {
    const files = await fs.readdir(TEMP_BASE_DIR);
    const fileInfos: TempFileInfo[] = [];

    for (const file of files) {
      const filePath = join(TEMP_BASE_DIR, file);

      try {
        const stats = await fs.stat(filePath);
        const parts = file.split('.');
        const extension = parts.pop() || '';
        const videoId = parts.join('.');

        fileInfos.push({
          path: filePath,
          videoId,
          extension,
          createdAt: stats.birthtime,
          sizeBytes: stats.size,
        });
      } catch {
        // Ignore stat errors for individual files
      }
    }

    return fileInfos.sort(
      (a, b) => b.createdAt.getTime() - a.createdAt.getTime()
    );
  } catch (error) {
    log(
      'temp_file_info_error',
      {
        error: String(error),
      },
      'error'
    );
    return [];
  }
}

/**
 * Get total disk usage of temporary files
 */
export async function getTempDiskUsage(): Promise<{
  totalFiles: number;
  totalSizeBytes: number;
  totalSizeMB: number;
  oldestFile?: Date;
  newestFile?: Date;
}> {
  try {
    const fileInfos = await getTempFileInfo();

    const totalSizeBytes = fileInfos.reduce(
      (sum, file) => sum + (file.sizeBytes || 0),
      0
    );
    const dates = fileInfos.map((f) => f.createdAt).filter(Boolean);

    return {
      totalFiles: fileInfos.length,
      totalSizeBytes,
      totalSizeMB:
        Math.round((totalSizeBytes / BYTES_PER_MB) * ROUNDING_FACTOR) /
        ROUNDING_FACTOR,
      oldestFile:
        dates.length > 0
          ? new Date(Math.min(...dates.map((d) => d.getTime())))
          : undefined,
      newestFile:
        dates.length > 0
          ? new Date(Math.max(...dates.map((d) => d.getTime())))
          : undefined,
    };
  } catch (error) {
    log(
      'temp_disk_usage_error',
      {
        error: String(error),
      },
      'error'
    );

    return {
      totalFiles: 0,
      totalSizeBytes: 0,
      totalSizeMB: 0,
    };
  }
}

/**
 * Check if temp directory has enough space for a file
 */
export async function checkTempDiskSpace(
  requiredBytes: number
): Promise<boolean> {
  try {
    const stats = await fs.statfs(TEMP_BASE_DIR);
    const availableBytes = stats.bavail * stats.bsize;

    const hasSpace = availableBytes > requiredBytes;

    log('temp_disk_space_check', {
      requiredMB:
        Math.round((requiredBytes / BYTES_PER_MB) * ROUNDING_FACTOR) /
        ROUNDING_FACTOR,
      availableMB:
        Math.round((availableBytes / BYTES_PER_MB) * ROUNDING_FACTOR) /
        ROUNDING_FACTOR,
      hasSpace,
    });

    return hasSpace;
  } catch (error) {
    log(
      'temp_disk_space_check_error',
      {
        error: String(error),
      },
      'warn'
    );

    // Assume we have space if we can't check
    return true;
  }
}

/**
 * Create a temp file with content
 */
export async function createTempFile(
  videoId: string,
  extension: string,
  content: string | Buffer
): Promise<string> {
  const filePath = createTempPath(videoId, extension);

  try {
    // Ensure directory exists
    await fs.mkdir(dirname(filePath), { recursive: true });

    // Write content
    await fs.writeFile(filePath, content);

    const stats = await fs.stat(filePath);

    log('temp_file_created', {
      path: filePath,
      videoId,
      extension,
      sizeBytes: stats.size,
    });

    return filePath;
  } catch (error) {
    log(
      'temp_file_creation_error',
      {
        path: filePath,
        videoId,
        extension,
        error: String(error),
      },
      'error'
    );
    throw error;
  }
}

================
File: src/db.ts
================
import { Pool } from 'pg';
import { log } from './log.js';

const cnn = process.env.DATABASE_URL || '';
// Use SSL only for non-local connections. Supabase local Postgres on 127.0.0.1:54322 does not support TLS.
const useSsl = !(
  cnn.includes('127.0.0.1') ||
  cnn.includes('localhost') ||
  cnn.includes('host.docker.internal')
);

const pool = new Pool({
  connectionString: cnn,
  ssl: useSsl ? { rejectUnauthorized: false } : false,
  keepAlive: true,
  max: 3,
  idleTimeoutMillis: 30_000, // Increased from 10s to 30s
  query_timeout: 30_000,
  statement_timeout: 30_000,
});

// Prevent process crashes from idle client errors (e.g., Supabase pooler resets)
// See: https://node-postgres.com/features/connecting#idle-clients
pool.on('error', (err) => {
  log('pg_pool_error', { err: String(err) }, 'warn');
});

export type SourceRow = {
  id: string;
  kind: string;
  url: string | null;
  name: string | null;
  domain?: string | null;
};

export async function getRssSources(): Promise<SourceRow[]> {
  const { rows } = await pool.query(
    `select id, kind, url, name
		 from public.sources
		 where kind = 'rss'
		   and url is not null
		   and coalesce(active, true)`
  );
  return rows;
}

export type YouTubeSourceRow = {
  id: string;
  kind: string;
  url: string | null;
  name: string | null;
  metadata: Record<string, unknown>;
};

export type SourceRowFull = {
  id: string;
  kind: string;
  url: string | null;
  name: string | null;
  domain?: string | null;
  metadata: Record<string, unknown>;
};

export async function getYouTubeSources(): Promise<YouTubeSourceRow[]> {
  const { rows } = await pool.query(
    `select id, kind, url, name, metadata from public.sources
     where kind in ('youtube_channel', 'youtube_search')
       and (url is not null or kind = 'youtube_search')
       and coalesce(active, true)`
  );
  return rows;
}

export async function getSourceById(
  sourceId: string
): Promise<SourceRowFull | null> {
  const { rows } = await pool.query(
    'select id, kind, url, name, metadata, domain from public.sources where id = $1',
    [sourceId]
  );
  return rows[0] ?? null;
}

export async function getOrCreateManualSource(
  kind: string,
  domain: string | null,
  name?: string | null,
  url?: string | null
): Promise<string> {
  // Try find an existing manual source by kind+domain+url (nullable)
  const { rows: found } = await pool.query(
    `select id from public.sources where kind = $1 and coalesce(domain,'') = coalesce($2,'') and coalesce(url,'') = coalesce($3,'') limit 1`,
    [kind, domain ?? null, url ?? null]
  );
  if (found[0]?.id) {
    return found[0].id as string;
  }
  const { rows } = await pool.query(
    `insert into public.sources (kind, name, url, domain, active, created_at, updated_at)
     values ($1,$2,$3,$4,true, now(), now())
     returning id`,
    [kind, name ?? null, url ?? null, domain ?? null]
  );
  return rows[0].id as string;
}

export async function updateSourceMetadata(
  sourceId: string,
  patch: Record<string, unknown>
): Promise<void> {
  // Merge JSONB metadata with a shallow patch
  await pool.query(
    `update public.sources
       set metadata = coalesce(metadata, '{}'::jsonb) || $2::jsonb,
           updated_at = now()
     where id = $1`,
    [sourceId, JSON.stringify(patch)]
  );
}

export async function upsertRawItem(params: {
  source_id: string;
  external_id: string;
  url: string;
  title?: string | null;
  kind?: string | null;
  metadata?: Record<string, unknown>;
}): Promise<string | null> {
  const { source_id, external_id, url, title, kind, metadata } = params;
  const { rows } = await pool.query(
    `insert into public.raw_items (source_id, external_id, url, title, kind, metadata)
     values ($1,$2,$3,$4,$5,$6)
     on conflict (source_id, external_id) do nothing
     returning id`,
    [
      source_id,
      external_id,
      url,
      title ?? null,
      kind ?? 'article',
      metadata ?? null,
    ]
  );
  return rows[0]?.id ?? null;
}

export async function findRawItemsByIds(
  ids: string[]
): Promise<Array<{ id: string; url: string; title: string | null }>> {
  if (!ids.length) {
    return [];
  }
  const { rows } = await pool.query(
    'select id, url, title from public.raw_items where id = any($1::uuid[])',
    [ids]
  );
  return rows;
}

export async function insertContents(params: {
  raw_item_id: string;
  text: string;
  html_url?: string | null;
  lang?: string | null;
  content_hash: string;
  transcript_url?: string | null;
  transcript_vtt?: string | null;
}): Promise<string> {
  const {
    raw_item_id,
    text,
    html_url,
    lang,
    content_hash,
    transcript_url,
    transcript_vtt,
  } = params;
  const { rows } = await pool.query(
    `insert into public.contents (raw_item_id, text, html_url, lang, content_hash, transcript_url, transcript_vtt)
     values ($1,$2,$3,$4,$5,$6,$7)
     returning id`,
    [
      raw_item_id,
      text,
      html_url ?? null,
      lang ?? null,
      content_hash,
      transcript_url ?? null,
      transcript_vtt ?? null,
    ]
  );
  return rows[0].id as string;
}

export async function findStoryIdByContentHash(
  hash: string
): Promise<string | null> {
  const { rows } = await pool.query(
    `select s.id
       from public.stories s
       join public.contents c on c.id = s.content_id
      where c.content_hash = $1
      limit 1`,
    [hash]
  );
  return rows[0]?.id ?? null;
}

export async function insertStory(params: {
  content_id: string;
  title?: string | null;
  canonical_url?: string | null;
  primary_url?: string | null;
  kind?: string | null;
  published_at?: string | null;
}): Promise<string> {
  const { content_id, title, canonical_url, primary_url, kind, published_at } =
    params;
  const { rows } = await pool.query(
    `insert into public.stories (content_id, title, canonical_url, primary_url, kind, published_at)
     values ($1,$2,$3,$4,$5,$6)
     returning id`,
    [
      content_id,
      title ?? null,
      canonical_url ?? null,
      primary_url ?? null,
      kind ?? 'article',
      published_at ?? null,
    ]
  );
  return rows[0].id as string;
}

export async function getStoryWithContent(storyId: string): Promise<{
  id: string;
  title: string | null;
  canonical_url: string | null;
  text: string;
  content_hash: string;
} | null> {
  const { rows } = await pool.query(
    `select s.id, s.title, s.canonical_url, c.text, c.content_hash
     from public.stories s
     join public.contents c on c.id = s.content_id
     where s.id = $1`,
    [storyId]
  );
  return rows[0] ?? null;
}

export async function upsertStoryOverlay(params: {
  story_id: string;
  why_it_matters?: string | null;
  chili?: number | null;
  confidence?: number | null;
  citations?: Record<string, unknown>;
  model_version?: string | null;
}): Promise<void> {
  const {
    story_id,
    why_it_matters,
    chili,
    confidence,
    citations,
    model_version,
  } = params;
  await pool.query(
    `insert into public.story_overlays (story_id, why_it_matters, chili, confidence, citations, model_version, analyzed_at)
     values ($1, $2, $3, $4, $5, $6, now())
     on conflict (story_id) do update set
       why_it_matters = excluded.why_it_matters,
       chili = excluded.chili,
       confidence = excluded.confidence,
       citations = excluded.citations,
       model_version = excluded.model_version,
       analyzed_at = excluded.analyzed_at`,
    [
      story_id,
      why_it_matters ?? null,
      chili ?? null,
      confidence ?? null,
      citations ?? null,
      model_version ?? null,
    ]
  );
}

export async function upsertStoryEmbedding(params: {
  story_id: string;
  embedding: number[];
  model_version?: string | null;
}): Promise<void> {
  const { story_id, embedding, model_version } = params;
  await pool.query(
    `insert into public.story_embeddings (story_id, embedding, model_version)
     values ($1, $2, $3)
     on conflict (story_id) do update set
       embedding = excluded.embedding,
       model_version = excluded.model_version`,
    [story_id, JSON.stringify(embedding), model_version ?? null]
  );
}

export default pool;

// Admin instrumentation helpers
export async function upsertPlatformQuota(
  provider: string,
  snapshot: {
    limit?: number | null;
    used?: number | null;
    remaining?: number | null;
    reset_at?: string | null;
  }
) {
  await pool.query(
    `insert into public.platform_quota (provider, quota_limit, used, remaining, reset_at, updated_at)
     values ($1,$2,$3,$4,$5, now())
     on conflict (provider) do update set
       quota_limit = excluded.quota_limit,
       used = excluded.used,
       remaining = excluded.remaining,
       reset_at = excluded.reset_at,
       updated_at = excluded.updated_at`,
    [
      provider,
      snapshot.limit ?? null,
      snapshot.used ?? null,
      snapshot.remaining ?? null,
      snapshot.reset_at ?? null,
    ]
  );
}

export async function upsertSourceHealth(
  sourceId: string,
  status: 'ok' | 'warn' | 'error',
  message?: string | null
) {
  await pool.query(
    `insert into public.source_health (source_id, status, last_success_at, last_error_at, message, updated_at)
     values ($1, $2::public.health_status, case when $2::public.health_status = 'ok' then now() else null end, case when $2::public.health_status = 'error' then now() else null end, $3, now())
     on conflict (source_id) do update set
       status = excluded.status,
       last_success_at = coalesce(excluded.last_success_at, public.source_health.last_success_at),
       last_error_at = coalesce(excluded.last_error_at, public.source_health.last_error_at),
       message = excluded.message,
       updated_at = excluded.updated_at`,
    [sourceId, status, message ?? null]
  );
}

export async function upsertJobMetrics(
  rows: Array<{ name: string; state: string; count: number }>
) {
  if (!rows.length) {
    return;
  }
  const PARAMS_PER_ROW = 3;
  const values = rows
    .map(
      (_, i) =>
        `($${i * PARAMS_PER_ROW + 1}, $${i * PARAMS_PER_ROW + 2}, $${i * PARAMS_PER_ROW + PARAMS_PER_ROW}, now())`
    )
    .join(',');
  const params: Array<string | number> = [];
  for (const r of rows) {
    params.push(r.name, r.state, r.count);
  }
  await pool.query(
    `insert into public.job_metrics (name, state, count, updated_at)
     values ${values}
     on conflict (name,state) do update set count = excluded.count, updated_at = excluded.updated_at`,
    params
  );
}

================
File: src/log.ts
================
export type LogLevel = 'debug' | 'info' | 'warn' | 'error';

export function log(
  evt: string,
  extra?: Record<string, unknown>,
  lvl: LogLevel = 'info'
) {
  const entry = {
    ts: new Date().toISOString(),
    lvl,
    evt,
    msg: evt, // backward-compatible key used in existing filters
    ...extra,
  };

  // Output to console with proper formatting
  if (lvl === 'error') {
    console.error(JSON.stringify(entry));
  } else if (lvl === 'warn') {
    console.warn(JSON.stringify(entry));
  } else {
    console.log(JSON.stringify(entry));
  }
}

================
File: src/util.ts
================
import { createHash } from 'node:crypto';

export function canonicalizeUrl(input: string): string {
  try {
    const u = new URL(input);
    // drop common tracking params
    const drop = new Set([
      'utm_source',
      'utm_medium',
      'utm_campaign',
      'utm_term',
      'utm_content',
      'utm_id',
      'gclid',
      'fbclid',
    ]);
    const kept = new URLSearchParams();
    for (const [k, v] of u.searchParams.entries()) {
      if (!drop.has(k.toLowerCase())) {
        kept.append(k, v);
      }
    }
    // sort params for stability
    const sorted = new URLSearchParams(
      Array.from(kept.entries()).sort(([a], [b]) => a.localeCompare(b))
    );
    u.search = sorted.toString();
    u.hash = '';
    return u.toString();
  } catch {
    return input;
  }
}

export function hashText(text: string): string {
  const normalized = text.replace(/\s+/g, ' ').trim();
  return createHash('sha256').update(normalized).digest('hex');
}

================
File: src/worker-old.ts
================
import 'dotenv/config';
import type { ConnectionOptions as TlsConnectionOptions } from 'node:tls';
import express from 'express';
import PgBoss from 'pg-boss';
// YouTube ingestion uses tasks directly below
import { log } from './log.js';
import { analyzeStory } from './tasks/analyze-story.js';
import { extractArticle } from './tasks/extract-article.js';
import { extractYouTubeContent } from './tasks/extract-youtube-content.js';

const DATABASE_URL: string = process.env.DATABASE_URL ?? '';
const BOSS_SCHEMA = process.env.BOSS_SCHEMA || 'pgboss';
const CRON_TZ = process.env.BOSS_CRON_TZ || 'UTC';
const BOSS_MIGRATE = process.env.BOSS_MIGRATE !== 'false';
// Numeric tuning knobs
const WORKER_MAX_CLIENTS = 2;
const HEARTBEAT_BATCH = 10;
const INGEST_PULL_BATCH = 5;
const CONTENT_FETCH_BATCH = 5;
const YT_FETCH_BATCH = 2;
const DEFAULT_PREVIEW_LIMIT = 10;
const MAX_PREVIEW_LIMIT = 50;
const RETRY_DELAY_MS = 10_000;
const METRICS_INTERVAL_MS = 3000;
const DEFAULT_PORT = 8080;
// HTTP status codes
const HTTP_OK = 200;
const HTTP_BAD_REQUEST = 400;
const HTTP_NOT_FOUND = 404;
const HTTP_SERVICE_UNAVAILABLE = 503;
// Use SSL only for non-local connections (local Supabase Postgres does not support TLS)
const USE_SSL = !(
  DATABASE_URL.includes('127.0.0.1') ||
  DATABASE_URL.includes('localhost') ||
  DATABASE_URL.includes('host.docker.internal')
);

if (!DATABASE_URL) {
  log('missing_env', { key: 'DATABASE_URL' }, 'error');
  process.exit(1);
}

// logging provided by ./log

let bossRef: PgBoss | null = null;

type FetchContentJobData = { rawItemIds: string[] };
type AnalyzeJobData = { storyId?: string };

// Discriminated results for one-off ingest endpoint
type YouTubeIngestResult =
  | { url: string; ok: true; raw_item_id: string; type: 'youtube' }
  | { url: string; ok: false; error: string; type: 'youtube' };

type ArticleIngestResult =
  | { url: string; ok: true; raw_item_id: string; type: 'article' }
  | { url: string; ok: false; error: string; type: 'article' };

// Helper: handle one ingest:pull job (reduces route complexity)
async function processIngestPullJob(
  boss: PgBoss,
  job: { id: string; data?: unknown }
) {
  const { source } = (job.data || {}) as { source?: string };
  log('ingest_pull_start', { jobId: job.id, source });
  if (source === 'rss') {
    await handleRssIngest(boss);
  }
  if (source === 'youtube') {
    await handleYouTubeIngest(boss);
  }
  await boss.complete('ingest:pull', job.id);
  log('ingest_pull_done', { jobId: job.id, source });
}

async function handleRssIngest(boss: PgBoss) {
  const { getRssSources } = await import('./db.js');
  const { ingestRssSource } = await import('./tasks/ingest-rss-source.js');
  const sources = await getRssSources();
  for (const src of sources) {
    if (!src.url) {
      continue;
    }
    await ingestRssSource(boss, { id: src.id, url: src.url });
  }
}

function computeQuotaResetAt(now: Date, resetHour: number): Date {
  const resetAt = new Date(
    now.getFullYear(),
    now.getMonth(),
    now.getDate(),
    resetHour,
    0,
    0,
    0
  );
  if (resetAt.getTime() < now.getTime()) {
    resetAt.setDate(resetAt.getDate() + 1);
  }
  return resetAt;
}

async function handleYouTubeIngest(boss: PgBoss) {
  if (!process.env.YOUTUBE_API_KEY) {
    log('ingest_youtube_skipped', { reason: 'missing_api_key' }, 'warn');
    return;
  }
  const { getYouTubeSources, upsertPlatformQuota } = await import('./db.js');
  const { ingestYouTubeSource } = await import(
    './tasks/ingest-youtube-source.js'
  );
  const { quotaTracker } = await import('./utils/quota-tracker.js');
  const sources = await getYouTubeSources();
  log('ingest_youtube_start', {
    comp: 'ingest',
    sourceCount: sources.length,
    quotaStatus: quotaTracker.checkQuotaStatus(),
  });
  for (const src of sources) {
    await ingestYouTubeSource(boss, src);
  }
  const status = quotaTracker.checkQuotaStatus();
  log('ingest_youtube_complete', {
    comp: 'ingest',
    sourcesProcessed: sources.length,
    quotaUsed: status.used,
    quotaRemaining: status.remaining,
  });
  try {
    const resetHour = Number(process.env.YOUTUBE_QUOTA_RESET_HOUR || '0') || 0;
    const now = new Date();
    const resetAt = computeQuotaResetAt(now, resetHour);
    await upsertPlatformQuota('youtube', {
      limit: Number(process.env.YOUTUBE_QUOTA_LIMIT || '10000'),
      used: status.used,
      remaining: status.remaining,
      reset_at: resetAt.toISOString(),
    });
  } catch (e) {
    log('platform_quota_update_error', { error: String(e) }, 'debug');
  }
}

async function initBoss() {
  const boss = new PgBoss({
    connectionString: DATABASE_URL,
    schema: BOSS_SCHEMA,
    ssl: USE_SSL
      ? ({ rejectUnauthorized: false } as TlsConnectionOptions)
      : false,
    application_name: 'zeke-worker',
    max: WORKER_MAX_CLIENTS,
    migrate: BOSS_MIGRATE,
  });
  boss.on('error', (err) => log('boss_error', { err: String(err) }));

  await boss.start();
  log('boss_started', { schema: BOSS_SCHEMA, tz: CRON_TZ });

  await Promise.all([
    boss.createQueue('system:heartbeat'),
    boss.createQueue('ingest:pull'),
    boss.createQueue('ingest:fetch-content'),
    boss.createQueue('ingest:fetch-youtube-content'),
    boss.createQueue('analyze:llm'),
  ]);

  await boss.schedule(
    'system:heartbeat',
    '*/5 * * * *',
    { ping: 'ok' },
    { tz: CRON_TZ }
  );
  await boss.schedule(
    'ingest:pull',
    '*/5 * * * *',
    { source: 'rss' },
    { tz: CRON_TZ }
  );
  await boss.schedule(
    'ingest:pull',
    '*/15 * * * *',
    { source: 'youtube' },
    { tz: CRON_TZ }
  );

  // Kick off an immediate run on startup for faster local feedback
  try {
    await boss.send('ingest:pull', { source: 'rss' });
    await boss.send('ingest:pull', { source: 'youtube' });
    log('ingest_bootstrap_queued', { when: 'startup' });
  } catch (e) {
    log('ingest_bootstrap_error', { err: String(e) }, 'warn');
  }

  await boss.work(
    'system:heartbeat',
    { batchSize: HEARTBEAT_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        log('heartbeat', { jobId: job.id, data: job.data });
        // Explicitly complete to avoid stuck scheduled jobs
        await boss.complete('system:heartbeat', job.id);
      }
    }
  );

  await boss.work(
    'ingest:pull',
    { batchSize: INGEST_PULL_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        await processIngestPullJob(boss, job);
      }
    }
  );

  await boss.work(
    'ingest:fetch-content',
    { batchSize: CONTENT_FETCH_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        log('fetch_content_start', { jobId: job.id });
        try {
          await extractArticle(job.data as FetchContentJobData, boss);
          await boss.complete('ingest:fetch-content', job.id);
        } catch (err) {
          await boss.fail('ingest:fetch-content', job.id, {
            error: String(err),
          });
        }
        log('fetch_content_done', { jobId: job.id });
      }
    }
  );

  await boss.work(
    'ingest:fetch-youtube-content',
    { batchSize: YT_FETCH_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        log('fetch_youtube_content_start', { jobId: job.id });
        try {
          await extractYouTubeContent(
            job.data as unknown as import(
              './tasks/extract-youtube-content.js'
            ).YouTubeExtractionJobData,
            boss
          );
          await boss.complete('ingest:fetch-youtube-content', job.id);
        } catch (err) {
          await boss.fail('ingest:fetch-youtube-content', job.id, {
            error: String(err),
          });
        }
        log('fetch_youtube_content_done', { jobId: job.id });
      }
    }
  );

  await boss.work(
    'analyze:llm',
    { batchSize: CONTENT_FETCH_BATCH },
    async (jobs) => {
      for (const job of jobs) {
        const { storyId } = (job.data as AnalyzeJobData) || {};
        log('analyze_llm_start', { jobId: job.id, storyId });
        try {
          if (!storyId) {
            throw new Error('Missing storyId in job data');
          }
          await analyzeStory(storyId);
          await boss.complete('analyze:llm', job.id);
        } catch (err) {
          await boss.fail('analyze:llm', job.id, { error: String(err) });
        }
        log('analyze_llm_done', { jobId: job.id, storyId });
      }
    }
  );

  bossRef = boss;
}

function registerHealth(app: express.Express) {
  app.get('/healthz', (_req, res) => {
    res.status(HTTP_OK).send('ok');
  });
}

function registerPreviewSource(app: express.Express) {
  // Small helper to isolate kind branching and reduce handler complexity
  async function computePreviewResultForSource(
    src: { id: string; url?: string; kind: string },
    limit: number
  ): Promise<unknown> {
    if (src.kind === 'rss' || src.kind === 'podcast') {
      const { previewRssSourceAction } = await import(
        './tasks/preview-rss-source.js'
      );
      return previewRssSourceAction({ id: src.id, url: src.url ?? '' }, limit);
    }
    if (src.kind === 'youtube_channel' || src.kind === 'youtube_search') {
      const { previewYouTubeSourceAction } = await import(
        './tasks/preview-youtube-source.js'
      );
      return previewYouTubeSourceAction(src, limit);
    }
    throw new Error('unsupported_kind');
  }

  // Extracted handler to reduce route cognitive complexity
  async function handlePreviewSource(
    req: express.Request,
    res: express.Response
  ) {
    try {
      const sourceId = (req.query.sourceId as string) || '';
      const rawLimit = Number.parseInt(
        String(req.query.limit ?? DEFAULT_PREVIEW_LIMIT),
        10
      );
      const limit = Math.min(
        Number.isNaN(rawLimit) ? DEFAULT_PREVIEW_LIMIT : rawLimit,
        MAX_PREVIEW_LIMIT
      );
      if (!sourceId) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'missing sourceId' });
      }
      const pg = (await import('./db.js')).default;
      const { rows } = await pg.query(
        'select id, kind, url, name, domain, metadata from public.sources where id = $1',
        [sourceId]
      );
      const src = rows[0];
      if (!src) {
        return res
          .status(HTTP_NOT_FOUND)
          .json({ ok: false, error: 'not_found' });
      }
      let result: unknown = null;
      try {
        result = await computePreviewResultForSource(src, limit);
      } catch (err) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: String(err) });
      }
      return res.json({ ok: true, ...(result as object) });
    } catch (e: unknown) {
      return res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  }
  app.get('/debug/preview-source', handlePreviewSource);
}

function registerIngestNow(app: express.Express) {
  app.post('/debug/ingest-now', async (_req, res) => {
    try {
      if (!bossRef) {
        throw new Error('boss_not_ready');
      }
      await bossRef.createQueue('ingest:pull');
      const { getRssSources } = await import('./db.js');
      const { ingestRssSource } = await import('./tasks/ingest-rss-source.js');
      const sources = await getRssSources();
      for (const src of sources) {
        if (!src.url) {
          continue;
        }
        await ingestRssSource(bossRef, { id: src.id, url: src.url });
      }
      res.json({ ok: true });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

function registerScheduleRss(app: express.Express) {
  app.post('/debug/schedule-rss', async (_req, res) => {
    try {
      if (!bossRef) {
        throw new Error('boss_not_ready');
      }
      await bossRef.createQueue('ingest:pull');
      await bossRef.schedule(
        'ingest:pull',
        '*/5 * * * *',
        { source: 'rss' },
        { tz: CRON_TZ }
      );
      res.json({ ok: true });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

function registerIngestYouTubeNow(app: express.Express) {
  app.post('/debug/ingest-youtube', async (_req, res) => {
    try {
      if (!bossRef) {
        throw new Error('boss_not_ready');
      }
      await bossRef.createQueue('ingest:pull');
      if (!process.env.YOUTUBE_API_KEY) {
        throw new Error('missing_api_key');
      }
      const { getYouTubeSources, upsertPlatformQuota } = await import(
        './db.js'
      );
      const { ingestYouTubeSource } = await import(
        './tasks/ingest-youtube-source.js'
      );
      const { quotaTracker } = await import('./utils/quota-tracker.js');
      const sources = await getYouTubeSources();
      for (const src of sources) {
        await ingestYouTubeSource(bossRef, src);
      }
      const status = quotaTracker.checkQuotaStatus();
      try {
        const resetHour =
          Number(process.env.YOUTUBE_QUOTA_RESET_HOUR || '0') || 0;
        const now = new Date();
        const resetAt = new Date(
          now.getFullYear(),
          now.getMonth(),
          now.getDate(),
          resetHour,
          0,
          0,
          0
        );
        if (resetAt.getTime() < now.getTime()) {
          resetAt.setDate(resetAt.getDate() + 1);
        }
        await upsertPlatformQuota('youtube', {
          limit: Number(process.env.YOUTUBE_QUOTA_LIMIT || '10000'),
          used: status.used,
          remaining: status.remaining,
          reset_at: resetAt.toISOString(),
        });
      } catch (e) {
        log('platform_quota_update_error', { error: String(e) }, 'debug');
      }
      res.json({ ok: true });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

function registerIngestSource(app: express.Express) {
  app.post('/debug/ingest-source', async (req, res) => {
    try {
      if (!bossRef) {
        throw new Error('boss_not_ready');
      }
      const sourceId =
        (req.query.sourceId as string) || (req.body?.sourceId as string) || '';
      if (!sourceId) {
        throw new Error('missing_sourceId');
      }
      await ingestSourceById(bossRef, sourceId);
      res.json({ ok: true });
    } catch (e: unknown) {
      const msg = String(e);
      if (msg.includes('not_found')) {
        return res
          .status(HTTP_NOT_FOUND)
          .json({ ok: false, error: 'not_found' });
      }
      if (msg.includes('unsupported_kind')) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'unsupported_kind' });
      }
      res.status(HTTP_SERVICE_UNAVAILABLE).json({ ok: false, error: msg });
    }
  });
}

async function ingestSourceById(boss: PgBoss, sourceId: string) {
  const pg = (await import('./db.js')).default;
  const { rows } = await pg.query(
    'select id, kind from public.sources where id = $1',
    [sourceId]
  );
  const row = rows[0];
  if (!row) {
    throw new Error('not_found');
  }
  if (row.kind === 'rss' || row.kind === 'podcast') {
    const { ingestRssSource } = await import('./tasks/ingest-rss-source.js');
    const { getSourceById } = await import('./db.js');
    const srcFull = await getSourceById(sourceId);
    if (!srcFull?.url) {
      throw new Error('no_url_for_source');
    }
    await ingestRssSource(boss, { id: sourceId, url: srcFull.url });
    return;
  }
  if (row.kind === 'youtube_channel' || row.kind === 'youtube_search') {
    const { ingestYouTubeSource } = await import(
      './tasks/ingest-youtube-source.js'
    );
    const { getSourceById } = await import('./db.js');
    const srcFull = await getSourceById(sourceId);
    if (!srcFull) {
      throw new Error('not_found');
    }
    await ingestYouTubeSource(boss, srcFull);
    return;
  }
  throw new Error('unsupported_kind');
}

function registerScheduleYouTube(app: express.Express) {
  app.post('/debug/schedule-youtube', async (_req, res) => {
    try {
      if (!bossRef) {
        throw new Error('boss_not_ready');
      }
      await bossRef.createQueue('ingest:pull');
      await bossRef.schedule(
        'ingest:pull',
        '*/15 * * * *',
        { source: 'youtube' },
        { tz: CRON_TZ }
      );
      res.json({ ok: true });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

function classifyUrl(u: string): 'youtube' | 'article' {
  try {
    const x = new URL(u);
    const h = x.hostname || '';
    if (h.includes('youtube.com') || h.includes('youtu.be')) {
      return 'youtube';
    }
    return 'article';
  } catch {
    return 'article';
  }
}

function extractYouTubeVideoId(u: string): string {
  try {
    const xu = new URL(u);
    if (xu.hostname.includes('youtu.be')) {
      return xu.pathname.replace('/', '');
    }
    if (xu.pathname.startsWith('/shorts/')) {
      return xu.pathname.split('/')[2] || '';
    }
    return xu.searchParams.get('v') || '';
  } catch {
    return '';
  }
}

async function processYoutubeUrl(
  u: string,
  boss: PgBoss
): Promise<YouTubeIngestResult> {
  const { getOrCreateManualSource, upsertRawItem } = await import('./db.js');
  const videoId = extractYouTubeVideoId(u);
  if (!videoId) {
    return { url: u, ok: false, error: 'no_video_id', type: 'youtube' };
  }
  const sourceId = await getOrCreateManualSource(
    'youtube_manual',
    'youtube.com',
    'YouTube Manual',
    null
  );
  const rawId = await upsertRawItem({
    source_id: sourceId,
    external_id: videoId,
    url: u,
    title: null,
    kind: 'youtube',
    metadata: { src: 'manual' },
  });
  if (rawId) {
    await boss.send('ingest:fetch-youtube-content', {
      rawItemIds: [rawId],
      videoId,
      sourceKind: 'youtube_manual',
    });
    return { url: u, ok: true, raw_item_id: rawId, type: 'youtube' };
  }
  return { url: u, ok: false, error: 'duplicate', type: 'youtube' };
}

async function processArticleUrl(
  u: string,
  boss: PgBoss
): Promise<ArticleIngestResult> {
  const { getOrCreateManualSource, upsertRawItem } = await import('./db.js');
  let domain: string | null = null;
  try {
    domain = new URL(u).hostname || null;
  } catch {
    domain = null;
  }
  const sourceId = await getOrCreateManualSource(
    'manual',
    domain,
    domain,
    null
  );
  const rawId = await upsertRawItem({
    source_id: sourceId,
    external_id: u,
    url: u,
    title: null,
    kind: 'article',
    metadata: { src: 'manual' },
  });
  if (rawId) {
    await boss.send('ingest:fetch-content', { rawItemIds: [rawId] });
    return { url: u, ok: true, raw_item_id: rawId, type: 'article' };
  }
  return { url: u, ok: false, error: 'duplicate', type: 'article' };
}

function registerIngestOneoff(app: express.Express) {
  app.post('/debug/ingest-oneoff', async (req, res) => {
    try {
      if (!bossRef) {
        throw new Error('boss_not_ready');
      }
      const body = (req.body || {}) as { urls?: string[] };
      const urls = Array.isArray(body.urls) ? body.urls.filter(Boolean) : [];
      if (urls.length === 0) {
        return res
          .status(HTTP_BAD_REQUEST)
          .json({ ok: false, error: 'no_urls' });
      }

      const results: Array<
        | {
            url: string;
            ok: true;
            raw_item_id: string;
            type: 'youtube' | 'article';
          }
        | { url: string; ok: false; error: string; type: 'youtube' | 'article' }
      > = [];
      for (const u of urls) {
        const kind = classifyUrl(u);
        if (kind === 'youtube') {
          results.push(await processYoutubeUrl(u, bossRef));
        } else {
          results.push(await processArticleUrl(u, bossRef));
        }
      }
      res.json({ ok: true, results });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

function registerDebugStatus(app: express.Express) {
  // Simple status snapshot for quick observability
  app.get('/debug/status', async (_req, res) => {
    try {
      const pg = (await import('./db.js')).default;
      const [
        { rows: sources },
        { rows: rawCounts },
        { rows: contentCounts },
        { rows: jobStats },
      ] = await Promise.all([
        pg.query(
          "select count(*)::int as sources_rss from public.sources where kind = 'rss' and url is not null"
        ),
        pg.query(
          "select count(*)::int as raw_total, count(*) filter (where discovered_at > now() - interval '24 hours')::int as raw_24h from public.raw_items"
        ),
        pg.query('select count(*)::int as contents_total from public.contents'),
        pg.query(
          'select name, state, count(*)::int as count from pgboss.job group by 1,2 order by 1,2'
        ),
      ]);
      res.json({
        ok: true,
        sources: sources[0],
        raw: rawCounts[0],
        contents: contentCounts[0],
        jobs: jobStats,
      });
    } catch (e: unknown) {
      res
        .status(HTTP_SERVICE_UNAVAILABLE)
        .json({ ok: false, error: String(e) });
    }
  });
}

function main() {
  // Global guards to avoid process crash on unhandled errors
  process.on('uncaughtException', (err) => {
    log('uncaught_exception', { err: String(err) });
  });
  process.on('unhandledRejection', (reason) => {
    log('unhandled_rejection', { reason: String(reason) });
  });

  // Start HTTP early to satisfy Cloud Run health checks
  const app = express();
  app.use(express.json());
  registerHealth(app);
  registerPreviewSource(app);
  registerIngestNow(app);
  registerScheduleRss(app);
  registerIngestYouTubeNow(app);
  registerIngestSource(app);
  registerScheduleYouTube(app);
  registerIngestOneoff(app);
  registerDebugStatus(app);

  const port = Number(process.env.PORT || DEFAULT_PORT);
  app.listen(port, () => log('http_listen', { port }));

  // Boss init with retry loop (no process exit)
  const attempt = async () => {
    try {
      await initBoss();
    } catch (err) {
      log('boss_start_error', { err: String(err) });
      setTimeout(() => {
        attempt();
      }, RETRY_DELAY_MS);
    }
  };
  attempt();

  // Light job metrics mirror (every 3s)
  setInterval(async () => {
    try {
      const pg = (await import('./db.js')).default;
      const { upsertJobMetrics } = await import('./db.js');
      const { rows: jobStats } = await pg.query(
        'select name, state, count(*)::int as count from pgboss.job group by 1,2 order by 1,2'
      );
      await upsertJobMetrics(
        jobStats as Array<{ name: string; state: string; count: number }>
      );
    } catch {
      null;
    }
  }, METRICS_INTERVAL_MS);

  // Graceful shutdown
  const shutdown = async (sig: string) => {
    try {
      log('shutdown_signal', { sig });
      if (bossRef) {
        await bossRef.stop({ graceful: true });
      }
    } finally {
      process.exit(0);
    }
  };
  process.on('SIGINT', () => {
    shutdown('SIGINT');
  });
  process.on('SIGTERM', () => {
    shutdown('SIGTERM');
  });
}

main();

================
File: src/worker.ts
================
/**
 * New Worker Entry Point - Clean and Simple
 *
 * This is the new main entry point that replaces the monolithic worker.ts.
 * It's much simpler and easier to understand for beginners.
 *
 * What this file does:
 * 1. Starts the worker service
 * 2. That's it!
 *
 * All the complexity has been moved to focused modules:
 * - core/worker-service.ts: Main service coordination
 * - core/job-definitions.ts: All job configurations
 * - core/job-orchestrator.ts: Consistent job triggering
 * - http/routes.ts: All HTTP endpoints
 */

import { startWorkerService } from './core/worker-service.js';
import { log } from './log.js';

/**
 * Main function - starts the worker service
 */
async function main(): Promise<void> {
  try {
    log('worker_starting', {
      version: '2.0-modular',
      nodeVersion: process.version,
      platform: process.platform,
      arch: process.arch,
      pid: process.pid,
      timestamp: new Date().toISOString()
    });

    const service = await startWorkerService();

    log('worker_ready', {
      message: 'Worker service is running. Check /healthz for status.',
      healthEndpoint: `http://localhost:${process.env.PORT || 8080}/healthz`,
      statusEndpoint: `http://localhost:${process.env.PORT || 8080}/debug/status`
    });

    // Service will run until process is terminated
    // Keep the service reference to prevent garbage collection
    process.on('SIGTERM', () => {
      log('worker_shutdown_signal', { signal: 'SIGTERM' });
      service.stop().finally(() => process.exit(0));
    });

    process.on('SIGINT', () => {
      log('worker_shutdown_signal', { signal: 'SIGINT' });
      service.stop().finally(() => process.exit(0));
    });

  } catch (error) {
    log('worker_startup_failed', {
      error: String(error),
      stack: error instanceof Error ? error.stack : undefined,
      timestamp: new Date().toISOString()
    }, 'error');
    process.exit(1);
  }
}

// Start the worker
main();

================
File: .dockerignore
================
# Dependencies
node_modules
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# Build outputs
dist
build
.next
.turbo

# Environment files
.env
.env.local
.env.development
.env.test
.env.production

# IDE files
.vscode
.idea
*.swp
*.swo

# OS files
.DS_Store
Thumbs.db

# Git
.git
.gitignore

# Documentation
README.md
*.md

# Test files
__tests__
*.test.js
*.test.ts
*.spec.js
*.spec.ts
coverage

# Logs
logs
*.log

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Dependency directories
jspm_packages/

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next

# Nuxt.js build / generate output
.nuxt

# Gatsby files
.cache/
public

# Storybook build outputs
.out
.storybook-out

# Temporary folders
tmp/
temp/

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

================
File: .env.production
================
## Production/remote configuration

# Set the worker DB password used by the 'worker' role in the database
WORKER_DB_PASSWORD="b52cd7f802d7dd20903fff4fc157fc2e15a66c047d100189b6bfd932896759ed"

# Supabase Postgres connection string (Session Pooler) for Cloud Run
# Use ?sslmode=no-verify with Cloud Run unless you configure CA trust.
# Replace <project-ref> and <region> with your Supabase project ref and region.
DATABASE_URL=postgresql://worker.<project-ref>:${WORKER_DB_PASSWORD}@<region>.pooler.supabase.com:5432/postgres?sslmode=no-verify

# pg-boss schema and timezone
BOSS_SCHEMA=pgboss
BOSS_CRON_TZ=UTC

# Cloud Run deploy settings
PROJECT_ID=your-gcp-project-id
REGION=us-central1
SERVICE=zeke-worker
BOSS_MIGRATE=false

# OpenAI API Configuration
OPENAI_API_KEY="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"

# YouTube Data API Configuration
YOUTUBE_API_KEY="AIzaSyCx_1Nb6oNmWEdvobB01mWFM1KbLgVVllU"
YOUTUBE_QUOTA_LIMIT=10000
YOUTUBE_QUOTA_RESET_HOUR=0
YOUTUBE_RATE_LIMIT_BUFFER=500

================
File: agents.md
================
# Worker Agents Guide

This guide orients coding agents working inside `worker/` so changes stay reliable, efficient, and secure.

**Scope:** Background pipeline (ingest ‚Üí extract ‚Üí analyze) using PostgreSQL, `pg-boss`, and external APIs. Do not use the Supabase JS SDK in the worker.

## Project Overview

- **Purpose:** Ingest sources (RSS, YouTube), fetch and extract content, generate analysis/embeddings, and manage jobs.
- **Key Modules:**
  - `src/worker.ts`: HTTP health/debug endpoints, `pg-boss` queues, schedulers, workers
  - `src/db.ts`: Postgres `pg` Pool and all DB access helpers
  - `src/ingest/*`: Source ingestion (RSS, YouTube)
  - `src/extract/*`: Content extraction and normalization
  - `src/analyze/llm.ts`: LLM analysis and embeddings
  - `src/fetch/*`, `src/utils/*`, `src/lib/*`: External API wrappers and helpers
- **Queues (pg-boss):** `system:heartbeat`, `ingest:pull`, `ingest:fetch-content`, `ingest:fetch-youtube-content`, `analyze:llm`.
- **HTTP Endpoints:** `/healthz`, `/debug/status`, `/debug/ingest-now`, `/debug/schedule-rss`, `/debug/ingest-youtube`, `/debug/schedule-youtube`.

## Build & Run

- **Local (Docker, full deps):** `bash scripts/deploy-local-worker.sh`
- **Build:** `pnpm run build`
- **Start (prod bundle, non-Docker):** `pnpm run start`
- From repo root, run the full stack with `pnpm run dev` and pipeline checks with `pnpm run test:pipeline`.

## Testing Instructions

- **Connectivity:** `pnpm run test:connection` (DB + pg-boss schema/permissions)
- **Queue Health:** `pnpm run test:transcription` (pg-boss tables, job counts)
- **YouTube API:** `node test-youtube-api.js`
- **Pipeline (sample):** `node test-youtube-pipeline.js`
- **Status Snapshots:** `curl http://localhost:8080/debug/status`
- Ensure Supabase (Postgres) is running locally before tests. Use root scripts to start DB/migrations if needed.

## Code Style Guidelines

- **DB access (critical):**
  - Use the shared `pg` Pool from `src/db.ts`. Do not import or use the Supabase JS SDK in the worker.
  - Always write parameterized SQL (e.g., `$1, $2`) and return typed results via helpers in `db.ts`.
  - Reuse idempotent patterns (`insert ... on conflict do update/do nothing`) to avoid duplicates.
  - Do not create new `Pool()` instances; use the exported default pool and the provided functions.
- **Job handlers:**
  - Keep handlers deterministic and resilient (validate inputs, catch errors, `complete`/`fail` jobs explicitly).
  - Prefer small `batchSize` and sequential processing unless safe to parallelize.
  - Schedule recurring work via `boss.schedule` with `CRON_TZ`.
- **Networking:**
  - Use `AbortController` timeouts (15s) for external fetches.
  - Normalize URLs with `canonicalizeUrl` and hash text with `hashText`.
- **LLM usage:**
  - `OPENAI_API_KEY` optional; code must gracefully fall back to stub analysis/embeddings.
  - Cap input sizes conservatively; sanitize/validate JSON responses.
- **Logging:**
  - Use `log(evt, extra?, lvl)` from `src/log.ts` for structured JSON logs. Avoid `console.log` directly.
- **TypeScript & modules:**
  - Strict TS, ESM modules. Keep functions small and typed; avoid `any` and non‚Äënull assertions.
  - Follow repo formatting (Biome/Prettier via workspace settings).

## Security Considerations

- **No Supabase SDK:** Connect directly to Postgres using `pg` (see `src/db.ts`) for proper pooling and SSL control.
- **Secrets:** Read only from env (`.env.development` for dev). Never log secrets. Keep API keys server‚Äëside.
- **Postgres SSL:** `db.ts` enables TLS for non‚Äëlocal hosts and relaxes verification for managed poolers (`sslmode=no-verify`).
- **Untrusted content:** Treat all external URLs/content as untrusted. Enforce timeouts and validate inputs before DB writes.
- **Data safety:** Parameterize SQL, avoid dynamic SQL construction, and validate JSON before persisting.

## Adding a New Job

1. **Define queue:** In `src/worker.ts`, ensure `boss.createQueue('my:queue')` and schedule if recurring.
2. **Implement worker:** `await boss.work('my:queue', opts, async (jobs) => { ... })` using structured logging and try/catch per job.
3. **DB access:** Add typed helpers to `src/db.ts`. Use the shared Pool and parameterized SQL.
4. **Enqueue:** From existing handlers, call `boss.send('my:queue', data)` when appropriate.
5. **Test:** Use local `.env.development`, run `pnpm run dev`, and verify via `/debug/status` and scripts in `worker/scripts/`.

## Environment Variables

- **DATABASE_URL:** Postgres connection string (local uses 127.0.0.1; prod uses Supabase pooler with TLS)
- **BOSS_SCHEMA:** PgBoss schema (default `pgboss`)
- **BOSS_CRON_TZ:** Cron timezone (default `UTC`)
- **BOSS_MIGRATE:** Whether boss runs migrations (often `false` in prod)
- **PORT:** HTTP port (default `8080`)
- **OPENAI_API_KEY:** Optional; enables real analysis/embeddings
- **YOUTUBE_API_KEY:** Enables YouTube ingestion and tests

## DB Access Pattern (Reference)

- **Pool:** Created once in `src/db.ts` with keep‚Äëalive, small `max`, and SSL toggled by host.
- **Helpers:** Export typed fns like `getRssSources`, `upsertRawItem`, `insertContents`, `upsertStoryOverlay`, `upsertStoryEmbedding`.
- **Embeddings:** Store as JSON (`JSON.stringify(embedding)`) matching DB vector/JSON strategy.
- **Errors:** `pool.on('error', ...)` prevents process crashes; log with `log('pg_pool_error', ...)`.

By following these conventions‚Äîespecially direct `pg` usage instead of the Supabase SDK‚Äîthe worker remains efficient, portable, and production‚Äësafe.

## Third‚ÄëParty APIs Pattern

Use a small, composable pattern for external services (e.g., YouTube) that matches our tasks style and improves testability.

- Client wrapper: put per‚Äëservice clients under `src/lib/<service>/<service>-client.ts`. The client:
  - Handles auth/env once (e.g., API key), base URL, and shared config.
  - Exposes a minimal object (transport + config). Do not bake business logic in the client.
  - Example: `createYouTubeClient()` returns `{ youtube, quotaLimit, quotaBuffer, quotaResetHour }`.
- Verb‚Äënoun functions: one function per file in `src/lib/<service>/` (no barrels):
  - Examples (YouTube): `search-videos.ts`, `search-channels.ts`, `get-video-details.ts`, `get-channel-uploads.ts`, `check-quota-status.ts`.
  - Signature: `(client, input) => Promise<Output>`; never read env inside these methods.
  - Keep side effects out; they should compose cleanly in higher‚Äëlevel fetchers/ingesters.
- Shared types: colocate `types.ts` with the client. Model only fields we use; expand as needed.
- Retries: use `src/utils/retry.ts` (`withRetry`, `isRetryableDefault`).
  - Treat `quotaExceeded` as non‚Äëretryable; allow 429/5xx and transient network errors to retry with backoff.
  - Customize `isRetryable` per method if a provider has special semantics.
- Quota/rate limits:
  - Keep provider quota awareness local (e.g., YouTube cost constants) and integrate with our `QuotaTracker` at the call site.
  - Prefer explicit `part`/fields to reduce cost; avoid over‚Äëfetching.
- Logging: consistent structured logs via `log(evt, extra, lvl)`; include inputs (sanitized), counts, and quota usage.
- Usage example (YouTube):
  - Create client once: `const yt = createYouTubeClient()`.
  - Call methods: `await searchVideos(yt, { query, maxResults })`, `await getVideoDetails(yt, ids)`.
- Don‚Äôts:
  - Don‚Äôt add barrels (`index.ts`) ‚Äî import concrete files.
  - Don‚Äôt access env inside method files; only in the client factory.
  - Don‚Äôt leak secrets or raw tokens to logs/errors.

This structure keeps call sites explicit, isolates auth/transport, and makes unit tests trivial (inject a fake client and stub the transport).

## Tasks Catalog (worker/src/tasks)

- analyze-story.ts: Generate overlays + embedding for a story (OpenAI or stub), then persist.
- extract-article.ts: Fetch + parse article content, create content/story, enqueue analysis.
- extract-youtube-content.ts: Extract audio ‚Üí transcribe ‚Üí VTT ‚Üí content/story ‚Üí enqueue analysis.
- fetch-youtube-channel-videos.ts: List channel uploads (via uploads playlist) + details with quota.
- fetch-youtube-search-videos.ts: Search videos + map to domain with quota.
- resolve-youtube-uploads-id.ts: Derive and persist channel uploads playlist ID.
- ingest-rss-source.ts: Fetch ‚Üí parse ‚Üí normalize ‚Üí upsert ‚Üí enqueue (per RSS source).
- preview-rss-source.ts: Fetch ‚Üí parse ‚Üí normalize ‚Üí preview items (no writes).
- ingest-youtube-source.ts: Orchestrate channel/search ingest, upsert raw items, enqueue extraction.
- preview-youtube-source.ts: Preview channel/search items with current quota status.

Primitives used by these tasks live under `extract/*`, `storage/*`, `transcribe/*`, and are single‚Äëpurpose (one function per file) without env/DB/queue access.

## Tasks vs Lib vs Primitives

Use this simple split to keep code easy to compose and test:

- Tasks (`src/tasks/*`): business logic that composes helpers + lib and may write to DB or enqueue jobs.
  - One function per file (verb-noun), typed inputs/outputs, explicit side effects.
  - Example: `analyze-story.ts`, `extract-article.ts`, `extract-youtube-content.ts`, `fetch-youtube-channel-videos.ts`.
- Lib (`src/lib/*`): thin third‚Äëparty clients only (auth, transport, minimal request building).
  - No domain logic, no DB, env only in client factory.
  - Example: `lib/openai/*`, `lib/youtube/*`, `utils/retry.ts`.
- Primitives (`src/extract/*`, `src/transcribe/*`, `src/storage/*`): single‚Äëpurpose local helpers.
  - One function per file (verb-noun), no env, no DB, no queues. Small, typed, focus on doing one thing well.
  - Examples: `extract-youtube-audio.ts`, `get-youtube-metadata.ts`, `generate-vtt-content.ts`, `prepare-youtube-transcript.ts`, `transcribe-audio.ts`.

Heuristic

- Calls external SDK? Put it in lib.
- Spawns a local tool or formats/derives local data? Put it in primitives.
- Combines multiple steps and performs side effects (DB/queues)? Put it in tasks.

================
File: ARCHITECTURE.md
================
# ZEKE Worker Architecture Guide

## üéØ Overview

The ZEKE Worker is responsible for:
- **Ingesting** news from RSS feeds and YouTube channels
- **Extracting** content from articles and videos
- **Analyzing** content with AI/LLM processing
- **Managing** the entire pipeline through job queues

This guide explains the **new modular architecture** that makes the system easier to understand and maintain.

## üèóÔ∏è Architecture Principles

### Before (Confusing)
- ‚ùå One giant 797-line file with everything mixed together
- ‚ùå HTTP endpoints and job processing in the same place
- ‚ùå Duplicate code paths for manual vs scheduled jobs
- ‚ùå Hard to understand what does what

### After (Clear)
- ‚úÖ **Single Responsibility**: Each module does one thing well
- ‚úÖ **Consistent Patterns**: All jobs triggered the same way
- ‚úÖ **Clear Separation**: HTTP, jobs, and business logic separated
- ‚úÖ **Beginner Friendly**: Easy to find and understand code

## üìÅ New File Structure

```
src/
‚îú‚îÄ‚îÄ worker.ts                  # üöÄ Main entry point (30 lines - new modular architecture)
‚îú‚îÄ‚îÄ core/                      # üß† Core business logic
‚îÇ   ‚îú‚îÄ‚îÄ worker-service.ts      # Main service coordinator
‚îÇ   ‚îú‚îÄ‚îÄ job-orchestrator.ts    # Consistent job triggering
‚îÇ   ‚îî‚îÄ‚îÄ job-definitions.ts     # All job configurations
‚îú‚îÄ‚îÄ http/                      # üåê HTTP endpoints
‚îÇ   ‚îî‚îÄ‚îÄ routes.ts              # All API routes
‚îú‚îÄ‚îÄ tasks/                     # üìã Business logic (unchanged)
‚îÇ   ‚îú‚îÄ‚îÄ ingest-rss-source.ts
‚îÇ   ‚îú‚îÄ‚îÄ extract-article.ts
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ worker-old.ts              # üîÑ Legacy architecture (backup)
‚îî‚îÄ‚îÄ [other existing files]     # üîß Utilities, DB, etc.
```

## üîÑ How Jobs Work Now

### The Old Confusing Way
```
Frontend ‚Üí API ‚Üí HTTP Endpoint ‚Üí Direct Function Call
                              ‚Üì
Scheduled Job ‚Üí Queue ‚Üí Worker ‚Üí Same Function (duplicated!)
```

### The New Clear Way
```
Frontend ‚Üí API ‚Üí Job Orchestrator ‚Üí Queue ‚Üí Worker ‚Üí Task Function
Scheduler ‚Üí Job Orchestrator ‚Üí Queue ‚Üí Worker ‚Üí Task Function
```

**Key Insight**: Everything goes through the **Job Orchestrator** now. No more duplicate code paths!

## üéÆ Job Orchestrator

The **Job Orchestrator** (`core/job-orchestrator.ts`) is the central hub that eliminates confusion:

```typescript
// ‚úÖ Clean: All job triggering goes through orchestrator
await orchestrator.triggerRssIngest();           // Manual trigger
await orchestrator.triggerYouTubeIngest();       // Manual trigger
await orchestrator.triggerStoryAnalysis(id);     // Analysis trigger

// ‚ùå Old way: Direct function calls mixed with queue sends
await ingestRssSource(boss, src);                // Direct call
await boss.send("ingest:pull", data);            // Queue send
```

### Available Operations
- `triggerRssIngest()` - Ingest all RSS sources
- `triggerYouTubeIngest()` - Ingest all YouTube sources
- `triggerRssSourceIngest(id)` - Ingest specific RSS source
- `triggerYouTubeSourceIngest(id)` - Ingest specific YouTube source
- `triggerContentExtraction(ids)` - Extract article content
- `triggerYouTubeContentExtraction(data)` - Extract video content
- `triggerStoryAnalysis(id)` - Analyze story with AI
- `triggerOneOffIngest(urls)` - Process arbitrary URLs

## üîß Job Definitions

All job configurations live in `core/job-definitions.ts`:

```typescript
// Queue names (type-safe)
export const QUEUES = {
  SYSTEM_HEARTBEAT: "system:heartbeat",
  INGEST_PULL: "ingest:pull",
  INGEST_FETCH_CONTENT: "ingest:fetch-content",
  // ... etc
} as const;

// Job data types (for safety)
export interface IngestPullJobData {
  source: "rss" | "youtube";
}

// Setup functions
await createJobQueues(boss);        // Create all queues
await scheduleRecurringJobs(boss);  // Set up cron schedules
await setupJobWorkers(boss, orch);  // Start all workers
```

## üåê HTTP Routes

All HTTP endpoints are in `http/routes.ts` and follow consistent patterns:

```typescript
// Health checks
GET  /healthz              # Simple health check
GET  /debug/status         # Detailed system status

// Manual job triggers (admin only)
POST /debug/ingest-now     # Trigger RSS ingest
POST /debug/ingest-youtube # Trigger YouTube ingest
POST /debug/ingest-source  # Trigger specific source
POST /debug/ingest-oneoff  # Process arbitrary URLs

// Source testing
GET  /debug/preview-source # Preview source content
```

**Key Point**: HTTP endpoints only call the orchestrator. No business logic in routes!

## üöÄ Getting Started

### 1. Understanding the Flow

1. **Entry Point**: `worker-new.ts` starts everything
2. **Service**: `worker-service.ts` coordinates all components
3. **Jobs**: `job-definitions.ts` defines what work gets done
4. **Orchestrator**: `job-orchestrator.ts` triggers work consistently
5. **Routes**: `routes.ts` handles HTTP requests
6. **Tasks**: `tasks/` contains the actual business logic

### 2. Adding a New Job

```typescript
// 1. Add queue name to job-definitions.ts
export const QUEUES = {
  // ... existing queues
  MY_NEW_QUEUE: "my:new-queue",
} as const;

// 2. Add job data type
export interface MyJobData {
  someId: string;
  options?: Record<string, unknown>;
}

// 3. Add queue creation
await boss.createQueue(QUEUES.MY_NEW_QUEUE);

// 4. Add worker
await boss.work(QUEUES.MY_NEW_QUEUE, async (jobs) => {
  for (const job of jobs) {
    await processMyJob(boss, job);
  }
});

// 5. Add orchestrator method
async triggerMyJob(data: MyJobData): Promise<void> {
  await boss.send(QUEUES.MY_NEW_QUEUE, data);
}

// 6. Add HTTP endpoint (if needed)
app.post("/debug/my-job", async (req, res) => {
  await orchestrator.triggerMyJob(req.body);
  res.json({ ok: true });
});
```

### 3. Common Patterns

**Error Handling**:
```typescript
try {
  await doWork(job.data);
  await boss.complete(QUEUE_NAME, job.id);
} catch (err) {
  await boss.fail(QUEUE_NAME, job.id, { error: String(err) });
}
```

**Logging**:
```typescript
log("job_start", { jobId: job.id, type: "my_job" });
// ... do work
log("job_done", { jobId: job.id, result: "success" });
```

**Type Safety**:
```typescript
const { someId } = (job.data as MyJobData) || {};
if (!someId) {
  throw new Error("Missing someId in job data");
}
```

## üîç Debugging

### Check System Status
```bash
curl http://localhost:8080/debug/status
```

### Trigger Jobs Manually
```bash
# RSS ingest
curl -X POST http://localhost:8080/debug/ingest-now

# YouTube ingest
curl -X POST http://localhost:8080/debug/ingest-youtube

# Specific source
curl -X POST "http://localhost:8080/debug/ingest-source?sourceId=123"
```

### Monitor Logs
```bash
# In development
pnpm run dev

# Check logs
pnpm run logs
```

## üéØ Benefits of New Architecture

1. **Easier to Learn**: Each file has a clear, single purpose
2. **Easier to Debug**: Consistent patterns and clear data flow
3. **Easier to Extend**: Add new jobs following established patterns
4. **Easier to Test**: Isolated components can be tested independently
5. **Easier to Maintain**: Changes are localized to specific modules

## ‚úÖ Migration Complete

The new modular architecture is now the primary implementation:

1. **‚úÖ New architecture active**: `worker.ts` now uses the modular system
2. **‚úÖ Legacy preserved**: `worker-old.ts` contains the original monolithic system
3. **‚úÖ Scripts updated**: All npm scripts use the new architecture by default
4. **‚úÖ Backward compatibility**: Use `:old` suffix to access legacy system

The migration maintains 100% functional compatibility while providing a much clearer and easier to work with codebase.

## üìö Quick Reference

### File Purposes
- `worker.ts` - üöÄ **Start here**: Simple entry point (new modular architecture)
- `core/worker-service.ts` - üéõÔ∏è **Coordinator**: Manages everything
- `core/job-orchestrator.ts` - üéØ **Trigger**: Consistent job starting
- `core/job-definitions.ts` - üìã **Config**: All job setup
- `http/routes.ts` - üåê **API**: All HTTP endpoints
- `tasks/*.ts` - üîß **Work**: Actual business logic
- `worker-old.ts` - üîÑ **Legacy**: Original monolithic system (backup)

### Common Commands
```bash
# Development (new architecture - default)
pnpm run dev                    # Start with modular architecture
pnpm run dev:old               # Start with legacy architecture

# Testing
pnpm run test:unit             # Unit tests for core modules
pnpm run test:integration      # End-to-end pipeline tests

# Manual triggers
curl -X POST localhost:8080/debug/ingest-now
curl -X POST localhost:8080/debug/ingest-youtube

# Status check
curl localhost:8080/debug/status
curl localhost:8080/healthz
```

================
File: Dockerfile
================
# Railway-compatible Dockerfile for ZEKE Worker (Core RSS Processing)
# YouTube processing is optional and will be skipped if YOUTUBE_API_KEY is not provided
# Cache bust: 2025-09-15-v3-clean-build
FROM node:20-alpine

WORKDIR /app

# Install minimal system dependencies for core functionality (Alpine)
RUN apk add --no-cache \
    curl \
    ca-certificates

# Copy package files
COPY package.json ./

# Install ALL dependencies (including dev dependencies for build)
RUN npm install

# Copy TypeScript configuration and source code
COPY tsconfig.json ./
COPY src ./src

# Build TypeScript
RUN npx tsc -p tsconfig.json

# Remove dev dependencies to reduce image size
RUN npm prune --production

# Set production environment
ENV NODE_ENV=production

# Disable YouTube processing by default (can be enabled by setting YOUTUBE_API_KEY)
ENV YOUTUBE_PROCESSING_ENABLED=false

# Expose port (Railway will set PORT env var)
EXPOSE 8080

# Health check - Railway-optimized
HEALTHCHECK --interval=30s --timeout=15s --start-period=90s --retries=5 \
    CMD curl -f http://localhost:${PORT:-8080}/healthz || exit 1

# Start the application
CMD ["node", "dist/worker.js"]

================
File: example-architecture.md
================
# Midday Engine Architecture

## Overview

The Midday Engine is a sophisticated financial data aggregation and processing system that connects to multiple banking providers, processes financial transactions, and maintains a centralized database. It's built using a modern, event-driven architecture with robust error handling and scalable background processing.

## System Architecture

```mermaid
graph TB
    subgraph "Frontend Layer"
        Dashboard[Dashboard App]
        API[API Routes]
    end

    subgraph "Engine Layer"
        Engine[Engine API<br/>Cloudflare Workers]
        Providers[Provider System]
        Teller[Teller Provider]
        Plaid[Plaid Provider]
        GoCardless[GoCardless Provider]
    end

    subgraph "Job Processing"
        TriggerDev[Trigger.dev]
        Jobs[Background Jobs]
        Scheduler[Job Scheduler]
    end

    subgraph "Data Layer"
        Supabase[(Supabase Database)]
        KV[Cloudflare KV]
        Storage[File Storage]
    end

    subgraph "External Services"
        Banks[Banking APIs]
        Notifications[Novu Notifications]
        Email[Resend Email]
    end

    Dashboard --> API
    API --> Engine
    Engine --> Providers
    Providers --> Teller
    Providers --> Plaid
    Providers --> GoCardless
    Teller --> Banks
    Plaid --> Banks
    GoCardless --> Banks

    API --> TriggerDev
    TriggerDev --> Jobs
    Jobs --> Scheduler
    Jobs --> Supabase
    Jobs --> Notifications
    Jobs --> Email

    Engine --> KV
    Jobs --> Storage
```

## Core Components

### 1. Engine API (`apps/engine`)

The Engine API is deployed as a Cloudflare Worker and provides RESTful endpoints for:

- **Transactions**: Fetch transaction data from banking providers
- **Accounts**: Retrieve account information and balances
- **Institutions**: Get supported banking institutions
- **Rates**: Exchange rate data

**Key Features:**
- OpenAPI specification with Zod validation
- Bearer token authentication
- Caching middleware for performance
- Multi-provider abstraction

**Deployment:**
- Production: `engine.midday.ai`
- Staging: `engine-staging.midday.ai`
- Uses Cloudflare KV for caching and mTLS certificates for secure banking connections

### 2. Provider System

The provider system abstracts different banking APIs through a unified interface:

```typescript
interface Provider {
  getTransactions(params: GetTransactionsRequest): Promise<GetTransactionsResponse>;
  getAccounts(params: GetAccountsRequest): Promise<GetAccountsResponse>;
  getAccountBalance(params: GetAccountBalanceRequest): Promise<GetAccountBalanceResponse>;
  getInstitutions(params: GetInstitutionsRequest): Promise<GetInstitutionsResponse>;
  getHealthCheck(): Promise<boolean>;
  deleteAccounts(params: DeleteAccountsRequest): void;
}
```

**Supported Providers:**
- **Teller**: US banking with mTLS authentication
- **Plaid**: US/Canada banking with OAuth
- **GoCardless**: European banking with API keys

### 3. Job Processing System (`packages/jobs`)

Built on Trigger.dev for reliable background processing with multiple trigger types:

#### Event Triggers (Manual/API-initiated)
- `TRANSACTIONS_MANUAL_SYNC`: User-initiated sync
- `TRANSACTIONS_IMPORT`: CSV/image import processing
- `INBOX_MATCH`: Document-transaction matching

#### Scheduled Triggers (Automatic)
- `TRANSACTIONS_SYNC`: Every 8 hours per team
- `EXCHANGE_RATES_UPDATE`: Daily at 12:00 UTC
- `BANK_CONNECTION_EXPIRING`: Weekly on Mondays
- `BANK_CONNECTION_DISCONNECTED`: Weekly on Mondays

#### Database Triggers (Reactive)
- `INBOX_UPLOAD`: File upload processing
- Supabase webhook triggers for real-time processing

## Database Schema

### Core Tables

#### `transactions`
```sql
CREATE TABLE transactions (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    internal_id text UNIQUE NOT NULL,  -- Format: {team_id}_{provider_transaction_id}
    team_id uuid NOT NULL,
    bank_account_id uuid,
    name text NOT NULL,
    description text,
    amount numeric NOT NULL,
    currency text NOT NULL,
    date date NOT NULL,
    method transactionMethods NOT NULL,
    status transactionStatus DEFAULT 'posted',
    category transactionCategories,
    category_slug text,
    balance numeric,
    manual boolean DEFAULT false,
    created_at timestamp with time zone DEFAULT now()
);
```

#### `bank_accounts`
```sql
CREATE TABLE bank_accounts (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    account_id text NOT NULL,  -- Provider's account ID
    team_id uuid NOT NULL,
    bank_connection_id uuid,
    name text,
    currency text,
    balance numeric DEFAULT 0,
    base_balance numeric,      -- Balance in team's base currency
    base_currency text,
    type account_type,         -- credit, depository, etc.
    enabled boolean DEFAULT true,
    manual boolean DEFAULT false,
    created_at timestamp with time zone DEFAULT now()
);
```

#### `bank_connections`
```sql
CREATE TABLE bank_connections (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    team_id uuid NOT NULL,
    institution_id text NOT NULL,
    provider bank_providers,   -- teller, plaid, gocardless
    name text NOT NULL,
    logo_url text,
    access_token text,         -- Encrypted access token
    enrollment_id text,        -- Teller enrollment ID
    reference_id text,         -- GoCardless reference ID
    expires_at timestamp with time zone,
    status connection_status DEFAULT 'connected',
    error_details text,
    last_accessed timestamp with time zone,
    created_at timestamp with time zone DEFAULT now(),
    UNIQUE(team_id, institution_id)
);
```

## Data Flow

### 1. Transaction Sync Process

```mermaid
sequenceDiagram
    participant User
    participant Dashboard
    participant TriggerDev
    participant Engine
    participant Provider
    participant Bank
    participant Database

    User->>Dashboard: Manual Sync
    Dashboard->>TriggerDev: Send Event
    TriggerDev->>Engine: Get Transactions
    Engine->>Provider: Fetch Data
    Provider->>Bank: API Call
    Bank-->>Provider: Transaction Data
    Provider-->>Engine: Normalized Data
    Engine-->>TriggerDev: Transaction List
    TriggerDev->>TriggerDev: Transform Data
    TriggerDev->>Database: Batch Upsert
    TriggerDev->>TriggerDev: Send Notifications
```

### 2. Data Transformation Pipeline

1. **Fetch**: Raw data from banking provider
2. **Transform**: Convert to standardized format
3. **Validate**: Ensure data integrity
4. **Batch**: Process in chunks of 500 transactions
5. **Upsert**: Insert/update with conflict resolution
6. **Notify**: Send notifications for new transactions

### 3. Error Handling

The system implements comprehensive error handling:

```typescript
// API Error Parsing
export function parseAPIError(error: unknown) {
  if (typeof error === "object" && error !== null && "error" in error) {
    const apiError = error as { error: { code: string; message: string } };
    return {
      code: apiError.error.code,
      message: apiError.error.message,
    };
  }
  return { code: "unknown", message: "An unknown error occurred" };
}
```

**Error Recovery:**
- Connection status tracking in `bank_connections.status`
- Automatic retry with exponential backoff
- Error details stored for debugging
- Email notifications for persistent failures

## Trigger Mechanisms

### 1. User-Initiated Actions

```typescript
// Manual sync from dashboard
export const manualSyncTransactionsAction = authActionClient
  .action(async ({ parsedInput: { connectionId }, ctx: { user } }) => {
    const event = await client.sendEvent({
      name: Events.TRANSACTIONS_MANUAL_SYNC,
      payload: {
        connectionId,
        teamId: user.team_id,
      },
    });
    return event;
  });
```

### 2. Scheduled Jobs

```typescript
// Automatic sync every 8 hours per team
client.defineJob({
  id: Jobs.TRANSACTIONS_SYNC,
  trigger: scheduler,  // Dynamic scheduler per team
  run: async (_, io, ctx) => {
    const teamId = ctx.source?.id as string;
    // Process all enabled accounts for team
  },
});
```

### 3. Database Webhooks

Supabase triggers for real-time processing:
- File uploads trigger document processing
- New connections trigger initial sync
- Transaction updates trigger notifications

## Performance Optimizations

### 1. Batch Processing

All database operations use batch processing to handle large datasets efficiently:

```typescript
export async function processBatch<T, R>(
  items: T[],
  limit: number,
  fn: (batch: T[]) => Promise<R[]>,
): Promise<R[]> {
  const batches: T[][] = [];

  // Split into batches of 500
  for (let i = 0; i < items?.length; i += limit) {
    batches.push(items.slice(i, i + limit));
  }

  // Process serially to avoid overwhelming the database
  for (const batch of batches) {
    await fn(batch);
  }
}
```

### 2. Caching Strategy

- **Engine API**: Cloudflare KV caching for institutions and rates
- **Dashboard**: Next.js cache with revalidation tags
- **Database**: Indexed queries with optimized schemas

### 3. Conflict Resolution

Transactions use composite unique keys to prevent duplicates:
```sql
-- Unique constraint on internal_id (team_id + provider_transaction_id)
ALTER TABLE transactions ADD CONSTRAINT transactions_internal_id_key UNIQUE (internal_id);
```

## Security

### 1. Authentication & Authorization
- Bearer token authentication for Engine API
- Row Level Security (RLS) in Supabase
- Team-based data isolation

### 2. Data Encryption
- Access tokens encrypted at rest
- mTLS certificates for banking connections
- Secure webhook signatures

### 3. Banking Compliance
- PCI DSS compliance through providers
- No sensitive banking data stored locally
- Audit trails for all financial operations

## Monitoring & Observability

### 1. Logging
- Structured logging with request IDs
- Error tracking with Sentry integration
- Performance monitoring

### 2. Notifications
- Real-time notifications via Novu
- Email alerts for connection issues
- In-app notifications for new transactions

### 3. Health Checks
- Provider health monitoring
- Connection status tracking
- Automated error recovery

## Deployment

### Engine API
- **Platform**: Cloudflare Workers
- **Environments**: Production, Staging
- **Configuration**: `wrangler.toml`

### Jobs System
- **Platform**: Trigger.dev Cloud
- **Runtime**: Node.js
- **Integrations**: Supabase, Resend, Novu

### Database
- **Platform**: Supabase (PostgreSQL)
- **Migrations**: Automated via Supabase CLI
- **Backups**: Automated daily backups

## Development Workflow

1. **Local Development**: Use Supabase local instance
2. **Testing**: Comprehensive test suites for providers
3. **Staging**: Full environment for testing
4. **Production**: Blue-green deployments

This architecture ensures reliable, scalable, and secure financial data processing while maintaining excellent developer experience and operational visibility.

## Detailed Job Descriptions

### Transaction Jobs

#### 1. Initial Sync (`TRANSACTIONS_INITIAL_SYNC`)
**Purpose**: First-time sync when a bank connection is established
**Trigger**: Event-based (when user connects a bank account)
**Process**:
1. Fetch all historical transactions from provider
2. Transform to internal format
3. Batch insert (500 transactions per batch)
4. Update account balances
5. Schedule recurring sync job (every 8 hours)
6. Send setup completion notification

```typescript
// Key features:
- Handles large transaction volumes efficiently
- Sets up recurring sync scheduler per team
- Comprehensive error handling with connection status updates
- Progress tracking with status updates
```

#### 2. Scheduled Sync (`TRANSACTIONS_SYNC`)
**Purpose**: Regular transaction updates for all connected accounts
**Trigger**: Dynamic scheduler (every 8 hours per team)
**Process**:
1. Query all enabled, non-manual accounts for team
2. Fetch latest transactions from each provider
3. Update account balances
4. Process new transactions in batches
5. Send notifications for new transactions
6. Update connection last_accessed timestamp

#### 3. Manual Sync (`TRANSACTIONS_MANUAL_SYNC`)
**Purpose**: User-initiated sync for specific connection
**Trigger**: User action from dashboard
**Process**:
1. Fetch latest transactions for specific connection
2. Process only new transactions (using `latest: true` flag)
3. Update account balance
4. Immediate cache invalidation
5. Real-time UI updates

#### 4. Transaction Import (`TRANSACTIONS_IMPORT`)
**Purpose**: Import transactions from CSV files or images
**Trigger**: File upload event
**Process**:
1. Parse CSV data or extract from images using OCR
2. Map columns to transaction fields
3. Validate and transform data
4. Handle currency conversion if needed
5. Batch insert with duplicate detection
6. Generate import summary report

### Banking Jobs

#### 1. Connection Expiring (`BANK_CONNECTION_EXPIRING`)
**Purpose**: Notify users of expiring bank connections
**Trigger**: Cron job (weekly on Mondays at 15:30 UTC)
**Process**:
1. Query connections expiring within 7 days
2. Group by team and user
3. Send email notifications
4. Create in-app notifications
5. Track notification delivery

#### 2. Connection Disconnected (`BANK_CONNECTION_DISCONNECTED`)
**Purpose**: Handle and notify about disconnected bank connections
**Trigger**: Cron job (weekly on Mondays at 14:30 UTC)
**Process**:
1. Identify connections with 'disconnected' status
2. Send reconnection instructions via email
3. Disable automatic sync for affected accounts
4. Log disconnection events for analytics

### Utility Jobs

#### 1. Exchange Rates Update (`EXCHANGE_RATES_UPDATE`)
**Purpose**: Daily currency exchange rate updates
**Trigger**: Cron job (daily at 12:00 UTC)
**Process**:
1. Fetch latest rates from engine API
2. Transform rate data for database storage
3. Batch upsert exchange rates
4. Update base currency calculations for accounts

#### 2. Inbox Processing (`INBOX_UPLOAD`, `INBOX_DOCUMENT`, `INBOX_MATCH`)
**Purpose**: Process uploaded documents and match with transactions
**Triggers**: Database triggers and events
**Process**:
1. **Upload**: Extract metadata from uploaded files
2. **Document**: OCR processing and data extraction
3. **Match**: AI-powered transaction matching

## Provider Implementation Details

### Teller Provider
**Authentication**: mTLS certificates
**Endpoints**:
- `GET /accounts` - List accounts
- `GET /accounts/{id}/transactions` - Get transactions
- `GET /accounts/{id}/balances` - Get balance
- `GET /institutions` - List institutions

**Key Features**:
- Real-time transaction data
- Comprehensive US bank coverage
- Strong security with certificate-based auth
- Webhook support for real-time updates

### Plaid Provider
**Authentication**: OAuth 2.0 with client credentials
**Endpoints**:
- `/accounts/get` - List accounts
- `/transactions/get` - Get transactions
- `/accounts/balance/get` - Get balance
- `/institutions/get` - List institutions

**Key Features**:
- US and Canada coverage
- Extensive transaction categorization
- Identity verification capabilities
- Sandbox environment for testing

### GoCardless Provider
**Authentication**: API key-based
**Endpoints**:
- `/accounts/` - Account management
- `/transactions/` - Transaction data
- `/institutions/` - Institution data

**Key Features**:
- European bank coverage
- Open Banking compliance
- Strong regulatory compliance
- Multi-country support

## Error Handling Patterns

### 1. Provider API Errors
```typescript
try {
  const transactions = await engine.transactions.list(params);
} catch (error) {
  if (error instanceof Midday.APIError) {
    const parsedError = parseAPIError(error);

    // Update connection status
    await supabase
      .from("bank_connections")
      .update({
        status: parsedError.code,
        error_details: parsedError.message,
      })
      .eq("id", connectionId);
  }
}
```

### 2. Retry Logic
The system implements exponential backoff retry logic:
- Initial retry after 1 second
- Maximum 3 retry attempts
- Exponential backoff (1s, 2s, 4s)
- Circuit breaker for persistent failures

### 3. Graceful Degradation
- Continue processing other accounts if one fails
- Partial sync completion tracking
- User notification of partial failures
- Automatic recovery attempts

## Notification System

### 1. Real-time Notifications (Novu)
**Types**:
- New transaction alerts
- Connection status changes
- Import completion
- Document matches

**Delivery Channels**:
- In-app notifications
- Email notifications
- Push notifications (mobile)

### 2. Email Notifications (Resend)
**Templates**:
- Transaction summaries
- Connection expiry warnings
- Import reports
- Error notifications

**Features**:
- Internationalization support
- Custom reply-to addresses
- Tracking and analytics
- Template versioning

## Cache Strategy

### 1. Engine API Caching
```typescript
// Cache middleware for specific routes
app.get("/institutions", cacheMiddleware);
app.get("/accounts", cacheMiddleware);
app.get("/transactions", cacheMiddleware);

// Cache configuration
const cacheMiddleware = cache({
  cacheName: "engine",
  cacheControl: "max-age=3600", // 1 hour
});
```

### 2. Dashboard Caching
```typescript
// Next.js cache with revalidation tags
export const getTransactions = async (params) => {
  return unstable_cache(
    async () => getTransactionsQuery(supabase, params),
    ["transactions", teamId],
    {
      revalidate: 180, // 3 minutes
      tags: [`transactions_${teamId}`],
    },
  );
};
```

### 3. Cache Invalidation
Strategic cache invalidation on data updates:
- Transaction sync invalidates transaction caches
- Account updates invalidate balance caches
- Connection changes invalidate account caches

## Development Guidelines

### 1. Adding New Providers
1. Implement the `Provider` interface
2. Add provider-specific API client
3. Implement data transformation functions
4. Add comprehensive tests
5. Update provider factory
6. Add configuration options

### 2. Creating New Jobs
1. Define job constants in `constants.ts`
2. Implement job with proper schema validation
3. Add error handling and logging
4. Include progress tracking
5. Add comprehensive tests
6. Document job behavior

### 3. Database Migrations
1. Use Supabase migration system
2. Include rollback procedures
3. Test with production data volumes
4. Coordinate with application deployments
5. Monitor performance impact

This comprehensive architecture documentation provides a complete understanding of how the Midday Engine operates, from high-level architecture to implementation details.

## Technical Implementation Details

### Complete Technology Stack

#### Frontend Stack
```json
{
  "framework": "Next.js 14.2.1",
  "runtime": "React 18.3.1",
  "language": "TypeScript 5.6.2",
  "styling": "Tailwind CSS with @todesktop/tailwind-variants",
  "state_management": "Zustand 4.5.5",
  "forms": "React Hook Form 7.53.0 + Zod validation",
  "ui_components": "@midday/ui (custom component library)",
  "animations": "Framer Motion 11.5.4",
  "charts": "Recharts 2.12.7",
  "internationalization": "next-international 1.2.4",
  "themes": "next-themes 0.3.0"
}
```

#### Backend Stack
```json
{
  "api_framework": "Hono 4.6.2 (Cloudflare Workers)",
  "database": "Supabase (PostgreSQL 15+)",
  "job_processing": "Trigger.dev 2.3.19",
  "authentication": "Supabase Auth + Row Level Security",
  "file_storage": "Supabase Storage + Cloudflare R2",
  "caching": "Cloudflare KV + Next.js Cache",
  "email": "Resend 3.5.0",
  "notifications": "Novu 2.0.0",
  "monitoring": "Sentry 8.x"
}
```

#### Banking Integrations
```json
{
  "providers": {
    "teller": "mTLS certificate-based authentication",
    "plaid": "OAuth 2.0 with Plaid SDK 27.0.0",
    "gocardless": "API key-based with Open Banking"
  },
  "security": "PCI DSS compliant through providers",
  "data_encryption": "TLS 1.3 + encrypted storage"
}
```

### Type Safety Implementation

#### 1. Database Schema to TypeScript Generation

The system maintains end-to-end type safety through automated type generation:

```typescript
// packages/supabase/package.json - Type generation script
{
  "scripts": {
    "db:generate": "supabase gen types --lang=typescript --project-id $PROJECT_ID --schema public > src/types/db.ts"
  }
}
```

Generated types provide complete type safety:

```typescript
// packages/supabase/src/types/db.ts
export type Database = {
  public: {
    Tables: {
      transactions: {
        Row: {
          id: string;
          internal_id: string;
          team_id: string;
          bank_account_id: string | null;
          name: string;
          amount: number;
          currency: string;
          date: string;
          method: Database["public"]["Enums"]["transactionMethods"];
          status: Database["public"]["Enums"]["transactionStatus"];
          // ... complete type definitions
        };
        Insert: {
          // Insert-specific types with optional fields
        };
        Update: {
          // Update-specific types with all optional fields
        };
      };
    };
    Enums: {
      transactionMethods: "payment" | "card_purchase" | "transfer" | "ach" | "wire" | "fee";
      transactionStatus: "posted" | "pending" | "excluded" | "completed";
      bank_providers: "gocardless" | "plaid" | "teller";
    };
  };
};
```

#### 2. Type-Safe Database Operations

All database operations use generated types for compile-time safety:

```typescript
// packages/supabase/src/client/server.ts
import type { Database } from "../types";

export const createClient = (options?: CreateClientOptions) => {
  return createServerClient<Database>(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    key,
    {
      cookies: {
        get(name: string) {
          return cookieStore.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value, ...options });
          } catch (error) {}
        },
      },
    },
  );
};

// Type-safe client usage
export type Client = SupabaseClient<Database>;
```

#### 3. Zod Schema Validation Patterns

The system uses Zod for runtime validation and type inference:

```typescript
// apps/dashboard/src/actions/schema.ts
export const connectBankAccountSchema = z.object({
  referenceId: z.string().nullable().optional(), // GoCardless
  accessToken: z.string().nullable().optional(), // Teller
  enrollmentId: z.string().nullable().optional(), // Teller
  provider: z.enum(["gocardless", "plaid", "teller"]),
  accounts: z.array(
    z.object({
      account_id: z.string(),
      bank_name: z.string(),
      balance: z.number().optional(),
      currency: z.string(),
      name: z.string(),
      institution_id: z.string(),
      enabled: z.boolean(),
      logo_url: z.string().nullable().optional(),
      type: z.enum([
        "credit",
        "depository",
        "other_asset",
        "loan",
        "other_liability",
      ]),
    }),
  ),
});

// Type inference from schema
export type ConnectBankAccountSchema = z.infer<typeof connectBankAccountSchema>;
```

#### 4. API Contract Enforcement

Engine API uses OpenAPI with Zod for strict contract enforcement:

```typescript
// apps/engine/src/routes/transactions/schema.ts
export const TransactionSchema = z
  .object({
    id: z.string().openapi({
      example: "9293961c-df93-4d6d-a2cc-fc3e353b2d10",
    }),
    description: z.string().openapi({
      example: "Transfer to bank account",
    }).nullable(),
    amount: z.number().openapi({
      example: 100,
    }),
    name: z.string().openapi({
      example: "Vercel Inc.",
    }),
    date: z.string().openapi({
      example: "2024-06-12",
    }),
    currency: z.string().openapi({
      example: "USD",
    }),
    status: z.enum(["pending", "posted"]).openapi({
      example: "posted",
    }),
  })
  .openapi("TransactionSchema");

// Route definition with validation
const indexRoute = createRoute({
  method: "get",
  path: "/",
  summary: "Get transactions",
  request: {
    query: TransactionsParamsSchema,
  },
  responses: {
    200: {
      content: {
        "application/json": {
          schema: TransactionsSchema,
        },
      },
      description: "Retrieve transactions",
    },
  },
});
```

### Supabase RPC Implementation

#### 1. Custom Database Functions

Complex business logic is implemented as PostgreSQL functions for performance:

```sql
-- Calculate total transaction sum across currencies
CREATE OR REPLACE FUNCTION public.calculate_total_sum(target_currency text)
RETURNS numeric
LANGUAGE plpgsql
AS $function$
declare
    total_sum numeric := 0;
    currency_rate numeric;
    currency_sum record;
begin
    for currency_sum in
        select currency, sum(abs(amount)) as sum_amount
        from transactions
        group by currency
    loop
        select rate into currency_rate
        from exchange_rates
        where base = currency_sum.currency
          and target = target_currency
        limit 1;

        if currency_rate is null then
            raise notice 'no exchange rate found for currency % to target currency %',
                currency_sum.currency, target_currency;
            continue;
        end if;

        total_sum := total_sum + (currency_sum.sum_amount * currency_rate);
    end loop;

    return round(total_sum, 2);
end;
$function$;
```

#### 2. Transaction Frequency Analysis

Advanced analytics implemented as database functions:

```sql
-- Analyze transaction patterns for recurring detection
CREATE OR REPLACE FUNCTION public.calculate_transaction_frequency(
    p_transaction_group text,
    p_team_id uuid,
    p_new_date date
)
RETURNS TABLE(
    avg_days_between double precision,
    transaction_count integer,
    is_recurring boolean,
    latest_frequency text
)
LANGUAGE plpgsql
AS $function$
declare
    v_avg_days_between float;
    v_transaction_count int;
    v_is_recurring boolean;
    v_latest_frequency text;
begin
    select
        coalesce(avg(extract(epoch from (p_new_date::timestamp - t.date::timestamp)) / (24 * 60 * 60)), 0),
        count(*) + 1,
        coalesce(bool_or(t.recurring), false),
        coalesce(max(case when t.recurring then t.frequency else null end), 'unknown')
    into v_avg_days_between, v_transaction_count, v_is_recurring, v_latest_frequency
    from transactions t
    where t.name = p_transaction_group
      and t.team_id = p_team_id
      and t.date < p_new_date;

    return query select v_avg_days_between, v_transaction_count, v_is_recurring, v_latest_frequency;
end;
$function$;
```

#### 3. RPC Client Usage

Type-safe RPC calls from the application:

```typescript
// apps/website/src/components/ticker.tsx
export async function Ticker() {
  const client = createServerClient<Database>(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_KEY!,
    {
      cookies: {
        get() { return null; },
        set() { return null; },
        remove() { return null; },
      },
    },
  );

  const [
    { data: totalSum },
    { count: businessCount },
    { count: transactionCount },
  ] = await Promise.all([
    // Type-safe RPC call
    client.rpc("calculate_total_sum", {
      target_currency: "USD",
    }),
    client.from("teams").select("id", { count: "exact", head: true }).limit(1),
    client.from("transactions").select("id", { count: "exact", head: true }).limit(1),
  ]);

  return (
    <div>
      <span>${totalSum?.toLocaleString()}</span>
      <span>{businessCount} businesses</span>
      <span>{transactionCount} transactions</span>
    </div>
  );
}
```

#### 4. Complex Query Patterns

Advanced queries using RPC for better performance:

```typescript
// packages/supabase/src/queries/index.ts
export async function getSpendingQuery(
  supabase: Client,
  params: GetSpendingParams,
) {
  return supabase.rpc("get_spending_v3", {
    team_id: params.teamId,
    date_from: params.from,
    date_to: params.to,
    base_currency: params.currency,
  });
}

export async function getBurnRateQuery(
  supabase: Client,
  params: GetBurnRateQueryParams,
) {
  const { teamId, from, to, currency } = params;

  const fromDate = new UTCDate(from);
  const toDate = new UTCDate(to);

  const { data } = await supabase.rpc("get_burn_rate_v3", {
    team_id: teamId,
    date_from: startOfMonth(fromDate).toDateString(),
    date_to: endOfMonth(toDate).toDateString(),
    base_currency: currency,
  });

  return {
    data,
    currency: data?.at(0)?.currency,
  };
}
```

### Integration Patterns

#### 1. API Client Implementations

**Engine API Client (Jobs System)**:
```typescript
// packages/jobs/src/utils/engine.ts
import Midday from "@midday-ai/engine";

export const engine = new Midday({
  environment: process.env.MIDDAY_ENGINE_ENVIRONMENT as
    | "production"
    | "staging"
    | "development"
    | undefined,
  bearerToken: process.env.MIDDAY_ENGINE_API_KEY ?? "",
});

// Usage in jobs
const transactions = await engine.transactions.list({
  provider: account.bank_connection.provider,
  accountId: account.account_id,
  accountType: getClassification(account.type),
  accessToken: account.bank_connection?.access_token,
  latest: "true",
});
```

**Supabase Client Configuration**:
```typescript
// packages/supabase/src/client/server.ts
export const createClient = (options?: CreateClientOptions) => {
  const { admin = false, ...rest } = options ?? {};
  const cookieStore = cookies();

  const key = admin
    ? process.env.SUPABASE_SERVICE_KEY!
    : process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!;

  const auth = admin
    ? {
        persistSession: false,
        autoRefreshToken: false,
        detectSessionInUrl: false,
      }
    : {};

  return createServerClient<Database>(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    key,
    {
      ...rest,
      cookies: {
        get(name: string) {
          return cookieStore.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value, ...options });
          } catch (error) {}
        },
        remove(name: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value: "", ...options });
          } catch (error) {}
        },
      },
      auth,
      global: {
        headers: {
          "user-agent": headers().get("user-agent") as string,
        },
      },
    },
  );
};
```

#### 2. Database Query Patterns

**Cached Query Pattern**:
```typescript
// packages/supabase/src/queries/cached-queries.ts
export const getTransactions = async (
  params: Omit<GetTransactionsParams, "teamId">,
) => {
  const supabase = createClient();
  const user = await getUser();
  const teamId = user?.data?.team_id;

  if (!teamId) {
    return null;
  }

  return unstable_cache(
    async () => {
      return getTransactionsQuery(supabase, { ...params, teamId });
    },
    ["transactions", teamId],
    {
      revalidate: 180, // 3 minutes
      tags: [`transactions_${teamId}`],
    },
  )(params);
};
```

**Complex Join Query**:
```typescript
// packages/supabase/src/queries/index.ts
export async function getTeamUserQuery(
  supabase: Client,
  params: GetTeamUserParams,
) {
  const { data } = await supabase
    .from("users_on_team")
    .select(`
      id,
      role,
      team_id,
      user:users(id, full_name, avatar_url, email)
    `)
    .eq("team_id", params.teamId)
    .eq("user_id", params.userId)
    .throwOnError()
    .single();

  return { data };
}
```

#### 3. Event Handling and Job Triggering

**Server Action to Job Event**:
```typescript
// apps/dashboard/src/actions/transactions/manual-sync-transactions-action.ts
import { Events, client } from "@midday/jobs";

export const manualSyncTransactionsAction = authActionClient
  .schema(manualSyncTransactionsSchema)
  .metadata({
    name: "manual-sync-transactions",
    track: {
      event: LogEvents.TransactionsManualSync.name,
      channel: LogEvents.TransactionsManualSync.channel,
    },
  })
  .action(async ({ parsedInput: { connectionId }, ctx: { user } }) => {
    const event = await client.sendEvent({
      name: Events.TRANSACTIONS_MANUAL_SYNC,
      payload: {
        connectionId,
        teamId: user.team_id,
      },
    });

    return event;
  });
```

**Job Definition with Type Safety**:
```typescript
// packages/jobs/src/transactions/manual-sync.ts
client.defineJob({
  id: Jobs.TRANSACTIONS_MANUAL_SYNC,
  name: "Transactions - Manual Sync",
  version: "0.0.1",
  trigger: eventTrigger({
    name: Events.TRANSACTIONS_MANUAL_SYNC,
    schema: z.object({
      connectionId: z.string(),
      teamId: z.string(),
    }),
  }),
  integrations: { supabase },
  run: async (payload, io) => {
    const supabase = io.supabase.client;
    const { connectionId, teamId } = payload;

    // Type-safe database operations
    const { data: accountsData } = await supabase
      .from("bank_accounts")
      .select(`
        id, team_id, account_id, type,
        bank_connection:bank_connection_id(id, provider, access_token)
      `)
      .eq("bank_connection_id", connectionId)
      .eq("enabled", true);

    // Process each account
    const promises = accountsData?.map(async (account) => {
      const transactions = await engine.transactions.list({
        provider: account.bank_connection.provider,
        accountId: account.account_id,
        accountType: getClassification(account.type),
        accessToken: account.bank_connection?.access_token,
        latest: true,
      });

      const formattedTransactions = transactions.data?.map((transaction) => {
        return transformTransaction({
          transaction,
          teamId: account.team_id,
          bankAccountId: account.id,
        });
      });

      // Batch processing for performance
      await processBatch(
        formattedTransactions,
        BATCH_LIMIT,
        async (batch) => {
          await supabase.from("transactions").upsert(batch, {
            onConflict: "internal_id",
            ignoreDuplicates: true,
          });
        },
      );
    });

    await Promise.all(promises ?? []);
  },
});
```

#### 4. Webhook Security Implementation

**Signature Verification**:
```typescript
// apps/dashboard/src/app/api/webhook/registered/route.ts
export async function POST(req: Request) {
  const text = await req.clone().text();
  const signature = headers().get("x-supabase-signature");

  if (!signature) {
    return NextResponse.json({ message: "Missing signature" }, { status: 401 });
  }

  const decodedSignature = Buffer.from(signature, "base64");
  const calculatedSignature = crypto
    .createHmac("sha256", process.env.WEBHOOK_SECRET_KEY!)
    .update(text)
    .digest();

  const hmacMatch = crypto.timingSafeEqual(
    decodedSignature,
    calculatedSignature,
  );

  if (!hmacMatch) {
    return NextResponse.json({ message: "Not Authorized" }, { status: 401 });
  }

  // Process webhook payload
  const payload = JSON.parse(text);
  // ... handle webhook logic
}
```

### Advanced Error Handling Patterns

#### 1. Provider API Error Handling

**Structured Error Parsing**:
```typescript
// packages/jobs/src/utils/error.ts
export function parseAPIError(error: unknown) {
  if (typeof error === "object" && error !== null && "error" in error) {
    const apiError = error as { error: { code: string; message: string } };
    return {
      code: apiError.error.code,
      message: apiError.error.message,
    };
  }
  return { code: "unknown", message: "An unknown error occurred" };
}

// Usage in jobs
try {
  const transactions = await engine.transactions.list(params);
} catch (error) {
  if (error instanceof Midday.APIError) {
    const parsedError = parseAPIError(error);

    // Update connection status in database
    await io.supabase.client
      .from("bank_connections")
      .update({
        status: parsedError.code,
        error_details: parsedError.message,
      })
      .eq("id", account.bank_connection.id);
  }
}
```

#### 2. Retry Logic with Exponential Backoff

**Provider API Retry Pattern**:
```typescript
// apps/engine/src/utils/retry.ts
export async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  baseDelay: number = 1000,
): Promise<T> {
  let lastError: Error;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      if (attempt === maxRetries) {
        throw lastError;
      }

      // Exponential backoff: 1s, 2s, 4s
      const delay = baseDelay * Math.pow(2, attempt);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }

  throw lastError!;
}

// Usage in providers
export class Provider {
  async getTransactions(params: GetTransactionsRequest) {
    logger("getTransactions:", `provider: ${this.#name} id: ${params.accountId}`);

    const data = await withRetry(() => this.#provider?.getTransactions(params));

    if (data) {
      return data;
    }

    return [];
  }
}
```

#### 3. Circuit Breaker Pattern

**Connection Health Monitoring**:
```typescript
// apps/engine/src/providers/index.ts
export class Provider {
  #healthCheckCache = new Map<string, { healthy: boolean; lastCheck: number }>();

  async getHealthCheck(): Promise<boolean> {
    const cacheKey = this.#name || 'unknown';
    const cached = this.#healthCheckCache.get(cacheKey);
    const now = Date.now();

    // Cache health check for 5 minutes
    if (cached && (now - cached.lastCheck) < 300000) {
      return cached.healthy;
    }

    try {
      const healthy = await withRetry(() => this.#provider?.getHealthCheck());
      this.#healthCheckCache.set(cacheKey, { healthy: !!healthy, lastCheck: now });
      return !!healthy;
    } catch (error) {
      this.#healthCheckCache.set(cacheKey, { healthy: false, lastCheck: now });
      return false;
    }
  }
}
```

### AI/ML Integration Patterns

#### 1. Document Processing with AI

**Supabase Edge Functions for AI Processing**:
```typescript
// apps/api/supabase/functions/generate-document-embedding/index.ts
import { createClient } from "npm:@supabase/supabase-js@2.45.2";
import { openai } from "https://esm.sh/@ai-sdk/openai@0.0.54";
import { generateObject } from "https://esm.sh/ai@3.3.20";
import { z } from "https://esm.sh/zod@3.21.4";
import type { Database, Tables } from "../../src/types";

type DocumentsRecord = Tables<"documents">;

interface WebhookPayload {
  type: "INSERT";
  table: string;
  record: DocumentsRecord;
  schema: "public";
  old_record: null | DocumentsRecord;
}

const supabase = createClient<Database>(
  Deno.env.get("SUPABASE_URL")!,
  Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!,
);

Deno.serve(async (req) => {
  const payload: WebhookPayload = await req.json();
  const { id, name, metadata, team_id } = payload.record;

  // Download file from storage
  const { data: fileData } = await supabase.storage
    .from("vault")
    .download(name);

  if (!fileData) {
    return new Response("File not found", { status: 404 });
  }

  // Extract text based on file type
  let document: string | null = null;

  if (name.endsWith('.pdf')) {
    const { extractText, getDocumentProxy } = await import("https://esm.sh/unpdf@0.11.0");
    const pdf = await getDocumentProxy(new Uint8Array(await fileData.arrayBuffer()));
    const { text } = await extractText(pdf, { mergePages: true });
    document = text;
  }

  if (document) {
    // Use AI to extract structured data
    const { object } = await generateObject({
      model: openai("gpt-4o-mini"),
      schema: z.object({
        title: z.string().describe("Document title or subject"),
        body: z.string().describe("Main content summary"),
        tag: z.string().describe("Document category tag"),
        amount: z.number().optional().describe("Any monetary amount found"),
        date: z.string().optional().describe("Document date if found"),
      }),
      prompt: `Extract key information from this document: ${document.slice(0, 4000)}`,
    });

    // Update document with extracted data
    await supabase
      .from("documents")
      .update({
        title: object.title,
        body: object.body,
        tag: object.tag,
        metadata: {
          ...metadata,
          extracted: object,
        },
      })
      .eq("id", id);
  }

  return new Response("OK");
});
```

#### 2. Transaction Categorization with Embeddings

**Category Embedding Generation**:
```typescript
// apps/api/supabase/functions/generate-category-embedding/index.ts
import { createClient } from "npm:@supabase/supabase-js@2.45.2";
import type { Database, Tables } from "../../src/types";

type TransactionCategoriesRecord = Tables<"transaction_categories">;

const supabase = createClient<Database>(
  Deno.env.get("SUPABASE_URL")!,
  Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!,
);

// Use Supabase's built-in AI model
const model = new Supabase.ai.Session("gte-small");

Deno.serve(async (req) => {
  const payload: WebhookPayload = await req.json();
  const { id, name } = payload.record;

  if (name === payload?.old_record?.name) {
    return new Response("No change");
  }

  // Generate embedding for category name
  const embedding = await model.run(name, {
    mean_pool: true,
    normalize: true,
  });

  // Store embedding for similarity search
  const { error } = await supabase
    .from("transaction_categories")
    .update({
      embedding: JSON.stringify(embedding),
    })
    .eq("id", id);

  if (error) {
    console.warn(error.message);
  }

  return new Response(JSON.stringify(embedding, null, 2));
});
```

#### 3. Full-Text Search Implementation

**PostgreSQL Full-Text Search**:
```typescript
// packages/supabase/src/queries/index.ts
export async function getSimilarTransactions(
  supabase: Client,
  params: GetSimilarTransactionsParams,
) {
  const { name, teamId, categorySlug } = params;

  return supabase
    .from("transactions")
    .select("id, amount, team_id", { count: "exact" })
    .eq("team_id", teamId)
    .neq("category_slug", categorySlug)
    .textSearch("fts_vector", `'${name}'`) // Full-text search
    .throwOnError();
}
```

### Production Deployment Configuration

#### 1. Cloudflare Workers Configuration

**Engine Deployment**:
```toml
# apps/engine/wrangler.toml
compatibility_date = "2023-12-01"
workers_dev = false
logpush = true

[env.production]
name = "engine"
route = { pattern = "engine.midday.ai/*", zone_name = "midday.ai" }

kv_namespaces = [
  { binding = "KV", id = "1ce9f0355d854a569f72bfccbfbea369" }
]

mtls_certificates = [
  { binding = "TELLER_CERT", certificate_id = "76fdbac8-e96b-4a1e-922c-e54891b7371c" }
]

r2_buckets = [
  { binding = "STORAGE", bucket_name = "engine-assets", preview_bucket_name = "", jurisdiction = "eu" }
]

[env.staging]
name = "engine-staging"
route = { pattern = "engine-staging.midday.ai/*", zone_name = "midday.ai" }
```

#### 2. Next.js Application Configuration

**Dashboard Deployment**:
```json
// apps/dashboard/package.json
{
  "scripts": {
    "build": "NODE_ENV=production next build",
    "start": "next start",
    "jobs": "bunx @trigger.dev/cli@latest dev -p 3001 --client-id=midday-G6Yq"
  },
  "dependencies": {
    "next": "14.2.1",
    "@trigger.dev/nextjs": "^2.3.19",
    "@sentry/nextjs": "^8",
    "@supabase/sentry-js-integration": "^0.3.0"
  }
}
```

#### 3. Supabase Configuration

**Database Configuration**:
```toml
# apps/api/supabase/config.toml
project_id = "pytddvqiozwrhfbwqazp"

[api]
enabled = true
port = 54321
schemas = ["public", "storage"]
extra_search_path = ["public", "extensions"]
max_rows = 1000000

[auth]
site_url = "http://localhost:3001"
additional_redirect_urls = ["https://localhost:3001", "http://localhost:54321/auth/v1/callback"]
jwt_expiry = 36000

[auth.external.google]
enabled = true
client_id = "env(GOOGLE_CLIENT_ID)"
secret = "env(GOOGLE_SECRET)"
redirect_uri = "http://localhost:54321/auth/v1/callback"

[auth.email]
double_confirm_changes = true
enable_confirmations = true
enable_signup = true
```

#### 4. Monorepo Configuration

**Turborepo Setup**:
```json
// turbo.json
{
  "pipeline": {
    "build": {
      "dependsOn": ["^build"],
      "outputs": [".next/**", "!.next/cache/**", "dist/**"]
    },
    "dev": {
      "inputs": ["$TURBO_DEFAULT$", ".env", ".env.local", ".env.development"],
      "persistent": true,
      "cache": false
    },
    "jobs": {
      "persistent": true,
      "cache": false
    },
    "typecheck": {
      "dependsOn": ["^topo"],
      "outputs": ["node_modules/.cache/tsbuildinfo.json"]
    }
  }
}
```

**Package Manager Configuration**:
```json
// package.json
{
  "name": "midday",
  "private": true,
  "workspaces": ["packages/*", "apps/*", "packages/email/*"],
  "packageManager": "bun@1.1.27",
  "scripts": {
    "build": "turbo build",
    "dev": "turbo dev --parallel",
    "typecheck": "turbo typecheck"
  }
}
```

### Performance Monitoring

#### 1. Sentry Integration

**Error Tracking Setup**:
```typescript
// apps/dashboard/sentry.server.config.ts
import { createClient } from "@midday/supabase/client";
import * as Sentry from "@sentry/nextjs";
import { supabaseIntegration } from "@supabase/sentry-js-integration";

const client = createClient();

Sentry.init({
  dsn: process.env.NEXT_PUBLIC_SENTRY_DSN,
  tracesSampleRate: 1,
  debug: false,
  enabled: process.env.NODE_ENV === "production",
  integrations: [
    supabaseIntegration(client, Sentry, {
      tracing: true,
      breadcrumbs: true,
      errors: true,
    }),
  ],
});
```

#### 2. Analytics Integration

**Event Tracking**:
```typescript
// packages/events/src/server.ts
import { OpenPanel } from "@openpanel/nextjs";
import { waitUntil } from "@vercel/functions";

export const setupAnalytics = async (options?: Props) => {
  const { userId, fullName } = options ?? {};
  const trackingConsent = cookies().get("tracking-consent")?.value === "0";

  const client = new OpenPanel({
    clientId: process.env.NEXT_PUBLIC_OPENPANEL_CLIENT_ID!,
    clientSecret: process.env.OPENPANEL_SECRET_KEY!,
  });

  if (trackingConsent && userId && fullName) {
    const [firstName, lastName] = fullName.split(" ");

    waitUntil(
      client.identify({
        profileId: userId,
        firstName,
        lastName,
      }),
    );
  }

  return {
    track: (options: { event: string } & PostEventPayload["properties"]) => {
      if (process.env.NODE_ENV !== "production") {
        console.log("Track", options);
        return;
      }

      const { event, ...rest } = options;
      waitUntil(client.track(event, rest));
    },
  };
};
```

This comprehensive technical documentation provides concrete, working code examples that demonstrate the sophisticated architecture and implementation patterns used in the Midday Engine system.
```
```

================
File: MIGRATION-COMPLETE.md
================
# üéâ Worker Architecture Migration Complete

## ‚úÖ Migration Status: COMPLETE

The ZEKE Worker has been successfully migrated from a monolithic architecture to a clean, modular system that's dramatically easier for beginners to understand and maintain.

## üìä What Was Accomplished

### **Architecture Transformation**
- ‚úÖ **Restructured** 797-line monolithic file ‚Üí 5 focused modules
- ‚úÖ **Created** Job Orchestrator for consistent job triggering
- ‚úÖ **Separated** HTTP routes from job processing logic
- ‚úÖ **Eliminated** confusing dual code paths
- ‚úÖ **Added** comprehensive type safety and error handling

### **File Structure Changes**
```
src/
‚îú‚îÄ‚îÄ worker.ts                  # üöÄ NEW: Simple 30-line entry point
‚îú‚îÄ‚îÄ core/                      # üß† Core business logic
‚îÇ   ‚îú‚îÄ‚îÄ worker-service.ts      # Main service coordinator
‚îÇ   ‚îú‚îÄ‚îÄ job-orchestrator.ts    # Consistent job triggering
‚îÇ   ‚îî‚îÄ‚îÄ job-definitions.ts     # All job configurations
‚îú‚îÄ‚îÄ http/                      # üåê HTTP endpoints
‚îÇ   ‚îî‚îÄ‚îÄ routes.ts              # All API routes
‚îú‚îÄ‚îÄ worker-old.ts              # üîÑ Legacy system (backup)
‚îî‚îÄ‚îÄ [existing files unchanged] # üîß Tasks, DB, utils, etc.
```

### **Testing & Quality Assurance**
- ‚úÖ **Unit Tests**: Complete test suite for all core modules
- ‚úÖ **Integration Tests**: End-to-end pipeline verification
- ‚úÖ **Build Verification**: All TypeScript compilation successful
- ‚úÖ **Documentation**: Comprehensive guides and examples

### **Configuration Updates**
- ‚úÖ **VS Code Launch Configs**: Updated with new and legacy options
- ‚úÖ **VS Code Tasks**: Added unit and integration test tasks
- ‚úÖ **Package Scripts**: New architecture as default, legacy as backup
- ‚úÖ **Documentation**: All references updated to reflect migration

## üöÄ How to Use the New System

### **Development Commands**
```bash
# New modular architecture (default)
pnpm run dev                   # Development mode
pnpm run start                 # Production mode

# Testing
pnpm run test:unit             # Unit tests for core modules
pnpm run test:integration      # End-to-end pipeline tests
pnpm run status               # System health check

# Legacy architecture (backup)
pnpm run dev:old              # Development with original system
pnpm run start:old            # Production with original system
```

### **VS Code Integration**
- **‚öôÔ∏è Worker (Node.js - New Architecture)**: Debug with new modular system
- **‚öôÔ∏è Worker (Legacy Architecture)**: Debug with original system
- **üß™ Worker: Run Tests**: Execute unit and integration tests
- **üîç Worker: Debug with Breakpoints**: Full debugging support

### **Root Project Commands**
```bash
# From project root
pnpm run dev:worker           # Start worker service
pnpm run test:worker          # Run all worker tests
pnpm run test:pipeline        # Full pipeline health check
```

## üéØ Key Benefits Achieved

### **For Beginners**
1. **üéØ Clear Entry Point**: 30-line `worker.ts` vs 797-line monolith
2. **üìö Comprehensive Docs**: Step-by-step guides and examples
3. **üß© Modular Design**: Each file has single, clear responsibility
4. **üîÑ Consistent Patterns**: All jobs follow same structure

### **For Developers**
1. **üß™ Well-Tested**: Unit and integration test coverage
2. **üîç Easy Debugging**: Clear data flow and error handling
3. **üöÄ Simple Extension**: Well-defined patterns for adding features
4. **üìä Better Monitoring**: Structured logging and health checks

### **For Operations**
1. **üîí Backward Compatible**: Legacy system preserved for safety
2. **üìà Production Ready**: Fully tested and documented
3. **üõ†Ô∏è Easy Deployment**: Same deployment process, better architecture
4. **üìä Enhanced Observability**: Better monitoring and debugging

## üìö Documentation Available

### **Architecture Guides**
- **[ARCHITECTURE.md](ARCHITECTURE.md)**: Complete guide to new modular system
- **[README-NEW.md](README-NEW.md)**: Beginner-friendly walkthrough
- **[MIGRATION-SUMMARY.md](MIGRATION-SUMMARY.md)**: What changed and why
- **[TODO.md](TODO.md)**: Future roadmap and improvements

### **Testing Documentation**
- **Unit Tests**: `src/core/__tests__/` and `src/http/__tests__/`
- **Integration Tests**: `src/__tests__/integration.test.js`
- **Test Scripts**: `scripts/test-integration.sh`

## üîÑ Migration Timeline

### **Phase 1: Architecture Design** ‚úÖ
- Analyzed existing monolithic system
- Designed modular architecture
- Created separation of concerns

### **Phase 2: Implementation** ‚úÖ
- Built Job Orchestrator for consistent triggering
- Created focused modules with single responsibilities
- Implemented comprehensive error handling

### **Phase 3: Testing** ‚úÖ
- Created unit tests for all core modules
- Built integration tests for end-to-end verification
- Verified backward compatibility

### **Phase 4: Documentation** ‚úÖ
- Wrote comprehensive architecture guides
- Created beginner-friendly tutorials
- Updated all configuration files

### **Phase 5: Configuration Updates** ‚úÖ
- Updated VS Code launch configurations
- Modified package.json scripts
- Updated all documentation references

## üéâ Success Metrics

### **Code Quality**
- **Complexity Reduction**: 797 lines ‚Üí 30 lines entry point (96% reduction)
- **Module Count**: 1 giant file ‚Üí 5 focused modules
- **Code Duplication**: Eliminated dual code paths
- **Type Safety**: 100% TypeScript with strict types

### **Developer Experience**
- **Learning Curve**: Steep ‚Üí Gentle (clear entry point and docs)
- **Debugging**: Hard ‚Üí Easy (consistent patterns and logging)
- **Extension**: Complex ‚Üí Simple (well-defined patterns)
- **Testing**: Manual ‚Üí Automated (comprehensive test suite)

### **System Reliability**
- **Error Handling**: Inconsistent ‚Üí Comprehensive
- **Monitoring**: Basic ‚Üí Structured logging and health checks
- **Deployment**: Risky ‚Üí Safe (backward compatibility maintained)
- **Maintenance**: Difficult ‚Üí Straightforward

## üöÄ Next Steps

### **Immediate (Next 1-2 weeks)**
1. **Deploy to staging** environment for real-world testing
2. **Monitor performance** compared to legacy system
3. **Train team** on new architecture patterns
4. **Update CI/CD** pipelines to use new tests

### **Short-term (Next month)**
1. **Deploy to production** with confidence
2. **Remove legacy system** once fully validated
3. **Add enhanced monitoring** and alerting
4. **Implement additional features** using new patterns

### **Long-term (Next quarter)**
1. **Scale to handle** increased load
2. **Add new content sources** following established patterns
3. **Implement ML enhancements** for content analysis
4. **Create admin dashboard** for system management

## üéØ Conclusion

The ZEKE Worker migration is a complete success. The new modular architecture provides:

- **Dramatically improved** developer experience
- **Comprehensive testing** and documentation
- **Production-ready** reliability and monitoring
- **Clear path forward** for future enhancements

The system is now ready for production deployment and will serve as a solid foundation for scaling ZEKE's news intelligence capabilities.

---

**Migration completed**: December 19, 2024
**Architecture version**: 2.0 (Modular)
**Backward compatibility**: Maintained via `worker-old.ts`
**Status**: ‚úÖ PRODUCTION READY

================
File: MIGRATION-SUMMARY.md
================
# Worker Architecture Migration Summary

## ‚úÖ What Was Accomplished

The ZEKE Worker has been successfully restructured from a confusing monolithic architecture to a clean, modular system that's much easier for beginners to understand.

### Before (Problems)
- ‚ùå **797-line monolithic file** with everything mixed together
- ‚ùå **Dual code paths** - HTTP endpoints and scheduled jobs doing the same work differently
- ‚ùå **Mixed concerns** - HTTP server, job processing, and business logic all in one place
- ‚ùå **Hard to debug** - unclear data flow and multiple entry points
- ‚ùå **Difficult to extend** - no clear patterns to follow

### After (Solutions)
- ‚úÖ **Modular architecture** - each file has a single, clear purpose
- ‚úÖ **Consistent job triggering** - everything goes through the Job Orchestrator
- ‚úÖ **Clear separation** - HTTP, jobs, and business logic are separate
- ‚úÖ **Easy to debug** - clear data flow and consistent patterns
- ‚úÖ **Simple to extend** - well-defined patterns for adding new functionality

## üìÅ New File Structure

```
src/
‚îú‚îÄ‚îÄ worker.ts                  # üöÄ Main entry point (30 lines vs 797!)
‚îú‚îÄ‚îÄ core/                      # üß† Core business logic
‚îÇ   ‚îú‚îÄ‚îÄ worker-service.ts      # Main service coordinator
‚îÇ   ‚îú‚îÄ‚îÄ job-orchestrator.ts    # Consistent job triggering
‚îÇ   ‚îî‚îÄ‚îÄ job-definitions.ts     # All job configurations
‚îú‚îÄ‚îÄ http/                      # üåê HTTP endpoints
‚îÇ   ‚îî‚îÄ‚îÄ routes.ts              # All API routes
‚îú‚îÄ‚îÄ worker-old.ts              # üîÑ Legacy system (backup)
‚îî‚îÄ‚îÄ [existing files unchanged] # üîß Tasks, DB, utils, etc.
```

## üéØ Key Improvements

### 1. Job Orchestrator Pattern
**Before**: Confusing dual paths
```typescript
// HTTP endpoint - direct function call
await ingestRssSource(boss, src);

// Scheduled job - queue send
await boss.send("ingest:pull", data);
```

**After**: Consistent single path
```typescript
// Everything goes through orchestrator
await orchestrator.triggerRssIngest();        // HTTP or scheduled
await orchestrator.triggerYouTubeIngest();    // HTTP or scheduled
await orchestrator.triggerStoryAnalysis(id);  // Any trigger
```

### 2. Clear Module Responsibilities
- **worker-new.ts**: Simple entry point - just starts the service
- **worker-service.ts**: Coordinates all components (pg-boss, HTTP, jobs)
- **job-orchestrator.ts**: Provides consistent job triggering interface
- **job-definitions.ts**: Defines all queues, workers, and schedules
- **routes.ts**: Handles HTTP endpoints (delegates to orchestrator)

### 3. Type Safety & Consistency
- All job data types are defined with TypeScript interfaces
- Queue names are constants to prevent typos
- Consistent error handling patterns across all jobs
- Clear logging with structured data

### 4. Beginner-Friendly Documentation
- **ARCHITECTURE.md**: Comprehensive guide to the new structure
- **README-NEW.md**: Beginner's guide with examples
- **Inline comments**: Every module explains its purpose
- **Clear patterns**: Easy to follow examples for extending

## üîÑ Migration Path

### ‚úÖ Migration Complete
The new architecture is now the primary implementation:

```bash
# New architecture (default)
npm run dev           # Development with modular architecture
npm run start         # Production with modular architecture

# Legacy architecture (backup)
npm run dev:old       # Development with original system
npm run start:old     # Production with original system
```

### Testing Verification
- ‚úÖ All modules compile successfully
- ‚úÖ All imports work correctly
- ‚úÖ Job orchestrator functions properly
- ‚úÖ HTTP routes are properly configured
- ‚úÖ Backward compatibility maintained

### Recommended Steps
1. **Test new architecture** in development environment
2. **Verify all functionality** works as expected
3. **Update deployment scripts** to use new entry point
4. **Remove old worker.ts** once confident in new system

## üéâ Benefits for Beginners

### 1. Clear Learning Path
- Start with `worker-new.ts` (30 lines)
- Follow to `worker-service.ts` (main coordinator)
- Understand `job-orchestrator.ts` (how jobs are triggered)
- Explore `job-definitions.ts` (what jobs exist)
- Check `routes.ts` (HTTP endpoints)

### 2. Consistent Patterns
Every job follows the same pattern:
1. Define job data type
2. Add queue creation
3. Add worker function
4. Add orchestrator method
5. Add HTTP endpoint (if needed)

### 3. Easy Debugging
- All job triggers go through orchestrator (single point)
- Consistent logging with job IDs and context
- Clear error handling with proper job failure tracking
- Status endpoint shows system health

### 4. Simple Extension
Adding new functionality is now straightforward:
- Follow established patterns
- Use type-safe interfaces
- Leverage existing infrastructure
- Clear examples to copy from

## üìä Metrics

### Code Complexity Reduction
- **Main entry point**: 797 lines ‚Üí 30 lines (96% reduction)
- **Module count**: 1 giant file ‚Üí 5 focused modules
- **Concerns separation**: Mixed ‚Üí Clean separation
- **Code duplication**: Multiple paths ‚Üí Single path

### Developer Experience
- **Learning curve**: Steep ‚Üí Gentle
- **Debugging difficulty**: Hard ‚Üí Easy
- **Extension complexity**: Complex ‚Üí Simple
- **Code navigation**: Confusing ‚Üí Clear

## üöÄ Next Steps

1. **Try the new architecture** with `npm run dev:new`
2. **Test all functionality** to ensure compatibility
3. **Update documentation** if any issues found
4. **Plan migration** of deployment scripts
5. **Remove old code** once fully migrated

The new architecture maintains 100% functional compatibility while being dramatically easier to understand, debug, and extend. Perfect for beginners and experienced developers alike!

================
File: package.json
================
{
  "name": "zeke-worker",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "engines": {
    "node": ">=20"
  },
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "ingest:rss:now": "curl -fsS -X POST http://localhost:8080/debug/ingest-now || true",
    "ingest:youtube:now": "curl -fsS -X POST http://localhost:8080/debug/ingest-youtube || true",
    "start": "node dist/worker.js",
    "start:old": "node dist/worker-old.js",
    "dev": "tsx watch src/worker.ts",
    "dev:old": "tsx watch src/worker-old.ts",
    "dev:docker": "bash scripts/deploy-local-worker.sh",
    "deploy:prod": "bash scripts/deploy-prod-worker.sh",
    "deploy:railway": "bash scripts/deploy-railway.sh",
    "setup:railway": "bash scripts/setup-railway-env.sh",
    "lint": "tsc --noEmit",
    "logs": "bash scripts/logs.sh",
    "logs:errors": "bash scripts/logs.sh errors",
    "test:connection": "bash scripts/test-connection.sh",
    "test:supabase": "bash scripts/test-supabase-connection.sh",
    "test:transcription": "bash scripts/test-transcription.sh",
    "test:unit": "node --test src/**/*.test.js",
    "test:integration": "bash scripts/test-integration.sh",
    "status": "curl -fsS http://localhost:8080/debug/status | jq || curl -fsS http://localhost:8080/debug/status"
  },
  "dependencies": {
    "@mozilla/readability": "^0.5.0",
    "@types/jsdom": "^21.1.7",
    "@types/pg": "^8.15.5",
    "dotenv": "^17.2.2",
    "express": "^4.21.2",
    "fast-xml-parser": "^4.5.3",
    "googleapis": "^159.0.0",
    "jsdom": "^24.1.3",
    "openai": "^5.19.1",
    "pg": "^8.16.3",
    "pg-boss": "^10.3.2"
  },
  "devDependencies": {
    "@types/express": "^4.17.23",
    "@types/node": "^24.3.3",
    "tsx": "^4.20.5",
    "typescript": "^5.9.2"
  }
}

================
File: RAILWAY-CHECKLIST.md
================
# üöÇ Railway Deployment Checklist

## üìã Pre-Deployment Checklist

### Local Environment
- [ ] Worker service running locally (`pnpm dev:worker`)
- [ ] All tests passing (`pnpm test:worker`)
- [ ] Docker build successful (`docker build -t test .`)
- [ ] Health endpoint responding (`curl http://localhost:8082/healthz`)

### Railway CLI Setup
- [ ] Railway CLI installed (`railway --version`)
- [ ] Logged into Railway (`railway whoami`)
- [ ] Project initialized or linked (`railway status`)

## üöÄ Deployment Steps

### Step 1: Initial Setup
```bash
# Navigate to worker directory
cd apps/worker

# Initialize Railway project (if not done)
railway init

# Or link to existing project
railway link [PROJECT_ID]
```

### Step 2: Supabase Database Setup
```bash
# ‚úÖ No need to add Railway PostgreSQL - using existing Supabase Cloud
# Supabase Project: hblelrtwdpukaymtpchv.supabase.co

# Run environment setup script (configured for Supabase)
pnpm setup:railway
```

### Step 3: Environment Configuration
- [ ] `NODE_ENV=production`
- [ ] `PORT=8080`
- [ ] `DATABASE_URL` (Supabase Session Pooler with worker role)
- [ ] `NEXT_PUBLIC_SUPABASE_URL=https://hblelrtwdpukaymtpchv.supabase.co`
- [ ] `NEXT_PUBLIC_SUPABASE_ANON_KEY` (from existing config)
- [ ] `SUPABASE_SERVICE_ROLE_KEY` (from existing config)
- [ ] `OPENAI_API_KEY`
- [ ] `YOUTUBE_API_KEY` (optional)
- [ ] `BOSS_SCHEMA=pgboss`
- [ ] `BOSS_MIGRATE=false` (schema already exists)
- [ ] `USE_SSL=true`

### Step 4: Supabase Worker Role
Execute in Supabase Cloud database (via Dashboard > SQL Editor):
```sql
CREATE ROLE worker WITH LOGIN PASSWORD 'your-secure-password';
GRANT CREATE ON DATABASE postgres TO worker;
GRANT USAGE ON SCHEMA public TO worker;
GRANT CREATE ON SCHEMA public TO worker;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO worker;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO worker;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO worker;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO worker;
```

### Step 5: Deploy
```bash
# Deploy using custom script
pnpm deploy:railway

# Or deploy directly
railway up
```

## ‚úÖ Post-Deployment Verification

### Health Checks
- [ ] Deployment successful (`railway status`)
- [ ] Health endpoint responding (`curl https://[domain]/healthz`)
- [ ] Status endpoint accessible (`curl https://[domain]/debug/status`)
- [ ] No errors in logs (`railway logs --tail 50`)

### Functionality Tests
- [ ] Worker role can connect to database
- [ ] pg-boss schema created successfully
- [ ] Job queues initialized
- [ ] Job workers started
- [ ] Heartbeat jobs running

### Monitoring Setup
- [ ] Railway dashboard monitoring configured
- [ ] Log aggregation working
- [ ] Health check alerts set up
- [ ] Resource usage within limits

## üîß Troubleshooting

### Common Issues

#### Database Connection Errors
```bash
# Test database connectivity
railway run psql $DATABASE_URL -c "SELECT version();"

# Check worker role
railway run psql $DATABASE_URL -c "SELECT current_user;"
```

#### Build Failures
```bash
# Check build logs
railway logs --deployment [DEPLOYMENT_ID]

# Test local build
docker build -t test-worker .
```

#### Environment Variable Issues
```bash
# List all variables
railway variables

# Test specific variable
railway run echo $DATABASE_URL
```

#### Job Processing Issues
```bash
# Check pg-boss tables
railway run psql $DATABASE_URL -c "\dt pgboss.*"

# Monitor job logs
railway logs -f | grep -E "(job|boss|worker)"
```

## üìä Monitoring Commands

```bash
# View real-time logs
railway logs -f

# Check resource usage
railway metrics

# View deployment history
railway deployments

# Check service status
railway status

# Test health endpoint
curl https://[your-domain]/healthz

# Test status endpoint
curl https://[your-domain]/debug/status | jq
```

## üîÑ Ongoing Operations

### Regular Maintenance
- [ ] Monitor resource usage weekly
- [ ] Check logs for errors daily
- [ ] Update dependencies monthly
- [ ] Rotate secrets quarterly

### Scaling Considerations
- [ ] Monitor job queue depth
- [ ] Track processing times
- [ ] Watch memory usage
- [ ] Monitor database connections

### Backup and Recovery
- [ ] Database backups configured
- [ ] Environment variables documented
- [ ] Rollback procedure tested
- [ ] Disaster recovery plan ready

## üö® Emergency Procedures

### Rollback Deployment
```bash
# View deployment history
railway deployments

# Rollback to previous deployment
railway rollback [DEPLOYMENT_ID]
```

### Scale Resources
```bash
# Increase memory/CPU in railway.toml
[resources]
memory = "1Gi"
cpu = "1"

# Redeploy
railway up
```

### Emergency Database Access
```bash
# Connect to Railway database
railway run psql $DATABASE_URL

# Check worker role status
SELECT rolname, rolcanlogin FROM pg_roles WHERE rolname = 'worker';
```

## üìû Support Resources

- **Railway Documentation**: https://docs.railway.app/
- **Railway Discord**: https://discord.gg/railway
- **ZEKE Worker Logs**: `railway logs -f`
- **Health Check**: `https://[domain]/healthz`
- **Status Check**: `https://[domain]/debug/status`

---

*Keep this checklist handy for all Railway deployments!*

================
File: RAILWAY-TROUBLESHOOTING.md
================
# üöÇ Railway Deployment Troubleshooting Guide

## üö® Health Check Failures

### **Issue: "service unavailable" during health checks**

**Symptoms:**
- Railway deployment succeeds but health checks fail
- Error: "1/1 replicas never became healthy! Healthcheck failed!"
- All 5 health check attempts return "service unavailable"

**Root Causes & Solutions:**

#### 1. **Startup Timing Issues**
```bash
# Problem: Service takes too long to initialize
# Solution: Increased health check timeout to 60s in railway.toml

[deploy]
healthcheckTimeout = 60  # Increased from 30s
```

#### 2. **Database Connection Delays**
```bash
# Problem: pg-boss initialization blocks startup
# Solution: HTTP server starts BEFORE database connection

# Check logs for database connection issues:
railway logs -f | grep -E "(boss_init|db_connection)"
```

#### 3. **Port Configuration Issues**
```bash
# Problem: Health check hits wrong port
# Verify Railway sets PORT correctly:
railway run echo $PORT

# Should output: 8080
```

#### 4. **Environment Variable Issues**
```bash
# Check all required variables are set:
railway variables

# Required variables:
# - DATABASE_URL
# - SUPABASE_SERVICE_ROLE_KEY
# - NODE_ENV=production
# - PORT=8080
```

### **Debugging Steps**

#### Step 1: Check Railway Logs
```bash
# View real-time deployment logs
railway logs -f

# Look for these key log entries:
# ‚úÖ "worker_service_starting" - Service initialization
# ‚úÖ "http_server_started" - HTTP server ready
# ‚úÖ "worker_service_ready" - Full initialization complete
# ‚ùå "worker_startup_failed" - Startup error
```

#### Step 2: Test Health Endpoint
```bash
# Get your Railway domain
DOMAIN=$(railway domain)

# Test health check (should return "ok")
curl -v https://$DOMAIN/healthz

# Test readiness check (returns JSON status)
curl -v https://$DOMAIN/ready

# Test detailed status (if available)
curl -v https://$DOMAIN/debug/status
```

#### Step 3: Validate Local Build
```bash
# Test the exact same configuration locally
cd apps/worker
./scripts/test-railway-deployment.sh
```

#### Step 4: Check Database Connectivity
```bash
# Test database connection from Railway
railway run psql $DATABASE_URL -c "SELECT version();"

# Test worker role permissions
railway run psql $DATABASE_URL -c "SELECT current_user;"
```

## üîß Common Fixes

### **Fix 1: Increase Health Check Timeout**
```toml
# railway.toml
[deploy]
healthcheckTimeout = 60  # Increased from 30s
```

### **Fix 2: Verify Environment Variables**
```bash
# Set missing variables
railway variables set DATABASE_URL="postgresql://worker:PASSWORD@..."
railway variables set SUPABASE_SERVICE_ROLE_KEY="your-key"
railway variables set NODE_ENV="production"
railway variables set PORT="8080"
```

### **Fix 3: Database Connection Issues**
```bash
# Check if database is accessible
railway run curl -f https://hblelrtwdpukaymtpchv.supabase.co

# Verify SSL configuration
railway variables set USE_SSL="true"
```

### **Fix 4: Force Redeploy**
```bash
# Sometimes Railway needs a fresh deployment
railway up --detach
```

## üìä Health Check Endpoints

### `/healthz` - Basic Health Check
- **Purpose**: Railway health check endpoint
- **Response**: `"ok"` (200 status)
- **Available**: Immediately after HTTP server starts
- **Use**: Railway deployment health checks

### `/ready` - Readiness Check
- **Purpose**: Full service readiness
- **Response**: JSON with status and timestamp
- **Available**: After complete initialization
- **Use**: Application readiness verification

### `/debug/status` - Detailed Status
- **Purpose**: Comprehensive service status
- **Response**: JSON with database stats, job counts, etc.
- **Available**: After pg-boss initialization
- **Use**: Debugging and monitoring

## üö® Emergency Procedures

### **Immediate Rollback**
```bash
# Get deployment history
railway deployments

# Rollback to previous deployment
railway rollback [DEPLOYMENT_ID]
```

### **Force Restart**
```bash
# Restart the service
railway restart
```

### **Check Resource Usage**
```bash
# Monitor resource consumption
railway metrics

# Check if hitting memory/CPU limits
```

## üìà Monitoring & Alerts

### **Set Up Monitoring**
```bash
# Monitor health endpoint
watch -n 30 'curl -f https://your-domain.railway.app/healthz'

# Monitor logs for errors
railway logs -f | grep -E "(error|failed|timeout)"
```

### **Key Metrics to Watch**
- **Health Check Response Time**: < 5s
- **Startup Time**: < 90s
- **Memory Usage**: < 400MB
- **CPU Usage**: < 50%

## üîç Advanced Debugging

### **Enable Debug Logging**
```bash
# Add debug environment variable
railway variables set DEBUG="*"

# Or specific debug patterns
railway variables set DEBUG="worker:*,boss:*"
```

### **Database Query Debugging**
```bash
# Check pg-boss tables
railway run psql $DATABASE_URL -c "\dt pgboss.*"

# Check job queue status
railway run psql $DATABASE_URL -c "SELECT name, state, count(*) FROM pgboss.job GROUP BY 1,2;"
```

### **Network Debugging**
```bash
# Test internal connectivity
railway run curl -v http://localhost:8080/healthz

# Test external connectivity
railway run curl -v https://api.openai.com/v1/models
```

## üìû Getting Help

### **Railway Support**
- **Discord**: https://discord.gg/railway
- **Documentation**: https://docs.railway.app/
- **Status Page**: https://status.railway.app/

### **ZEKE-Specific Issues**
- **Test Script**: `./scripts/test-railway-deployment.sh`
- **Local Development**: `pnpm dev:worker`
- **Integration Tests**: `pnpm test:integration`

---

**Remember**: The health check endpoint (`/healthz`) is designed to respond immediately, even before full service initialization. If it's failing, the issue is likely with basic HTTP server startup, not database connectivity.

================
File: railway.toml
================
[build]
builder = "dockerfile"
dockerfilePath = "Dockerfile"

[deploy]
healthcheckPath = "/healthz"
healthcheckTimeout = 60
restartPolicyType = "on_failure"
restartPolicyMaxRetries = 3

[env]
# These will need to be set in Railway dashboard
# Using existing Supabase Cloud database (no Railway PostgreSQL needed)
# DATABASE_URL = "postgresql://worker:${{WORKER_DB_PASSWORD}}@aws-0-us-east-1.pooler.supabase.com:5432/postgres"
# NEXT_PUBLIC_SUPABASE_URL = "https://hblelrtwdpukaymtpchv.supabase.co"
# NEXT_PUBLIC_SUPABASE_ANON_KEY = "${{SUPABASE_ANON_KEY}}"
# SUPABASE_SERVICE_ROLE_KEY = "${{SUPABASE_SERVICE_ROLE_KEY}}"
# OPENAI_API_KEY = "${{OPENAI_API_KEY}}"
# YOUTUBE_API_KEY = "${{YOUTUBE_API_KEY}}"
# NODE_ENV = "production"
# PORT = "8080"

[variables]
# Default port for Railway
PORT = "8080"
NODE_ENV = "production"
BOSS_SCHEMA = "pgboss"
BOSS_MIGRATE = "false"  # Schema already exists in Supabase
USE_SSL = "true"

# YouTube processing is optional - will be skipped if YOUTUBE_API_KEY is not set
YOUTUBE_PROCESSING_ENABLED = "false"

# Railway-specific optimizations
[build.env]
NODE_ENV = "production"
PNPM_FLAGS = "--prod --frozen-lockfile"

# Resource limits (adjust based on needs)
[resources]
memory = "512Mi"
cpu = "0.5"

# Networking
[networking]
port = 8080
protocol = "http"

# Logging configuration
[logging]
level = "info"
format = "json"

# Auto-scaling (Railway Pro feature)
[scaling]
min_replicas = 1
max_replicas = 3
target_cpu = 70
target_memory = 80

================
File: test-youtube-api.js
================
#!/usr/bin/env node

// Simple test script to verify YouTube API client functionality
import 'dotenv/config';
import { getChannelUploads } from './dist/lib/youtube/get-channel-uploads.js';
import { searchVideos } from './dist/lib/youtube/search-videos.js';
import { createYouTubeClient } from './dist/lib/youtube/youtube-client.js';

async function testYouTubeAPI() {
  if (!process.env.YOUTUBE_API_KEY) {
    process.exit(1);
  }

  try {
    const client = createYouTubeClient();
    const videos = await searchVideos(client, {
      query: 'AI research',
      maxResults: 2,
    });

    if (videos.length > 0) {
      const _video = videos[0];
    }
    const channelVideos = await getChannelUploads(
      client,
      'UUbfYPyITQ-7l4upoX8nvctg',
      2
    );

    if (channelVideos.length > 0) {
      const _channelVideo = channelVideos[0];
    }
  } catch (error) {
    if (error.response) {
    }
    process.exit(1);
  }
}

testYouTubeAPI();

================
File: test-youtube-pipeline.js
================
#!/usr/bin/env node

// Test script to verify the complete YouTube processing pipeline
import 'dotenv/config';
import { extractAudio } from './dist/extract/extract-youtube-audio.js';
import { getVideoMetadata } from './dist/extract/get-youtube-metadata.js';
import { transcribeAudio } from './dist/transcribe/whisper.js';
import { cleanupVideoTempFiles } from './dist/utils/temp-files.js';

async function testYouTubePipeline() {
  // Use a short test video (Two Minute Papers - should be under 5 minutes)
  const testVideoId = 'nHBgc_oNfQw'; // Latest Two Minute Papers video
  const testVideoUrl = `https://www.youtube.com/watch?v=${testVideoId}`;

  const tempFiles = [];

  try {
    const metadata = await getVideoMetadata(testVideoId);

    // Check if video is too long for testing (allow up to 6 minutes for this test)
    if (metadata.duration && metadata.duration > 360) {
      return;
    }
    const audioResult = await extractAudio(testVideoUrl, testVideoId);
    tempFiles.push(audioResult.audioPath);
    const transcriptionResult = await transcribeAudio(
      audioResult.audioPath,
      testVideoId
    );
    tempFiles.push(...transcriptionResult.tempFiles);
  } catch (_error) {
  } finally {
    // Cleanup temp files
    if (tempFiles.length > 0) {
      try {
        await cleanupVideoTempFiles(tempFiles);
      } catch (_cleanupError) {}
    }
  }
}

testYouTubePipeline();

================
File: tsconfig.json
================
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "ESNext",
    "moduleResolution": "node",
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": false,
    "declaration": true,
    "declarationMap": true,
    "sourceMap": false,
    "downlevelIteration": true,
    "baseUrl": ".",
    "rootDir": "./src",
    "outDir": "./dist",
    "paths": {
      "@/*": ["./*"],
      "@zeke/*": ["../../packages/*"]
    }
  },
  "include": ["src/**/*.ts"],
  "exclude": ["node_modules", "dist", "**/*.test.ts"]
}
