### **Problem**

1. too many pieces of content coming in
2. data ingestion pipeline is pretty dumb

### Current YouTube ingestion pipeline (2024-04)

```text
YouTube source (channel/search) [public.sources]
   |
   | ingest:pull cron queues ingest:source jobs (pg-boss)
   v
fetch-youtube-*-videos (YouTube Data API v3)
   |  - playlistItems.search + videos.list enrich rows
   v
buildDiscoveryYouTube → upsertDiscovery (public.discoveries)
   |
   | enqueue ingest:fetch-youtube-content w/ videoId
   v
extractYouTubeContent
   |- downloadVideoCaptions() via YouTube Captions API → VTT + segments
   |- prepareYouTubeTranscript() wraps VTT + plaintext
   |- insertContents() + insertStory()/findStoryIdByContentHash()
   |- enqueue analyze:llm
   v
analyzeStory (analyze:llm)
   |- OpenAI/stub summarises + embeds
   |- upsertStoryOverlay() + upsertStoryEmbedding()
```

**Current inputs feeding the pipeline**
- `public.sources` rows with `kind` (`youtube_channel` or `youtube_search`), `metadata.upload_playlist_id`, optional search `query`, and max result settings.
- YouTube Data API responses (playlist items / search + videos.list) providing `videoId`, `title`, `description`, `publishedAt`, `channelId`, `channelTitle`, `thumbnailUrl`, ISO-8601 `duration`, `viewCount`, `likeCount`, `commentCount`, `tags`, `categoryId`, `defaultLanguage`, `defaultAudioLanguage`.
- Caption metadata + VTT payloads from the YouTube Captions API (language, track id, `trackKind`, text segments).
- Derived fields inside engine (e.g., `hashText(enhancedText)`, transcript URLs, normalized duration seconds).

**What we persist today**
- `public.discoveries`: `source_id`, `external_id` (videoId), canonical URL, title, and the metadata JSON blob above.
- `public.contents`: formatted transcript bundle (`text`), `html_url`, `lang`, `content_hash`, `transcript_url` (`youtube://<id>`), inline VTT (`transcript_vtt`).
- `public.stories`: one row per deduped transcript (`content_id`, title fallback to metadata/discovery, canonical URL, kind `youtube`, publication timestamp).
- `public.story_overlays`: LLM analysis outputs (`why_it_matters`, `chili`, `confidence`, `citations`, `model_version`).
- `public.story_embeddings`: dense vector (currently one embedding per story+model) for retrieval.
- `public.source_health` bookkeeping for ingest success/error, plus quota metrics via `quotaTracker`.

**Improvements focused on YouTube + vector coverage**
1. Replace the heavy `yt-dlp` + Whisper path with the official Captions API (`captions.list` → `captions.download` per https://developers.google.com/youtube/v3/docs/captions/list). Only fall back to audio extraction when captions are absent or gated. This drops compute/storage spend dramatically.
2. Persist caption metadata alongside transcripts (track id, language, `isAutoGenerated`) to drive quality heuristics and future model selection.
3. Split structured video metadata into a dedicated table/JSON column (duration seconds, counts, tags, category) so we can vectorise and filter without reparsing the transcript blob.
4. Generate multiple embeddings per story: (a) full transcript (already), (b) title+description snippet, (c) tag/category string, (d) channel synopsis/health, (e) optional per-segment embeddings for highlight search. Store them with model + semantic purpose labels.
5. Capture additional scalar features (publish cadence, engagement deltas, caption quality) and either project them into embeddings or keep for hybrid search/ranking.
6. Cache downloaded caption text to avoid re-fetching on retries and to support upcoming re-vectorisation jobs without re-ingest.


I think that we're doing vector embeddings on the entire transcript.

But whatre other things that we can put into the vector embedding input?


``` typescript
export type story_embedding_input
rawTranscript: string
videoDescription: string?
nameMentions
videoDuration
videoLikes


```
